{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83ab510a-ffd1-41ba-bcd0-5ce173c74d70",
   "metadata": {},
   "source": [
    "## Database Design\n",
    "\n",
    "\n",
    "Do not take this course, the institutor talking with no logical, and she think where she talks, just waste your time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73715687-e19b-4c10-8b86-83148da7e7b8",
   "metadata": {},
   "source": [
    "## Course Description\n",
    "\n",
    "A good database design is crucial for a high-performance application.  Just like you wouldn't start building a house without the benefit of a blueprint, you need to think about how your data will be stored beforehand.  Taking the time to design a database saves time and frustration later on, and a well-designed database ensures ease of access and retrieval of information.  While choosing a design, a lot of considerations have to be accounted for.  In this course, you'll learn how to process, store, and organize data in an efficient way.  You'll see how to structure data through normalization and present your data with views.  [Finally, you'll learn how to manage your database and all of this will be done on a variety of datasets from book sales, car rentals, to music reviews.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bc3db9-7114-4d0a-bfbe-996ed04cec8c",
   "metadata": {},
   "source": [
    "##  Processing, Storing, and Organizing Data\n",
    "Free\n",
    "0%\n",
    "\n",
    "Start your journey into database design by learning about the two approaches to data processing, OLTP and OLAP. In this first chapter, you'll also get familiar with the different forms data can be stored in and learn the basics of data modeling.\n",
    "\n",
    "    OLTP and OLAP    50 xp\n",
    "    OLAP vs. OLTP    100 xp\n",
    "    Which is better?    50 xp\n",
    "    Storing data    50 xp\n",
    "    Name that data type!    100 xp\n",
    "    Ordering ETL Tasks    100 xp\n",
    "    Recommend a storage solution    50 xp\n",
    "    Database design    50 xp\n",
    "    Classifying data models    100 xp\n",
    "    Deciding fact and dimension tables    100 xp\n",
    "    Querying the dimensional model    100 xp \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c475807f-82ee-40fe-81bb-c5c63c87f8d9",
   "metadata": {},
   "source": [
    "##  Database Schemas and Normalization\n",
    "0%\n",
    "\n",
    "In this chapter, you will take your data modeling skills to the next level. You'll learn to implement star and snowflake schemas, recognize the importance of normalization and see how to normalize databases to different extents.\n",
    "\n",
    "    Star and snowflake schema    50 xp\n",
    "    Running from star to snowflake    50 xp\n",
    "    Adding foreign keys    100 xp\n",
    "    Extending the book dimension    100 xp\n",
    "    Normalized and denormalized databases    50 xp\n",
    "    Querying the star schema    100 xp\n",
    "    Querying the snowflake schema    100 xp\n",
    "    Updating countries    100 xp\n",
    "    Extending the snowflake schema    100 xp\n",
    "    Normal forms    50 xp\n",
    "    Converting to 1NF    100 xp\n",
    "    Converting to 2NF    100 xp\n",
    "    Converting to 3NF    100 xp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34ce575-1337-4f2d-96ec-0dd810d5f507",
   "metadata": {},
   "source": [
    "##  Database Views\n",
    "0%\n",
    "\n",
    "Get ready to work with views! In this chapter, you will learn how to create and query views. On top of that, you'll master more advanced capabilities to manage them and end by identifying the difference between materialized and non-materialized views.\n",
    "\n",
    "    Database views    50 xp\n",
    "    Tables vs. views    100 xp\n",
    "    Viewing views    100 xp\n",
    "    Creating and querying a view    100 xp\n",
    "    Managing views    50 xp\n",
    "    Creating a view from other views    100 xp\n",
    "    Granting and revoking access    100 xp\n",
    "    Updatable views    50 xp\n",
    "    Redefining a view    100 xp\n",
    "    Materialized views    50 xp\n",
    "    Materialized versus non-materialized    100 xp\n",
    "    Creating and refreshing a materialized view    100 xp\n",
    "    Managing materialized views    50 xp \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17460a77-bdaa-4029-99e5-f2d55294d4e8",
   "metadata": {},
   "source": [
    "##  Database Management\n",
    "0%\n",
    "\n",
    "This final chapter ends with some database management-related topics. You will learn how to grant database access based on user roles, how to partition tables into smaller pieces, what to keep in mind when integrating data, and which DBMS fits your business needs best.\n",
    "\n",
    "    Database roles and access control    50 xp\n",
    "    Create a role    100 xp\n",
    "    GRANT privileges and ALTER attributes    100 xp\n",
    "    Add a user role to a group role    100 xp\n",
    "    Table partitioning    50 xp\n",
    "    Reasons to partition    50 xp\n",
    "    Partitioning and normalization    100 xp\n",
    "    Creating vertical partitions    100 xp\n",
    "    Creating horizontal partitions    100 xp\n",
    "    Data integration    50 xp\n",
    "    Data integration do's and dont's    100 xp\n",
    "    Analyzing a data integration plan    50 xp\n",
    "    Picking a Database Management System (DBMS)    50 xp\n",
    "    SQL versus NoSQL    50 xp\n",
    "    Choosing the right DBMS    100 xp \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2c8a4e-2a35-4541-8190-a51d96700e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e41e95a-473a-4137-a2cf-131c92c4cf47",
   "metadata": {},
   "source": [
    "## OLTP and OLAP\n",
    "\n",
    "\n",
    "\n",
    "Hello, my name is Lis, I'm a Curriculum Manager here at DataCamp.  This course will be talking about database design.  So what does that entail exactly?  To put it simply, in this course we are asking the question: [How should we organize and manage data?]  To answer this, we have to consider the different schemas, management options, and objects that make up a database.  Some examples are listed here, and they are covered throughout the course.  These topics all affect the way data is stored and accessed.  Some enable faster query speeds.  Some take up less memory than others.  And notablely, some cost more money than others.  \n",
    "\n",
    "\n",
    "    Schemas: How should my daya be logically organized?\n",
    "    Normalization: Should my data have minimal dependency and redundancy?\n",
    "    Views: What joins will be done most often?\n",
    "    Access control: Should all users of the data have the same level of access?\n",
    "    DBMS: How do I pick between all the SQL and noSQL options?\n",
    "    and more?\n",
    "    \n",
    "And as we will soon find out in this course, there is no one right answer to this motivating question.  It will come down to how the data will be used.  Now lets dive in.  \n",
    "\n",
    "\n",
    "OLTP and OLAP are approaches to processing data, and they will be referenced throughout this course.  They help define the way data is going to flow, be structured, and stored.  If you figure out which fits your business case, desiging your database will be much easier.  [OLTP stands for Online Transaction Processing].  [OLAP stands for Online Analytical Processing].  As the name hint, the OLTP approach is oriented around transactions, while the other is oriented around analytics.  Before going into formal definitions, lets look at some use case of each.  Say you are in charge of data management at a bookstore.  You would use an OLTP approach to keep track of the prices of books, while to analyze the most profitable books, an OLAP approach woule be more appropriate.  [To keep track all customer transactions, you would use an OLTP approach to insert sales as customers finish paying].  However, if you wanted to do sophisticated analysis on sales, like most loyal customers - you would use OLAP.  An OLTP database would be used to track when when employees have worked, while to run an analysis on who deserves employee of the month, you would need to switch over to OLAP.  Are you starting to see the diff?  [OLTP focus on supporting day-to-day operations, while OLAP tasks are vaguer and focus on business decision making]  \n",
    "\n",
    "[The OLTP systems are application-oriented, like for bookkeeping for example.  OLAP systems are oriented around a certain subject that's under analysis, like last quarter's book sales].  The data in OLTP systems can be seen as a current snapshot of transactions that are archived often.  The data in OLAP systems tend to be data from over a large period of time that has been consolidated for long-term analysis.  This means OLAP tends to have more data than OLTP.  As we saw in the bookstore example, the commonly executed OLTP queries are simpler and require a quick query or update.  On the other hand, OLAP systems used for analysis require more complex queries.  In terms of how these approaches are being used, OLTP systems are used by more people throughout a company and even a company's customers, while OLAP systems are typically used by only analysts and data scientists at a company.  \n",
    "\n",
    "\n",
    "[OLTP and OLAP systems work together, in fact, they need each other.  OLTP data is usually stored in an operational database that is pulled and cleaned to create an OLAP data warehouse].(But how, we want to tract the user experience, we want to track the business functions)  We'll get into data warehouses and other storage solutions in the next video.  Without transactional data, no analyses can be done in the first place.  Analyses from OLAP systems are used to inform business practice and day-to-day activity, thereby influencing the OLTP databases.  \n",
    "\n",
    "\n",
    "To wrap up, here is what you should take away from this video.  Before implementing anything, figure out your business requirements because there are many design decisions you'll have to make.  The way you set up your database now wil affect how it can be effectively used in the future.  Start by figuring out if you need an OLAP or OLTP approach, or perhaps both.  You should now be comfortable with the differences between both.  These are the two most common approaches.  However, they are not exhaustive, but they are an excellent start to get you on the right path to designing your database.  In later videos, we'll learn more about the technical difference between both approaches.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57576fa4-79af-4027-b9bc-273f2e01da68",
   "metadata": {},
   "source": [
    "## OLAP vs. OLTP\n",
    "\n",
    "You should now be familiar with the differences between OLTP and OLAP. In this exercise, you are given a list of cards describing a specific approach which you will categorize between OLAP and OLTP.\n",
    "Instructions\n",
    "100XP\n",
    "\n",
    "    Categorize the cards into the approach that they describe best.\n",
    "\n",
    "OLAP:\n",
    "    Typically use a data warehouse\n",
    "    Helps businesses with decision making and problem solving\n",
    "    Queries a large amount of data\n",
    "\n",
    "OLTP:\n",
    "    Data is inserted and updated more often\n",
    "    Typically use an operational database\n",
    "    Most likely to have data from the past hour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d99099-8ca0-400f-bd61-667746453b2d",
   "metadata": {},
   "source": [
    "## Which is better?\n",
    "\n",
    "The city of Chicago receives many 311 service requests throughout the day. 311 service requests are non-urgent community requests, ranging from graffiti removal to street light outages. Chicago maintains a data repository of all these services organized by type of requests. In this exercise, Potholes has been loaded as an example of a table in this repository. It contains pothole reports made by Chicago residents from the past week.\n",
    "\n",
    "Explore the dataset. What data processing approach is this larger repository most likely using?\n",
    "Instructions\n",
    "50 XP\n",
    "Instructions\n",
    "50 XP\n",
    "Possible Answers\n",
    "\n",
    "    OLTP because this table could not be used for any analysis.\n",
    "    OLAP because each record has a unique service request number.\n",
    "#    OLTP because this table's structure appears to require frequent updates.\n",
    "    OLAP because this table focuses on pothole requests only.\n",
    "    \n",
    "    Hint\n",
    "\n",
    "    Run SELECT * FROM Potholes to take look at the data contained in this table.\n",
    "[    The rows current_status and most_recent_action indicate that this table expects updates on records.]\n",
    "    Does this data look like transactional day-to-day data?\n",
    "\n",
    "Incorrect submission\n",
    "This data could be used for analysis, it just wouldn't be as efficient as using an OLAP approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea352640-e33f-4070-bf03-ea829b58e72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "select * from Potholes;\n",
    "\n",
    "creation_date\tcurrent_status\tcompletion_date\tservice_request_id\ttype_of_service\tmost_recent_action\tstreet_address\tzip\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03380123\tPothole in Street\tnull\t10300 S WALLACE ST\t60628.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03388180\tPothole in Street\tnull\t4100 S WESTERN BLVD\t60609.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03388493\tPothole in Street\tnull\t5230 S NEW ENGLAND AVE\t60638.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03386050\tPothole in Street\tnull\t1053 E 92ND ST\t60619.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03382135\tPothole in Street\tnull\t4756 W 85TH ST\t60652.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03382643\tPothole in Street\tnull\t3623 N NORA AVE\t60634.0\n",
    "2018-12-17T00:00:00.000\tCompleted\t2018-12-18T00:00:00.000\t18-03378491\tPothole in Street\tCompleted Upon Arrival\t719 N DRAKE AVE\t60624.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03380795\tPothole in Street\tnull\t1900 S HAMLIN AVE\t60623.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03388657\tPothole in Street\tnull\t1899 S KOMENSKY AVE\t60623.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03383604\tPothole in Street\tnull\t1646 N NATOMA AVE\t60707.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03387491\tPothole in Street\tnull\t3530 W GOVERNORS PKWY\t60624.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03381825\tPothole in Street\tnull\t7558 W DEVON AVE\t60631.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03381945\tPothole in Street\tnull\t2411 N NEWCASTLE AVE\t60707.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03388109\tPothole in Street\tnull\t6300 S NARRAGANSETT AVE\t60638.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03381921\tPothole in Street\tnull\t4400 W BELDEN AVE\t60639.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03389580\tPothole in Street\tnull\t1127 W SCHUBERT AVE\t60614.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03378282\tPothole in Street\tnull\t5606 W BRYN MAWR AVE\t60646.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03378374\tPothole in Street\tnull\t9400 S WOODLAWN AVE\t60619.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03381490\tPothole in Street\tnull\t1105 N CHRISTIANA AVE\t60651.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03383626\tPothole in Street\tnull\t10359 S FAIRFIELD AVE\t60655.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03382580\tPothole in Street\tnull\t5508 S WHIPPLE ST\t60629.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03388378\tPothole in Street\tnull\t1358 S KEDZIE AVE\t60623.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03382611\tPothole in Street\tnull\t3021 N NEWLAND AVE\t60634.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03384843\tPothole in Street\tnull\t4711 N VIRGINIA AVE\t60625.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03383522\tPothole in Street\tnull\t7926 S COTTAGE GROVE AVE\t60619.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03378054\tPothole in Street\tnull\t4142 N WESTERN AVE\t60618.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03382506\tPothole in Street\tnull\t7300 W WAVELAND AVE\t60634.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03382926\tPothole in Street\tnull\t2550 N PULASKI RD\t60639.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03381396\tPothole in Street\tnull\t3116 N SACRAMENTO AVE\t60618.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03381474\tPothole in Street\tnull\t2410 W GRAND AVE\t60612.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03388778\tPothole in Street\tnull\t8247 S GREEN ST\t60620.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03385654\tPothole in Street\tnull\t4155 W 82ND PL\t60652.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03379525\tPothole in Street\tnull\t5300 S FRANCISCO AVE\t60632.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03381588\tPothole in Street\tnull\t6231 W GUNNISON ST\t60630.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03382660\tPothole in Street\tnull\t1517 W WEBSTER AVE\t60614.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03377810\tPothole in Street\tnull\t945 W DIVERSEY PKWY\t60614.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03384615\tPothole in Street\tnull\t3707 E 116TH ST\t60617.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03380164\tPothole in Street\tnull\t10000 S LOWE AVE\t60628.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03385464\tPothole in Street\tnull\t5001 S LA CROSSE AVE\t60638.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03386292\tPothole in Street\tnull\t2356 E 71ST ST\t60649.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03378165\tPothole in Street\tnull\t1121 N LATROBE AVE\t60651.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03383170\tPothole in Street\tnull\t6644 S STONY ISLAND AVE\t60637.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03382675\tPothole in Street\tnull\t2640 W MONROE ST\t60612.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03388384\tPothole in Street\tnull\t1959 S TROY ST\t60623.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03382923\tPothole in Street\tnull\t5046 N AVERS AVE\t60625.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03389579\tPothole in Street\tnull\t2600 N LAKEWOOD AVE\t60614.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03383524\tPothole in Street\tnull\t4834 N LONG AVE\t60630.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03379830\tPothole in Street\tnull\t2305 W 34TH ST\t60608.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03389578\tPothole in Street\tnull\t2640 N SHEFFIELD AVE\t60614.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03385110\tPothole in Street\tnull\t4180 W 82ND ST\t60652.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03380063\tPothole in Street\tnull\t6030 N NAGLE AVE\t60646.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03379292\tPothole in Street\tnull\t854 E 40TH ST\t60653.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03383449\tPothole in Street\tnull\t3411 S EMERALD AVE\t60616.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03378454\tPothole in Street\tnull\t4100 S WESTERN AVE\t60609.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03387506\tPothole in Street\tnull\t1145 S WASHTENAW AVE\t60612.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03379518\tPothole in Street\tnull\t10200 S HALSTED ST\t60628.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03388112\tPothole in Street\tnull\t5400 N WESTERN AVE\t60625.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03381370\tPothole in Street\tnull\t2300 N WESTERN AVE\t60647.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03385374\tPothole in Street\tnull\t10800 S SANGAMON ST\t60643.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03384523\tPothole in Street\tnull\t300 S HOMAN AVE\t60624.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03380286\tPothole in Street\tnull\t3542 N PULASKI RD\t60641.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03387600\tPothole in Street\tnull\t2747 N LARAMIE AVE\t60639.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03379342\tPothole in Street\tnull\t4830 S KEATING AVE\t60632.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03387505\tPothole in Street\tnull\t1145 S FAIRFIELD AVE\t60612.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03387458\tPothole in Street\tnull\t908 S CAMPBELL AVE\t60612.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03378386\tPothole in Street\tnull\t2138 N DAYTON ST\t60614.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03386479\tPothole in Street\tnull\t1819 W HOWARD ST\t60626.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03387502\tPothole in Street\tnull\t2421 W 21ST PL\t60608.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03382321\tPothole in Street\tnull\t8350 W BELMONT AVE\t60634.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03382231\tPothole in Street\tnull\t1344 E 88TH ST\t60619.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03378466\tPothole in Street\tnull\t1405 W BELLE PLAINE AVE\t60613.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9427a7-8c48-449a-9b36-89c3031c4d16",
   "metadata": {},
   "source": [
    "## Storing data\n",
    "\n",
    "\n",
    "\n",
    "Lets discuss the different ways you can store data.  Data can be stored in 3 different levels.  \n",
    "\n",
    "The first is [structured data], which is usually defined by schemas.  \n",
    "[Data types and tables are not only defined, but relationships between tables are also defined, using concepts like foreign keys].  \n",
    "\n",
    "The second is [unstructured data], which is schemaless and data in its rawest form, meaning its not clean.  Most data n the world is unstructured.  Examples includes media files and raw text.  \n",
    "\n",
    "The third is [semi-structured] data, which does not follow a larger schema, rather it has an ad-hoc self-describing structure.  Therefore, it has some structure.  This is an inherently vague definition as there can be a lot of variation between structured and unstructured data.  Examples includes NoSQL, XML, and JSON.  \n",
    "\n",
    "Because its clean and organized, structured data is easier to analyze.  However, its not as flexible because it need to follow a schema, which makes it less scalable.  These are trade-offs to consider as you move between structured and unstructured data.  You should already be familiar with traditional databases.  They generally follow relational schemas.  Operational databases, which are used for OLTP, are an example of traditional database.  Decades ago, traditional databases used to be enoughfor data storage.  Then as data analytics took off, data warehouses were popularized for OLAP approaches.  And now in the age of big data, we need to analyze and store even more data, which is where the data lake comes in.  \n",
    "\n",
    "I used to term \"traditional databases\" because many people consider data warehouse and lakes to be a type of database.  Data warehouses are optimized for read-only analytics.  They combine data from multiple sources and use massively parallel processing for faster queries.  In their database design, they typically use dimensional modeling and a denormalized schema.  We will walk through both of these terms later in the course.  \n",
    "\n",
    "\n",
    "Data warehouse:\n",
    "    Optimized for analytics - OLAP\n",
    "        Organized for reading/aggregating data\n",
    "        Usually read-only\n",
    "    Contains data from multiple sources\n",
    "    Massively Parallel Processing (MPP)\n",
    "    Typically uses a denormalized schema and dimensional modeling\n",
    "Data marts\n",
    "    Subset of data warehouse\n",
    "\n",
    "\n",
    "Amazon, Google, and Microsoft all often data warehouse solutions, know ar Redshift, Big Query, and Azure SQL Data Warehouse, respectively.  [A data mart is a subset of a data warehouse dedicated to a specific topic].  Data marts allow departments to have easier access to the data that matters to them.  Technically, traditional databases and warehouses can store unstructured data, but not cost-effectively.  Data Lake storage is cheaper because it uses object storage as opposed to the traditional block or file storage.  This allows massive amounts of data to be stored effectivey of all types, from streaming data to operational databases.  Lakes are massive because they store all the data that might be used.  \n",
    "\n",
    "Data lakes are often petabytes in size - that's 1000 terabytes.  Unstructured data is the most scable, which permits this size.  Lakes are schema-on-read, meaning the schema is created as data is read.  Warehouses and traditional databases are classified as schema-on-write because the schema is predefined.  Data lakes have to be organized and cataloged well, otherwise it becomes an aptly named \"data swamp\".  Data lakes aren't  only limited to storage.  Its becoming popular to run analytics on data lakes.  This is especially true for tasks like deep learning and data discovery, which needs a lot of data that doesn't need to be that clean.  Again the big 3 cloud providers all ofer a data lake solution.  \n",
    "\n",
    "When we think about where to store data, we have to think about how data will get there and in what form.  Extract Transform Load and Extract Load Transform are 2 different approaches for describing data flows.  They get into the intricacies of building data pipelines, which we will not get into.  ETL is the more traditional approach for warehousing and smaller-scale analutics.  But ELT has become more common with big data projects.  In ETL data is transformed before loading into storage, usually to follow the storages' schema, as in the case with warehouse.  In ELT, the data is stored in its native form in a storage solution like a data lake.  The portions of data are transformed for different purposes, from building a data warehouse to doing deep learning.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2adaf7-dfda-4173-932f-b6114c84bcce",
   "metadata": {},
   "source": [
    "## Name that data type!\n",
    "\n",
    "In the previous video, you learned about structured, semi-structured, and unstructured data. Structured data is the easiest to analyze because it is organized and cleaned. On the other hand, unstructured data is schemaless, but scales well. In the middle we have semi-structured data for everything in between.\n",
    "Instructions\n",
    "100XP\n",
    "\n",
    "Each of these cards hold a type of data. Place them in the correct category.\n",
    "\n",
    "Unstructured:\n",
    "    To-do list in a text editor\n",
    "    Zip file of all text messages ever received\n",
    "    Images in your photo library\n",
    "\n",
    "Semi-structured:\n",
    "    JSON object of tweets outputted in real-time by the Twitter API\n",
    "    <note><from>Lis</from><heading>Thanks Ruanne!</heading><body>You rock</body></note>    \n",
    "\n",
    "Structured:\n",
    "    A relational database with latest withdrawals and deposits made by clients\n",
    "    CSVs of open data downloaded from your local government websites  [Semi-structured]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c89834b-0db3-4ebe-ac0c-5df4ec687527",
   "metadata": {},
   "source": [
    "## Ordering ETL Tasks\n",
    "\n",
    "You have been hired to manage data at a small online clothing store. Their system is quite outdated because their only data repository is a traditional database to record transactions.\n",
    "\n",
    "You decide to upgrade their system to a data warehouse after hearing that different departments would like to run their own business analytics. You reason that an ELT approach is unnecessary because there is relatively little data (< 50 GB).\n",
    "Instructions\n",
    "100XP\n",
    "\n",
    "In the ETL flow you design, different steps will take place. Place the steps in the most appropriate order.\n",
    "\n",
    "eCommerce API outputs real time data of transactions\n",
    "Python script drops null rows and clean data into pre-determined columns\n",
    "Resulting dataframes is written into an AWS Redshift Warehouse\n",
    "\n",
    "Hint\n",
    "\n",
    "[    Python scripts are often used to transform data to match a specified schema.]\n",
    "    Data marts are a subset of data warehouses.\n",
    "    In ETL, you need to clean data before writing it to a data warehouse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aad440f-6d8d-4464-80a7-e54da8687be7",
   "metadata": {},
   "source": [
    "## Recommend a storage solution\n",
    "\n",
    "When should you choose a data warehouse over a data lake?\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "\n",
    "    To train a machine learning model with a 150 GB of raw image data.\n",
    "    1\n",
    "    To store real-time social media posts that may be used for future analysis\n",
    "    2\n",
    "    To store customer data that needs to be updated regularly\n",
    "    3\n",
    "#    To create accessible and isolated data repositories for other analysts\n",
    "    4\n",
    "    \n",
    "Hint\n",
    "\n",
    "    Data lakes store unstructured data, which is data in its native form.\n",
    "    Data lakes are better equipped to handle big data problems.\n",
    "    The main use of warehouses is not to store data, but to run analysis.\n",
    "\n",
    "That's right! Analysts will appreciate working in a data warehouse more because of its organization of structured data that make analysis easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fcfac8-871d-4233-9fd6-ee87ed7c42f9",
   "metadata": {},
   "source": [
    "## Database design\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now lets learn more about database design means.  [Database design determines how data is logically stored].  This is crucial because it affects how the database will be queried, whether for reading data or updating data.  There are 2 important concepts to know when it comes to database design: [Database models and schemas].  Database models are high-level specifications for database structure.  [The relational model, which is the most popular, is the model used to make relational databases].  It defines rows as records and columns as attributes.  It calls for rules such as each row having unique keys.  There are other models that exist that do not enforce the same rules.  \n",
    "\n",
    "A schema is a database's blueprint.  In other words, the implementation of the database model.  It takes the logical structure more granularly by defining the specific tables, fields, relationships, indexes, and views a database will have.  Schemas must be respected when inserting structured data into a relational database.  [The first step to database design is data modeling].  __This is the abstract design phase, where we define a data model for the data to be stored.  There are 3 levels to a data model: \n",
    "\n",
    "[    A conceptual data model] describes what the database contains, such as its entities, relationships, sttributes. \n",
    "[    A logical data model] decides how these entities and relationships map to tables.  \n",
    "[    A physical data model] looks at how data will be physically stored at the lowest level of abstraction.  \n",
    "These 3 levels of a data model ensure consistency and provide a plan for implementation and use.  \n",
    "\n",
    "Here is a simplified example of where we want to store songs.  In this case, the entities are songs, albums, and artists with various [] attributes.  Their relationships are denoted by blue rhombuses (<>).  \n",
    "\n",
    "\n",
    "[Conceptual - ER diagram]\n",
    "    Song[song_id, title, length] --N--<>has-- Album[album_id, title, num_songs] --N--<>creates--1-- Artist[artist_id, label, ganre]\n",
    "\n",
    "\n",
    "Here we have a conceptual idea of the data we want to store.  And here is the a corresponding schema using the relational model.  The fastest way to create a schema is to translate the entities into tables.  But just because its the easiest, doesn't mean its the best.  \n",
    "\n",
    "\n",
    "[Logical - schema]\n",
    "   Songs\n",
    "    song_id     bigint\n",
    "    title       char\n",
    "    length      float\n",
    "    album_id    bigint  ----\n",
    "                           | \n",
    "   Albums                  |\n",
    "    album_id    bigint  ----\n",
    "    title       char\n",
    "    num_songs   int\n",
    "    artist_id   bigint  ----\n",
    "                           |\n",
    "   Artists                 |\n",
    "    artist_id   bigint  ----\n",
    "    genre       char\n",
    "    label       char\n",
    "\n",
    "\n",
    "Lets look at some other ways this ER diagram could be converted.  For example, you could opt to have one table because you don't want to run so many joins to get song information.  Or you could add tables for genre and label.  Many songs share these attributes, and having one place for them helps with data integrity.  [The biggest difference here is how the tables are determined].  There are different pros and cons to these 3 examples shown below.  The next chapter on normalization and denormalization will expand on this.  From the prerequisites, you should be familiar with the relational model.  \n",
    "\n",
    "\n",
    "   Songs\n",
    "    song_id          bigint\n",
    "    song_title       char\n",
    "    length           float\n",
    "    album_title      bigint\n",
    "    num_songs_album  int\n",
    "    artist_name      char\n",
    "    genre            char\n",
    "    label            char\n",
    "\n",
    "\n",
    "   Songs\n",
    "    song_id     bigint\n",
    "    title       char\n",
    "    length      float\n",
    "    album_id    bigint  ----\n",
    "                           | \n",
    "   Albums                  |\n",
    "    album_id    bigint  ----\n",
    "    title       char\n",
    "    num_songs   int\n",
    "    artist_id   bigint  ----\n",
    "                           |\n",
    "   Artists                 |\n",
    "    artist_id   bigint  ----\n",
    "    genre_id    bigint  -----------------------------|\n",
    "    label_id    bigint  ------Label                Genre\n",
    "                               label_id  bigint     genre_id  bigint\n",
    "                               label     char       genre     char\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "[Dimensional modeling is an adaptation of the relational model specifically for data warehouse].  Its optimized for OLAP type of queries that aim to analyze rather than update.  To do this, it uses the [star schema].  In the next chapter, we'll dive into that more.  [The schema of a dimentional model tends to be easy to interpret and extend].  This is a big plus for analysts working on the data warehouse.  Dimensional models are made up of 2 types of tables: [fact and dimension tables].  __What the fact table holds is decided by the business use-case.  It contains records of key metric, and this metric changes often.__  Fact tables also hold foreign keys to dimension tables.  __Dimension tables hold descriptions of specific attributes and these do not change as often.__  \n",
    "\n",
    "So what does that means?  Back to the Song Analysis example, The center (turquoise) table is a fact table called songs, It contains foreign keys to surrounding (purple) dimension tables.  These dimension table expand on the attributes of a fact table, such as the album it is in and the artist who mdade it.  The records in fact tables often change as new songs get inserted.  Albums, labels, artists and genres will be shared by more than one song - hence records in dimenstion tables won't change as much.  \n",
    "\n",
    "\n",
    "Dimension table               Fact table               Dimension table\n",
    "  Album                         Songs                    Artist\n",
    "    album_id                      album_id                 artist_id\n",
    "    album_title                   artist_id                artist_name\n",
    "    num_songs                     label_id                 gender\n",
    "    release_date                  genre_id                 age\n",
    "                                  song_title\n",
    "Dimension table                   song_length          Simension table\n",
    "  Label                                                  Genre\n",
    "    label_id                                               genre_id\n",
    "    label_name                                             genre_name\n",
    "    address\n",
    "\n",
    "\n",
    "Summing it up, to detect a fact table in a dimention model, consider what is being analyzed and how often entities change.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae14438-07be-4582-bc22-c416da5e5ce1",
   "metadata": {},
   "source": [
    "## Classifying data models\n",
    "\n",
    "In the previous video, we learned about three different levels of data models: conceptual, logical, and physical.\n",
    "Instructions\n",
    "100XP\n",
    "\n",
    "Each of these cards hold a tool or concept that fits into a certain type of data model. Place the cards in the correct category.\n",
    "\n",
    "Conceptual Data Model:\n",
    "    Gathers business requirements\n",
    "    Relational model   [Logical Data Model]\n",
    "    \n",
    "Logical Data Model:\n",
    "    Entities, attributes, and relationships   [Conceptual Data Model]\n",
    "    Determining tables and columns\n",
    "    \n",
    "Physical Data Model:\n",
    "    File structure of data storage\n",
    "    \n",
    "Hint\n",
    "\n",
    "    Database design is a step in determining the logical structure of a database.\n",
    "    Entity-Relation diagrams are useful creating conceptual data models.\n",
    "    Physical data models focus on how data will be physically stored.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36967b74-725c-4f01-91e8-ee93f56cb8f2",
   "metadata": {},
   "source": [
    "## Deciding fact and dimension tables\n",
    "\n",
    "Imagine that you love running and data. It's only natural that you begin collecting data on your weekly running routine. You're most concerned with tracking how long you are running each week. You also record the route and the distances of your runs. You gather this data and put it into one table called Runs with the following schema:\n",
    "  runs\n",
    "    duration_mins - float\n",
    "    week - int\n",
    "    month - varchar(160)\n",
    "    year - int\n",
    "    park_name - varchar(160)\n",
    "    city_name - varchar(160)\n",
    "    distance_km - float\n",
    "    route_name - varchar(160)\n",
    "\n",
    "After learning about dimensional modeling, you decide to restructure the schema for the database. Runs has been pre-loaded for you.\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "\n",
    "    Question 1\n",
    "    Out of these possible answers, what would be the best way to organize the fact table and dimensional tables?\n",
    "Possible Answers\n",
    "\n",
    "#    A fact table holding duration_mins and foreign keys to dimension tables holding route details and week details, respectively.\n",
    "    A fact table holding week,month, year and foreign keys to dimension tables holding route details and duration details, respectively.\n",
    "    A fact table holding route_name,park_name, distance_km,city_name, and foreign keys to dimension tables holding week details and duration details, respectively.\n",
    "    \n",
    "Incorrect submission\n",
    "Try again. route_name,park_name, distance_km, and city_name would be better in it's own dimension table. We're more interested in the run itself, than where it took place.\n",
    "\n",
    "\n",
    "    Question 2\n",
    "    Create a dimension table called route that will hold the route information.\n",
    "    Create a dimension table called week that will hold the week information.\n",
    "\n",
    "Hint\n",
    "\n",
    "[    Route information includes the columns park_name, city_name, distance_km, and route_name.\n",
    "    Week information includes the columns week, month, and year.\n",
    "    Make sure to look at the schema diagram for the data type of each columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419501d4-385d-46de-b11a-c13ab48ac822",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT * FROM Runs;\n",
    "\n",
    "\n",
    "duration_mins\tweek\tmonth\tyear\tpark_name\tcity_name\tdistance_km\troute_name\n",
    "24.5\t3\tMay\t2019\tProspect Park\tBrooklyn\t5\tSimple Loop\n",
    "61\t3\tMay\t2019\tCentral Park\tNew York City\t8\tResevoir Loop\n",
    "24.5\t3\tMay\t2019\tCentral Park\tNew York City\t5\tLake Loop\n",
    "24.5\t4\tMay\t2019\tCentral Park\tNew York City\t5\tLake Loop\n",
    "48\t4\tMay\t2019\tProspect Park\tBrooklyn\t10\tGrove Run\n",
    "23\t4\tMay\t2019\tProspect Park\tBrooklyn\t5\tSimple Loop\n",
    "24.5\t1\tJune\t2019\tCentral Park\tNew York City\t5\tLake Loop\n",
    "54.96\t1\tJune\t2019\tPennypack Park\tPhiladelphia\t12\tPenny Trail Extended\n",
    "38.4\t1\tJune\t2019\tCentral Park\tNew York City\t8\tResevoir Loop\n",
    "23.75\t1\tJune\t2019\tCentral Park\tNew York City\t5\tLake Loop\n",
    "57.6\t2\tJune\t2019\tPennypack Park\tPhiladelphia\t12\tPenny Trail Extended\n",
    "31.8\t2\tJune\t2019\tPennypack Park\tPhiladelphia\t6\tPenny Trail\n",
    "49\t2\tJune\t2019\tLiberty State Park\tJersey City\t10\tWater Front Run\n",
    "23.35\t2\tJune\t2019\tProspect Park\tBrooklyn\t5\tSimple Loop\n",
    "39.2\t3\tJune\t2019\tCentral Park\tNew York City\t8\tResevoir Loop\n",
    "27.48\t3\tJune\t2019\tPennypack Park\tPhiladelphia\t6\tPenny Trail\n",
    "28.8\t3\tJune\t2019\tPennypack Park\tPhiladelphia\t6\tPenny Trail\n",
    "47.5\t3\tJune\t2019\tLiberty State Park\tJersey City\t10\tWater Front Run\n",
    "24\t4\tJune\t2019\tCentral Park\tNew York City\t5\tLake Loop\n",
    "53\t4\tJune\t2019\tProspect Park\tBrooklyn\t10\tGrove Run\n",
    "24.5\t4\tJune\t2019\tProspect Park\tBrooklyn\t5\tSimple Loop\n",
    "37.36\t4\tJune\t2019\tCentral Park\tNew York City\t8\tResevoir Loop\n",
    "28.8\t1\tJuly\t2019\tPennypack Park\tPhiladelphia\t6\tPenny Trail\n",
    "23\t1\tJuly\t2019\tProspect Park\tBrooklyn\t5\tSimple Loop\n",
    "49\t1\tJuly\t2019\tLiberty State Park\tJersey City\t10\tWater Front Run\n",
    "45.8\t1\tJuly\t2019\tLiberty State Park\tJersey City\t10\tWater Front Run\n",
    "24\t2\tJuly\t2019\tProspect Park\tBrooklyn\t5\tSimple Loop\n",
    "47.5\t2\tJuly\t2019\tProspect Park\tBrooklyn\t10\tGrove Run\n",
    "24\t2\tJuly\t2019\tCentral Park\tNew York City\t5\tLake Loop\n",
    "53\t2\tJuly\t2019\tLiberty State Park\tJersey City\t10\tWater Front Run\n",
    "24.5\t3\tJuly\t2019\tCentral Park\tNew York City\t5\tLake Loop\n",
    "37.36\t3\tJuly\t2019\tCentral Park\tNew York City\t8\tResevoir Loop\n",
    "24.5\t3\tJuly\t2019\tProspect Park\tBrooklyn\t5\tSimple Loop\n",
    "\n",
    "\n",
    "\n",
    "-- Create a route dimension table\n",
    "CREATE TABLE ___(\n",
    "\troute_id INTEGER PRIMARY KEY,\n",
    "    ___ VARCHAR(160) NOT NULL,\n",
    "    ___ VARCHAR(160) NOT NULL,\n",
    "    distance_km ___ NOT NULL,\n",
    "    ___ VARCHAR(160) NOT NULL\n",
    ");\n",
    "-- Create a week dimension table\n",
    "CREATE TABLE ___(\n",
    "\tweek_id INTEGER PRIMARY KEY,\n",
    "    week ___ NOT NULL,\n",
    "    ___ VARCHAR(160) NOT NULL,\n",
    "    ___ ___ NOT NULL\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14dfe04-8663-48f1-8bbc-decb0cd60018",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Create a route dimension table\n",
    "CREATE TABLE route(\n",
    "\troute_id INTEGER PRIMARY KEY,\n",
    "    route_name VARCHAR(160) NOT NULL,\n",
    "    park_name VARCHAR(160) NOT NULL,\n",
    "    distance_km float NOT NULL,\n",
    "    city_name VARCHAR(160) NOT NULL\n",
    ");\n",
    "-- Create a week dimension table\n",
    "CREATE TABLE week(\n",
    "\tweek_id INTEGER PRIMARY KEY,\n",
    "    week int NOT NULL,\n",
    "    month VARCHAR(160) NOT NULL,\n",
    "    year int NOT NULL\n",
    ");\n",
    "\n",
    "\n",
    "\n",
    "Solution ------------------------------------------------------------------------------------------------------------\n",
    "-- Create a route dimension table\n",
    "CREATE TABLE route (\n",
    "\troute_id INTEGER PRIMARY KEY,\n",
    "    park_name VARCHAR(160) NOT NULL,\n",
    "    city_name VARCHAR(160) NOT NULL,\n",
    "    distance_km FLOAT NOT NULL,\n",
    "    route_name VARCHAR(160) NOT NULL\n",
    ");\n",
    "-- Create a week dimension table\n",
    "CREATE TABLE week(\n",
    "\tweek_id INTEGER PRIMARY KEY,\n",
    "    week INTEGER NOT NULL,\n",
    "    month VARCHAR(160) NOT NULL,\n",
    "    year INTEGER NOT NULL\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a9484a-e2c0-4896-a11d-eb3c6c922847",
   "metadata": {},
   "source": [
    "## Querying the dimensional model\n",
    "\n",
    "Here it is! The schema reorganized using the dimensional model:\n",
    "\n",
    "route_dim             runs_fact            week_dim\n",
    "  route_id              route_id             week_id\n",
    "  park_name             week_id              week\n",
    "  city_name             duration_mins        month\n",
    "  distance_km                                year\n",
    "  route_name\n",
    "\n",
    "Let's try to run a query based on this schema. How about we try to find the number of minutes we ran in July, 2019? We'll break this up in two steps. First, we'll get the total number of minutes recorded in the database. Second, we'll narrow down that query to week_id's from July, 2019.\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "\n",
    "    Question 1\n",
    "    Calculate the sum of the duration_mins column.\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    Join week_dim and runs_fact.\n",
    "    Get all the week_id's from July, 2019.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e54c60-db86-4cb6-9036-eb94089224f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT \n",
    "\t-- Select the sum of the duration of all runs\n",
    "\tsum(duration_mins)\n",
    "FROM \n",
    "\truns_fact;\n",
    "    \n",
    "sum\n",
    "1172.1599999999999\n",
    "\n",
    "\n",
    "\n",
    "SELECT \n",
    "\t-- Get the total duration of all runs\n",
    "\tSUM(duration_mins)\n",
    "FROM \n",
    "\truns_fact\n",
    "-- Get all the week_id's that are from July, 2019\n",
    "INNER JOIN ___ ON ___.___ = ___.___\n",
    "WHERE ___ = 'July' and ___ = '2019';\n",
    "\n",
    "\n",
    "SELECT \n",
    "\t-- Get the total duration of all runs\n",
    "\tSUM(duration_mins)\n",
    "FROM \n",
    "\truns_fact\n",
    "-- Get all the week_id's that are from July, 2019\n",
    "INNER JOIN week_dim ON runs_fact.week_id = week_dim.week_id\n",
    "WHERE month = 'July' and year = '2019';\n",
    "\n",
    "sum\n",
    "381.46000000000004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917dd0d5-efd0-4254-b8e7-c0b1dcae68f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_fact\n",
    "\n",
    "duration_mins\tweek_id\troute_id\n",
    "24.5\t601\t101\n",
    "61\t601\t103\n",
    "24.5\t601\t104\n",
    "24.5\t602\t104\n",
    "48\t602\t102\n",
    "23\t602\t101\n",
    "24.5\t603\t104\n",
    "54.96\t603\t106\n",
    "38.4\t603\t103\n",
    "23.75\t603\t104\n",
    "57.6\t604\t106\n",
    "31.8\t604\t105\n",
    "49\t604\t107\n",
    "23.35\t604\t101\n",
    "39.2\t605\t103\n",
    "27.48\t605\t105\n",
    "28.8\t605\t105\n",
    "47.5\t605\t107\n",
    "24\t606\t104\n",
    "53\t606\t102\n",
    "24.5\t606\t101\n",
    "37.36\t606\t103\n",
    "28.8\t607\t105\n",
    "23\t607\t101\n",
    "49\t607\t107\n",
    "45.8\t607\t107\n",
    "24\t608\t101\n",
    "47.5\t608\t102\n",
    "24\t608\t104\n",
    "53\t608\t107\n",
    "24.5\t609\t104\n",
    "37.36\t609\t103\n",
    "24.5\t609\t101\n",
    "\n",
    "\n",
    "week_dim\n",
    "\n",
    "week_id\tweek\tmonth\tyear\n",
    "601\t3\tMay\t2019\n",
    "602\t4\tMay\t2019\n",
    "603\t1\tJune\t2019\n",
    "604\t2\tJune\t2019\n",
    "605\t3\tJune\t2019\n",
    "606\t4\tJune\t2019\n",
    "607\t1\tJuly\t2019\n",
    "608\t2\tJuly\t2019\n",
    "609\t3\tJuly\t2019\n",
    "\n",
    "\n",
    "route_dim\n",
    "route_id\tpark_name\tcity_name\tdistance_km\troute_name\n",
    "101\tProspect Park\tBrooklyn\t5\tSimple Loop\n",
    "102\tProspect Park\tBrooklyn\t10\tGrove Run\n",
    "103\tCentral Park\tNew York City\t8\tResevoir Loop\n",
    "104\tCentral Park\tNew York City\t5\tLake Loop\n",
    "105\tPennypack Park\tPhiladelphia\t6\tPenny Trail\n",
    "106\tPennypack Park\tPhiladelphia\t12\tPenny Trail Extended\n",
    "107\tLiberty State Park\tJersey City\t10\tWater Front Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c5c5f0-8e83-429a-a077-cb6ed87da01b",
   "metadata": {},
   "source": [
    "## Star and snowflake schema\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Congrats on finishing the first chapter.  We're now going to jump in where we left off with the [star schema]. The star schema is the simplest form of the dimensional model.  Some use the term \"star schema\" and \"dimensional model\" interchangeably.  Remember that the star schema is made up of 2 tables: [fact and dimension tables].  Fact tables hold records of metrics that are described further by dimension tables.  Throughout this chapter, we are going to use another bookstore example.  However, this time, you work for a company that sells books in bulk to bookstores across the US and Canada.  You have a database to keep track of book sales.  Lets take a look at the star schema for this database.  \n",
    "\n",
    "                                        dim_book_star\n",
    "                                          book_is    int           PK\n",
    "                                          title      varchar(256)\n",
    "                                          author     varchar(256)\n",
    "                                          publisher  varchar(256)\n",
    "                                          genre      varchar(128)\n",
    "                                                  |\n",
    "                                                 |||\n",
    "dim_store_star                          fact_booksales                   dim_time_star\n",
    "  store_id       int           PK         sales_id      int    PK          time_id      int    PK\n",
    "  store_address  varchar(256)         -   book_id       int    FK -        day          int\n",
    "  city           varchar(128)   -------   time_id       int    FK ------   month        int\n",
    "  state          varchar(128)         -   store_id      int    FK -        quarter      int\n",
    "  country        varchar(128)             sales_amount  float              year         int\n",
    "                                          quantity      int\n",
    "\n",
    "\n",
    "Excluding primary and foreign keys, the fact table holds the sale amount and quantity of books.  Its connected to dimension tables with details on the books sold, the time the sale took place, and the store buying the books.  You may notice the lines connecting these tables have a special pattern.  These lines represent a one-to-many relationship.  For example, a store can be part of many book sales, but one sale can only belong to one store.  The star schema got its name because it tends to look like a star wit its different extension points.  \n",
    "\n",
    "Now that we have a good grasp of the star schema, lets look at the snowflake schema.  The snowflake schema is an extension of the star schema.  Off the bat, we see that it has more tables.  You may not be able to see all the details in the slide, but don't worry it will be broken down in later slides.  \n",
    "\n",
    "[The snowflake schema looks like above diagram, but with other many-to-one tables linked to the dimension tables]\n",
    "\n",
    "\n",
    "dim_author_sf                           dim_book_sf                      dim_genre_sf\n",
    "  author_id    int          PK        -   book_is    int           PK      genre_id    int          PK\n",
    "  author       varchar(256)     -------   title      varchar(256)  ------  genre       varchar(256)\n",
    "                                      -   author     varchar(256)  -\n",
    "                                          publisher  varchar(256)\n",
    "                                          genre      varchar(128)\n",
    "                                                  |\n",
    "                                                 |||\n",
    "dim_store_sf                            fact_booksales                   dim_time_sf\n",
    "  store_id       int           PK         sales_id      int    PK          time_id      int    PK\n",
    "  store_address  varchar(256)         -   book_id       int    FK -        day          int\n",
    "  city_id        int           FK -----   time_id       int    FK ------   month        int    FK\n",
    "       |||                            -   store_id      int    FK -        quarter      int\n",
    "        |                                 sales_amount  float              year         int\n",
    "dim_city_sf                               quantity      int                  |||\n",
    "  city_id        varchar(128)  PK                                             |\n",
    "  city           varchar(128)                                             dim_month_sf\n",
    "  state          varchar(128)  FK                                           month_id   int   PK\n",
    "                                                                            month      int\n",
    "  \n",
    "# *******************************************************************************************************************\n",
    "The information contained in this schema is the same as the star schema.  In fact, the fact table is the same but the way the dimension tables are structured is different.  We see that they extend more, hence its namesake.  [The star schema extend one dimsion, while the snowflake schema extends over more than one dimension].  This is because the dimension tables are normalized.  So what is normalization?  [Normalization is a technique that divides tables into smaller tables and connects them via relationships]  __The goal is to reduce redundancy and increase data integrity__  So how does this happen?  There are several forms of normalization, which we'll delve into later.  __But the basic idea is to identify repeating groups of data and create new tables for them__  Lets go back to our exampleand see how these tables were normalized.  Here is the book dimension in the star schema.  [What could be repeating here?]  Primary key are inherently unique.  For book titles, although there is possible repeat here, it is not common.  On the other hand, authors often publish more than one book, [publishers definitely publish many books], and a lot of books share genres.  We can create new tables for them, and it results in the following snowflake schema.  Do you see how these repeating groups now have their own table?  On to the store dimension.  [City, states, and countries can definitely have more than one book stores within them].  \n",
    "\n",
    "dim_book_star\n",
    "  book_id     int           PK\n",
    "  title       varchar(256)\n",
    "  author      varchar(256)\n",
    "  publisher   varchar(256)\n",
    "  genre       varchar(256)\n",
    "\n",
    "\n",
    "                                         dim_publisher_sf\n",
    "                                           publisher_id     int          PK\n",
    "                                           publisher        varchar(256)\n",
    "                                               |\n",
    "                                              |||\n",
    "dim_author_sf                            dim_book_sf                     -       dim_genre_sf\n",
    "  author_id   int          PK          -   book_id      int          PK  --------  genre_id       int         PK\n",
    "  author      varchar(256)     ---------   title        varchar()256     -         genre          varchar(128)\n",
    "                                       -   author_id    int          FK\n",
    "                                           publisher_id int          FK\n",
    "                                           genre_id     int          FK\n",
    "\n",
    "\n",
    "dim_store_sf\n",
    "  store_id       int         PK\n",
    "  store_address  varchar(256)\n",
    "  city_id        int         FK\n",
    "|||\n",
    "|\n",
    "dim_city_sf\n",
    "  city_id        int         PK\n",
    "  city           varchar(128)\n",
    "  state_id       int         FK\n",
    "|||\n",
    "|\n",
    "dim_state_sf\n",
    "  state_id       int         PK\n",
    "  state          varchar(128)\n",
    "  country_id     int         FK\n",
    "|||\n",
    "|\n",
    "dim_country_sf\n",
    "  country_id     int         PK\n",
    "  country        varchar(128) \n",
    "\n",
    "We can apply same skills to normalize dimension tables representing the book stores.  Do you notice that the way we structure these repeating groups is a bit different from the book dimension?  [An author can have published in different genres and with various publishers, hence why they were different dimemsions]. However, a city stays in the same state and country; thus they extend each other over 3 dimensions.  The same is done on the time dimension.  A day is a part of a month that is a part of a quarter, and so on.  \n",
    "\n",
    "And then we put all the normalized dimensions together to get the snowflake schema.  Geting the hang of this? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76cff00-4adf-4681-9608-6977c337f4e1",
   "metadata": {},
   "source": [
    "## Running from star to snowflake\n",
    "\n",
    "Remember your running database from last chapter?\n",
    "\n",
    "route_dim             runs_fact            week_dim\n",
    "  route_id              route_id             week_id\n",
    "  park_name             week_id              week\n",
    "  city_name             duration_mins        month\n",
    "  distance_km                                year\n",
    "  route_name\n",
    "\n",
    "After learning about the snowflake schema, you convert the current star schema into a snowflake schema. To do this, you normalize route_dim and week_dim. Which option best describes the resulting new tables after doing this?\n",
    "\n",
    "The tables runs_fact, route_dim, and week_dim have been loaded.\n",
    "Instructions\n",
    "50 XP\n",
    "Instructions\n",
    "50 XP\n",
    "Possible Answers\n",
    "\n",
    "    week_dim is extended two dimensions with new tables for month and year. route_dim is extended one dimension with a new table for city.\n",
    "    week_dim is extended two dimensions with new tables for month and year. route_dim is extended two dimensions with new tables for city and park.\n",
    "#    week_dim is extended three dimensions with new tables for week, month and year. route_dim is extended one dimension with new tables for city and park.\n",
    "\n",
    "Hint\n",
    "\n",
    "    Look through the tables route_dim and week_dim to see what columns hold repeated information.\n",
    "    Remember how dimensions can connect to each other via foreign keys, like the city, state, and country variables in the slides?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4a2639-0273-4a07-997d-358beafeba68",
   "metadata": {},
   "source": [
    "## Adding foreign keys\n",
    "\n",
    "Foreign key references are essential to both the snowflake and star schema. When creating either of these schemas, correctly setting up the foreign keys is vital because they connect dimensions to the fact table. They also enforce a one-to-many relationship, because unless otherwise specified, a foreign key can appear more than once in a table and primary key can appear only once.\n",
    "\n",
    "The fact_booksales table has three foreign keys: book_id, time_id, and store_id. In this exercise, the four tables that make up the star schema below have been loaded. However, the foreign keys still need to be added.\n",
    "\n",
    "                                        dim_book_star\n",
    "                                          book_is    int           PK\n",
    "                                          title      varchar(256)\n",
    "                                          author     varchar(256)\n",
    "                                          publisher  varchar(256)\n",
    "                                          genre      varchar(128)\n",
    "                                                  |\n",
    "                                                 |||\n",
    "dim_store_star                          fact_booksales                   dim_time_star\n",
    "  store_id       int           PK         sales_id      int    PK          time_id      int    PK\n",
    "  store_address  varchar(256)         -   book_id       int    FK -        day          int\n",
    "  city           varchar(128)   -------   time_id       int    FK ------   month        int\n",
    "  state          varchar(128)         -   store_id      int    FK -        quarter      int\n",
    "  country        varchar(128)             sales_amount  float              year         int\n",
    "                                          quantity      int\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    In the constraint called sales_book, set book_id as a foreign key.\n",
    "    In the constraint called sales_time, set time_id as a foreign key.\n",
    "    In the constraint called sales_store, set store_id as a foreign key.\n",
    "\n",
    "Hint\n",
    "\n",
    "    The table with the foreign keys, fact_booksales, is the table that will be altered.\n",
    "    The (___) holds the column name that will serve as the foreign key.\n",
    "    The primary keys of the dimension tables need to be referenced as foreign keys, therefore, a dimension table should come after REFERENCES.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ac6be-b00a-464f-8f8c-b19c89f5fdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Add the book_id foreign key\n",
    "ALTER TABLE ___ ADD CONSTRAINT sales_book\n",
    "    FOREIGN KEY (___) REFERENCES ___ (___);\n",
    "    \n",
    "-- Add the time_id foreign key\n",
    "ALTER TABLE ___ ___ ___ ___\n",
    "    ___ ___ (___) REFERENCES ___ (___);\n",
    "    \n",
    "-- Add the store_id foreign key\n",
    "___ ___ ___ ___ ___ ___\n",
    "    ___ ___ (___) ___ ___ (___);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d93a2-62f1-4bac-b227-68d304168bb2",
   "metadata": {},
   "source": [
    "# *******************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa30b9-33ab-4c77-bc44-bdb261e0dda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Add the book_id foreign key\n",
    "ALTER TABLE fact_booksales ADD CONSTRAINT sales_book\n",
    "    FOREIGN KEY (book_id) REFERENCES dim_book_star (book_id);\n",
    "    \n",
    "-- Add the time_id foreign key\n",
    "ALTER TABLE fact_booksales ADD CONSTRAINT sales_time\n",
    "    FOREIGN KEY (time_id) REFERENCES dim_time_star (time_id);\n",
    "    \n",
    "-- Add the store_id foreign key\n",
    "ALTER TABLE fact_booksales ADD CONSTRAINT sales_store\n",
    "    FOREIGN KEY (Store_id) REFERENCEs dim_store_star (store_id);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5d9706-3bc6-4d49-9236-b34d20882f46",
   "metadata": {},
   "source": [
    "## Extending the book dimension\n",
    "\n",
    "In the video, we saw how the book dimension differed between the star and snowflake schema. The star schema's dimension table for books, dim_book_star, has been loaded and below is the snowflake schema of the book dimension.\n",
    "\n",
    "                                         dim_publisher_sf\n",
    "                                           publisher_id     int          PK\n",
    "                                           publisher        varchar(256)\n",
    "                                               |\n",
    "                                              |||\n",
    "dim_author_sf                            dim_book_sf                     -       dim_genre_sf\n",
    "  author_id   int          PK          -   book_id      int          PK  --------  genre_id       int         PK\n",
    "  author      varchar(256)     ---------   title        varchar()256     -         genre          varchar(128)\n",
    "                                       -   author_id    int          FK\n",
    "                                           publisher_id int          FK\n",
    "                                           genre_id     int          FK\n",
    "\n",
    "\n",
    "In this exercise, you are going to extend the star schema to meet part of the snowflake schema's criteria. Specifically, you will create dim_author from the data provided in dim_book_star.\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "\n",
    "    Question 1\n",
    "    Create dim_author with a column for author.\n",
    "    Insert all the distinct authors from dim_book_star into dim_author.\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    Alter dim_author to have a primary key called author_id.\n",
    "    Output all the columns of dim_author.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74e9327-838b-49fb-b918-bd93ecac08b2",
   "metadata": {},
   "source": [
    "# *******************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31505dce-63fd-4ec9-97f5-e14637aced79",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Create dim_author with an author column\n",
    "CREATE TABLE dim_author (\n",
    "    author VARCHAR(256)  NOT NULL\n",
    ");\n",
    "\n",
    "-- Insert authors into the new table\n",
    "INSERT INTO dim_author\n",
    "SELECT DISTINCT author FROM dim_book_star;\n",
    "\n",
    "\n",
    "\n",
    "-- Create a new table for dim_author with an author column\n",
    "CREATE TABLE dim_author(\n",
    "    author varchar(256)  NOT NULL\n",
    ")\n",
    "\n",
    "-- Insert authors\n",
    "INSERT INTO dim_author\n",
    "SELECT DISTINCT author FROM dim_book_star\n",
    "\n",
    "-- Add a primary key ------------------------------------------------------------------------------------------------\n",
    "ALTER TABLE dim_author ADD COLUMN author_id SERIAL PRIMARY KEY\n",
    "\n",
    "-- Output the new table\n",
    "SELECT * FROM dim_author"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87536e2c-b0b1-4ae1-8a9b-59d183b7c406",
   "metadata": {},
   "source": [
    "## Normalized and denormalized databases\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now that we have a grasp on normalization, lets talk about why we would want to normalize a database.  You should be familiar th these 2 schemars by now.  The star schema and the snowflake schema.  They both storing fictional company data on the sales of books in bulk to stores across the US and Canada.  On the left you have the [star schema with denormalized dimension tables].  And on the right, you have the [snowflake schema with normalized dimension tables].  The normalized database looks way more complicated.  Adn it is in some ways.  \n",
    "\n",
    "For example, lets say you wanted to get all the quantity of all books by Octavia E. Bulter sold in Vancouver in Q4 of 2018.   Based on the denormalized schema, you can run the following query to accomplish this.  It composed of 3 joins, which makes sense based on the 3 dimension tables in the starschema.  \n",
    "\n",
    "\n",
    "SELECT SUM(quantity) FROM fact_booksales \n",
    "    INNER JOIN dim_store_star ON fact_booksales.store_id = dim_store_star.store_id\n",
    "    INNER JOIN dim_book_star ON fact_booksales.book_id = dim_book_star.book_id\n",
    "    INNER JOIN dim_time_star ON fact_booksales.time_id = dim_time_star.time_id\n",
    "WHERE \n",
    "    dim_store_star.city = 'Vancouver' AND dim_book_star.author = 'Octavia E. Bulter' AND\n",
    "    dim_time_star.yesr = 2018 and dim_time_star.quarter = 4;\n",
    "\n",
    "\n",
    "And what would the query look like on the normalized schema?  A lot longer.  There is a total of 8 inner joins.  This makes sense based on the snowflake schema diagram.  The normalized snowflake schema has considerable more tables.  this means [more joins, which means slower queries.  \n",
    "\n",
    "\n",
    "SELECT \n",
    "    SUM(fact_booksales.quantity)\n",
    "FROM\n",
    "    fact_booksales\n",
    "INNER JOIN dim_store_sf ON fact_booksales.store_id = dim_store_sf.store_id\n",
    "INNER JOIN dim_city_sf ON dim_store_sf.city_id = dim_city_sf.city_id\n",
    "INNER JOIN dim_book_sf ON fact_booksales.book_id = dim_book_sf\n",
    "INNER JOIN dim_author_sf ON dim_book_sf.author_id = dim_author_sf.author_id\n",
    "INNER JOIN dim_time_sf ON fact_booksales.time_id = dim_time_sf.time_id\n",
    "INNER JOIN dim_month_sf ON dim_time_sf.month_id = dim_month_sf.month_id\n",
    "INNER JOIN dim_quarter_sf ON dim_time_sf.quarter_id = dim_quarter_sf.quarter_id\n",
    "INNER JOIN dim_year_sf ON dim_quarter_sf.year_id = dim_year_sf.year_id\n",
    "Where \n",
    "    dim_store_star.city = 'Vancouver' AND dim_book_star.author = 'Octavia E. Bulter' AND\n",
    "    dim_time_star.yesr = 2018 and dim_time_star.quarter = 4;\n",
    "\n",
    "\n",
    "[So why would we want to normalize a database?  Normalization saves space].  This isn't intuitive seeing how normalized database have more tables.  Lets take a look at the store table in our denormalized database.  Here we see a lot of information in bold - such as [USA], [California], [New York], and [Brooklyn].  This type of denormalized structure enables a lot of data redundancy.  \n",
    "\n",
    "-------------------------------------------------------------------\n",
    "id  | store_address    | city          | state       | country\n",
    "1   | 67 First St      | Brooklyn      | New York    | USA\n",
    "2   | 12 Jefferson Rd  | San Francisco | California  | USA\n",
    "3   | 90 Coolidge St   | Log Angeles   | California  | USA\n",
    "4   | 85 Main Ave      | Brooklyn      | New York    | USA\n",
    "5   | 123 Bedford St   | Brooklyn      | New York    | USA\n",
    "\n",
    "\n",
    "If we normalize that previous schema, we got this: we see that although we are using more tables, there is no data redundancy.  The string \"Brooklyn\" is only stored once.  And the state records are stored separately because many cities share the same state, and country.  We don't need to repeat that information, instead, we can one record holding the string Califoria.  [Here we see how normalization eliminates data redundancy]. \n",
    "\n",
    "\n",
    "dim_store_sf                          dim_city_sf                              dim_state_sf\n",
    "---------------------------------     ------------------------------------     ------------------------------------\n",
    "id  | store_address    | city_id      city_id | city_name      | state_id      state_id  | state       | country_id\n",
    "1   | 67 First St      | 2            2       | Brooklyn       | 43            43        | New York    | 121\n",
    "2   | 12 Jefferson Rd  | 3            3       | San Francisco  | 36            36        | California  | 121\n",
    "3   | 90 Coolidge St   | 4            4       | Log Angeles    | 36                       \n",
    "4   | 85 Main Ave      | 2                                                               \n",
    "5   | 123 Bedford St   | 2                                                               \n",
    "\n",
    "\n",
    "__Normalization ensures better data integrity through its design__.  First it enforces data consistency.  Data entry can get messy, and at times people will fill out fields differently.  For example, when referring to California, someone might initials CA.  [Since the states are already entered in a table, we can ensure naming conventions through referential integrity].  Secondly, because duplicates are reduced, [modification of any data becomes safer and simpler].  Say in the previous example, you wanted to update the spelling of a state - you wouldn't have to find each record referring to the state, instead, you could make that change in the states table by altering one record.  From there, you can be confident that the new spelling will be enacted for all stores in that state.  Lastly, [since tables are smaller and organized more by object, its easier to alter the database schema].  You can extend a smaller table without having to alter a larger table holding all the vital data.  \n",
    "\n",
    "\n",
    "\n",
    "To recap, here are the pros and cons of normalization.  Now normalization seems sppealing, especially for database maintenance.  [However, normalization requires a lot more joins making queries more complicated], which can make indexing and reading of data slower.  Deciding between normalization and denormalization comes down to how read- or write- intensive your database is going to be.  Remember OLTP and OLAP?  Can you guess which [OLTP] prefers normalization?  \n",
    "\n",
    "\n",
    "[OLTP is write-intensive meaning we're updating and writing often.  Normalization makes sense because we want to add data quickly and consistently].  OLAP is read-intensive because we're running analytics on the data.  This means we want to prioritize quicker read queries.  Remember how much more joins the normalized query had over the denormalized query?  8 joins to 3 joins.  OLAP should try to avoid that.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282a269b-27d3-4180-bc63-74f43d9e292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT COUNT(a.book_id) \n",
    "FROM fact_booksales a LEFT JOIN dim_store_star b ON a.store_id = b.store_id\n",
    "LEFT JOIN dim_time_star c ON a.time_id = c.time_id\n",
    "LEFT JOIN dim_book_star d ON a.book_id = d.book_id\n",
    "WHERE d.publisher = 'Octavia E. Bulter' AND b.city = 'Vancouver' AND c.year = 2018 AND c.quarter = 4\n",
    "\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "SELECT SUM(quantity) FROM fact_booksales \n",
    "    INNER JOIN dim_store_star ON fact_booksales.store_id = dim_store_star.store_id\n",
    "    INNER JOIN dim_book_star ON fact_booksales.book_id = dim_book_star.book_id\n",
    "    INNER JOIN dim_time_star ON fact_booksales.time_id = dim_time_star.time_id\n",
    "WHERE \n",
    "    dim_store_star.city = 'Vancouver' AND dim_book_star.author = 'Octavia E. Bulter' AND\n",
    "    dim_time_star.yesr = 2018 and dim_time_star.quarter = 4;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f6e245-86d1-44f2-97e1-724bd51586e4",
   "metadata": {},
   "source": [
    "## Querying the star schema\n",
    "\n",
    "The novel genre hasn't been selling as well as your company predicted. To help remedy this, you've been tasked to run some analytics on the novel genre to find which areas the Sales team should target. To begin, you want to look at the total amount of sales made in each state from books in the novel genre.\n",
    "\n",
    "Luckily, you've just finished setting up a data warehouse with the following star schema:\n",
    "\n",
    "                                        dim_book_star\n",
    "                                          book_is    int           PK\n",
    "                                          title      varchar(256)\n",
    "                                          author     varchar(256)\n",
    "                                          publisher  varchar(256)\n",
    "                                          genre      varchar(128)\n",
    "                                                  |\n",
    "                                                 |||\n",
    "dim_store_star                          fact_booksales                   dim_time_star\n",
    "  store_id       int           PK         sales_id      int    PK          time_id      int    PK\n",
    "  store_address  varchar(256)         -   book_id       int    FK -        day          int\n",
    "  city           varchar(128)   -------   time_id       int    FK ------   month        int\n",
    "  state          varchar(128)         -   store_id      int    FK -        quarter      int\n",
    "  country        varchar(128)             sales_amount  float              year         int\n",
    "                                          quantity      int\n",
    "                                          \n",
    "\n",
    "The tables from this schema have been loaded.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "[    Select \"state\" from the appropriate table and the total \"sales_amount\".]\n",
    "    Complete the JOIN on book_id.\n",
    "    Complete the JOIN to connect the dim_store_star table\n",
    "    Conditionally select for books with the genre novel.\n",
    "    Group the results by state.\n",
    "\n",
    "Hint\n",
    "\n",
    "    The JOINs are done on the shared foreign keys between tables. In this schema, these keys end with _id.\n",
    "    The fact_booksales table needs to be joined with dim_store_star and dim_book_star.\n",
    "    The values of genre and state are held in dim_book_star and dim_store_star, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f79147-e356-4fdf-a80e-2042df98d5f2",
   "metadata": {},
   "source": [
    "<img src=\"https://assets.datacamp.com/production/repositories/5311/datasets/75bc5e6de9085df105fd4cd1af69752786096617/book-star.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de615d0d-21e0-4dcd-a20d-d7901f878ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Output each state and their total sales_amount\n",
    "SELECT dim_store_star.state, SUM(sales_amount)\n",
    "FROM fact_booksales\n",
    "\t-- Join to get book information\n",
    "    JOIN dim_book_star on fact_booksales.book_id = dim_book_star.book_id\n",
    "\t-- Join to get store information\n",
    "    JOIN dim_store_star on dim_store_star.store_id = fact_booksales.store_id\n",
    "-- Get all books with in the novel genre\n",
    "WHERE  \n",
    "    dim_book_star.genre = 'novel'\n",
    "-- Group results by state\n",
    "GROUP BY\n",
    "    dim_store_star.state;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e66d38-b6b2-4132-bf79-9af59a6f8bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Output each state and their total sales_amount\n",
    "SELECT dim_store_star.state, sum(sales_amount)\n",
    "FROM fact_booksales\n",
    "\t-- Join to get book information\n",
    "    JOIN dim_book_star on fact_booksales.book_id = dim_book_star.book_id\n",
    "\t-- Join to get store information\n",
    "    JOIN dim_store_star on fact_booksales.store_id = dim_store_star.store_id\n",
    "-- Get all books with in the novel genre\n",
    "WHERE  \n",
    "    dim_book_star.genre = 'novel'\n",
    "-- Group results by state\n",
    "GROUP BY\n",
    "    dim_store_star.state;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bf065e-04f4-49f4-a347-f817c4f21caf",
   "metadata": {},
   "source": [
    "## Querying the snowflake schema\n",
    "\n",
    "Imagine that you didn't have the data warehouse set up. Instead, you'll have to run this query on the company's operational database, which means you'll have to rewrite the previous query with the following snowflake schema:\n",
    "\n",
    "\n",
    "dim_author_sf                           dim_book_sf                      dim_genre_sf\n",
    "  author_id    int          PK        -   book_is    int           PK      genre_id    int          PK\n",
    "  author       varchar(256)     -------   title      varchar(256)  ------  genre       varchar(256)\n",
    "                                      -   author     varchar(256)  -\n",
    "                                          publisher  varchar(256)\n",
    "                                          genre      varchar(128)\n",
    "                                                  |\n",
    "                                                 |||\n",
    "dim_store_sf                            fact_booksales                   dim_time_sf\n",
    "  store_id       int           PK         sales_id      int    PK          time_id      int    PK\n",
    "  store_address  varchar(256)         -   book_id       int    FK -        day          int\n",
    "  city_id        int           FK -----   time_id       int    FK ------   month        int    FK\n",
    "       |||                            -   store_id      int    FK -        quarter      int\n",
    "        |                                 sales_amount  float              year         int\n",
    "dim_city_sf                               quantity      int                  |||\n",
    "  city_id        varchar(128)  PK                                             |\n",
    "  city           varchar(128)                                             dim_month_sf\n",
    "  state          varchar(128)  FK                                           month_id   int   PK\n",
    "                                                                            month      int\n",
    "  \n",
    "\n",
    "The tables in this schema have been loaded. Remember, our goal is to find the amount of money made from the novel genre in each state.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Select state from the appropriate table and the total sales_amount.\n",
    "    Complete the two JOINS to get the genre_id's.\n",
    "    Complete the three JOINS to get the state_id's.\n",
    "    Conditionally select for books with the genre novel.\n",
    "    Group the results by state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411fabf6-aadc-4e97-8fa2-71459c98bf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Output each state and their total sales_amount\n",
    "SELECT ___.___, ___(___)\n",
    "FROM ___\n",
    "    -- Joins for genre\n",
    "    JOIN dim_book_sf on ___.___ = ___.___\n",
    "    JOIN dim_genre_sf on ___.___ = ___.___\n",
    "    -- Joins for state \n",
    "    JOIN ___ on ___.store_id = ___.store_id \n",
    "    JOIN ___ on ___.city_id = ___.city_id\n",
    "\tJOIN ___ on  ___.state_id = ___.state_id\n",
    "-- Get all books with in the novel genre and group the results by state\n",
    "WHERE  \n",
    "    ___.___ = '___'\n",
    "GROUP BY\n",
    "    ___.___;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ce69d5-9ebd-4f29-a9f6-f3b9334836a3",
   "metadata": {},
   "source": [
    "<img src=\"https://assets.datacamp.com/production/repositories/5311/datasets/0c83b37648663810b40ef2b13503e64a56bbb856/book-snowflake-copy.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0c91b2-022d-4efb-b09b-ac5ffffbe53d",
   "metadata": {},
   "source": [
    "## Updating countries\n",
    "\n",
    "Going through the company data, you notice there are some inconsistencies in the store addresses. These probably occurred during data entry, where people fill in fields using different naming conventions. This can be especially seen in the [country] field, and you decide that countries should be represented by their abbreviations. The only countries in the database are Canada and the United States, which should be represented as [USA] and [CA].\n",
    "\n",
    "In this exercise, you will compare the records that need to be updated in order to do this task on the star and snowflake schema. dim_store_star and dim_country_sf have been loaded.\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "\n",
    "    Question 1\n",
    "    Output all the records that need to be updated in the star schema so that countries are represented by their abbreviations.\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7ba76f-2d01-4b93-b31a-b2ca5854253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Output records that need to be updated in the star schema\n",
    "SELECT * FROM ___\n",
    "WHERE ___ != 'USA' AND ___ !='CA';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16dbcad-afd7-4da4-886b-5619dc957de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1f644a-3b0c-4bf2-b919-42d28d048394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fddbe70-278c-4566-8715-f77d2226c133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24f6687-9b9e-439d-9ec5-e4ae3997b9ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173c2c66-b1d4-4a10-bc25-a7dc348f1245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb20e54-5a0a-4c99-8fd5-74b6512230fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7693f2-95aa-4dcc-ab18-7b372553da4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be53629-28ea-4af1-9e04-23db7107d64c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac85077-12d8-445a-b3e8-751c38b456f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02438922-e44f-4ffa-934e-0b802026a2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02791159-a945-4094-b340-97c5e1367f91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d2e179-e17b-4423-8f25-adc925a69c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b6f5a7-9c71-4bf2-8d72-3fb30b67f0b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a934687-2538-46e4-9c07-abf68489f5d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651d6342-4ea0-4022-9e6a-0951272bcef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee0b74f-d4a0-4585-8e44-1dc640c5a2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b8cd87-b8a9-42cd-bc45-002140922c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9694616c-d3e7-42be-932d-8b44a32e0f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47f2dfb-cc03-471e-8151-82a4ce9927bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc543945-3361-49f9-ad49-542e42cc7755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cc60a6-5501-4d6e-81a5-3b871ae85fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b429e1f-9b9a-4237-9241-3d834299b1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec48346c-3ab7-44ec-9fd7-4ba19b98d537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e9354c-3d87-4286-91c2-db1ad4338171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ec1553-55cf-4c22-bd3a-3536b39ba808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c71ca2-a02e-4a39-93b4-fb84d74c7423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647549cf-c3f9-4a74-90ef-5dff2fa7d43b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed703d43-a5a9-443e-8fce-67de40b8f4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f3c0e3-e2fb-44a1-98a9-9822c83f0ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552eded5-7b1f-4ea8-bb4e-03f04e3d9d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa2248f-6f95-421a-8eb3-153428979729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffd0d71-55a8-4dcf-9fe9-5f42dc7adfa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f875bd88-be41-47e9-90f6-fe44968a6bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b8ae6b-c83d-47d6-a603-145d02546404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7852dc31-179b-4e0e-a497-aee946fc2d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8cedb0-99d6-4d0d-ae08-a4c5f3edf6af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9276286-028e-4c5e-b443-e95e03fd2a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9639c2d-3f89-4e6b-a4cd-e628a53302aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d91633-347d-488a-80bf-c4e09057d6cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2d368d-ca9a-4ce6-8314-c3507f47caa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef9a601-edca-4c21-bef0-4ba260e3ecc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5f135f-dbcf-4d99-90b5-889cf3812ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d12be5-1c13-45e8-a99e-8937b318fc96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37feb076-875b-40a1-81d1-8a006e9b6163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61493f8c-83eb-4639-abc3-85ffa60280df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9038bec1-1c45-4a91-9f53-2dd2b802236c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cc0d2e-6154-4bef-bdd0-d565a7c4f167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782fcf0f-4f5c-4a47-85a8-edd9ba4b9ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2778f39d-5aa1-4e44-a709-3d2043d2510b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee257bf-bb35-40fc-9a12-bf4e8abdf9de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb972b5-5a64-4da3-b4d1-4df8324ecf24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5495c8-1308-4055-ad02-6508569ee800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12c9f40-2d8c-44a2-ae40-d6008e89afb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c06d14-c188-4cc6-bc3d-8bf72c647edc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdca797c-8074-4de7-8160-dcf0e33e9b11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215b51f5-da6a-42c0-a2fe-77cc8e339c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fa7527-1924-415a-aaea-b1875d9e8f66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1904ab37-9bc3-4129-a67b-a42d9b6a5940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d8bfcf-e92b-4602-8616-833d90d50a53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6825074f-be6f-43a3-b360-9ed88f0e7b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe41589-00fa-4f22-a698-abe66f60ffd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d75cfb-94ab-4c81-9181-d2c4e2c88679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcb097e-51dd-418a-bdf5-0195012a8f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc30f03-afe2-44a8-9580-41f4e56c7991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3870bf8-b60d-40fa-9573-186675e7a79f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f214e7-6f7d-472b-b9ba-048e8e5ff1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff09158c-9356-4d0c-8654-64dcdf9124a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdaf51a-6e23-46b0-9a97-0edbee64add2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ff3e22-e42a-44ed-91c9-c85bb817937c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d523c4-dad0-4891-97a0-c743e389955a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d83fece-4fd1-4fa3-9490-99aeb7ca2fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddae06c-2abe-4d93-8d2e-de15c98ece7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ef480a-04d6-4651-b4b0-9ba567ea8af8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a918d718-9468-4b5e-b253-df25ee9587b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dff143-2dfc-40dc-be2b-e2c847c6209b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57e79b5-107b-4e72-a9e5-9a758b40a607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af8288e-f0d9-4b15-9c4f-0307b8de736e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661f2bdc-73b8-4f23-9ea7-7116231f82c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ddaf1b-524e-49d8-84f9-891043d8f435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0760a6-06a8-4ae0-be87-c92b0aa2ae93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0be024-c175-4a9b-9d76-6b40ad8705f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
