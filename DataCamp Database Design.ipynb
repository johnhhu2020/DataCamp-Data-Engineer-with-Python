{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83ab510a-ffd1-41ba-bcd0-5ce173c74d70",
   "metadata": {},
   "source": [
    "## Database Design\n",
    "\n",
    "\n",
    "Do not take this course, the institutor talking with no logical, and she think where she talks, just waste your time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73715687-e19b-4c10-8b86-83148da7e7b8",
   "metadata": {},
   "source": [
    "## Course Description\n",
    "\n",
    "A good database design is crucial for a high-performance application.  Just like you wouldn't start building a house without the benefit of a blueprint, you need to think about how your data will be stored beforehand.  Taking the time to design a database saves time and frustration later on, and a well-designed database ensures ease of access and retrieval of information.  While choosing a design, a lot of considerations have to be accounted for.  In this course, you'll learn how to process, store, and organize data in an efficient way.  You'll see how to structure data through normalization and present your data with views.  [Finally, you'll learn how to manage your database and all of this will be done on a variety of datasets from book sales, car rentals, to music reviews.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bc3db9-7114-4d0a-bfbe-996ed04cec8c",
   "metadata": {},
   "source": [
    "##  Processing, Storing, and Organizing Data\n",
    "Free\n",
    "0%\n",
    "\n",
    "Start your journey into database design by learning about the two approaches to data processing, OLTP and OLAP. In this first chapter, you'll also get familiar with the different forms data can be stored in and learn the basics of data modeling.\n",
    "\n",
    "    OLTP and OLAP    50 xp\n",
    "    OLAP vs. OLTP    100 xp\n",
    "    Which is better?    50 xp\n",
    "    Storing data    50 xp\n",
    "    Name that data type!    100 xp\n",
    "    Ordering ETL Tasks    100 xp\n",
    "    Recommend a storage solution    50 xp\n",
    "    Database design    50 xp\n",
    "    Classifying data models    100 xp\n",
    "    Deciding fact and dimension tables    100 xp\n",
    "    Querying the dimensional model    100 xp \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c475807f-82ee-40fe-81bb-c5c63c87f8d9",
   "metadata": {},
   "source": [
    "##  Database Schemas and Normalization\n",
    "0%\n",
    "\n",
    "In this chapter, you will take your data modeling skills to the next level. You'll learn to implement star and snowflake schemas, recognize the importance of normalization and see how to normalize databases to different extents.\n",
    "\n",
    "    Star and snowflake schema    50 xp\n",
    "    Running from star to snowflake    50 xp\n",
    "    Adding foreign keys    100 xp\n",
    "    Extending the book dimension    100 xp\n",
    "    Normalized and denormalized databases    50 xp\n",
    "    Querying the star schema    100 xp\n",
    "    Querying the snowflake schema    100 xp\n",
    "    Updating countries    100 xp\n",
    "    Extending the snowflake schema    100 xp\n",
    "    Normal forms    50 xp\n",
    "    Converting to 1NF    100 xp\n",
    "    Converting to 2NF    100 xp\n",
    "    Converting to 3NF    100 xp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34ce575-1337-4f2d-96ec-0dd810d5f507",
   "metadata": {},
   "source": [
    "##  Database Views\n",
    "0%\n",
    "\n",
    "Get ready to work with views! In this chapter, you will learn how to create and query views. On top of that, you'll master more advanced capabilities to manage them and end by identifying the difference between materialized and non-materialized views.\n",
    "\n",
    "    Database views    50 xp\n",
    "    Tables vs. views    100 xp\n",
    "    Viewing views    100 xp\n",
    "    Creating and querying a view    100 xp\n",
    "    Managing views    50 xp\n",
    "    Creating a view from other views    100 xp\n",
    "    Granting and revoking access    100 xp\n",
    "    Updatable views    50 xp\n",
    "    Redefining a view    100 xp\n",
    "    Materialized views    50 xp\n",
    "    Materialized versus non-materialized    100 xp\n",
    "    Creating and refreshing a materialized view    100 xp\n",
    "    Managing materialized views    50 xp \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17460a77-bdaa-4029-99e5-f2d55294d4e8",
   "metadata": {},
   "source": [
    "##  Database Management\n",
    "0%\n",
    "\n",
    "This final chapter ends with some database management-related topics. You will learn how to grant database access based on user roles, how to partition tables into smaller pieces, what to keep in mind when integrating data, and which DBMS fits your business needs best.\n",
    "\n",
    "    Database roles and access control    50 xp\n",
    "    Create a role    100 xp\n",
    "    GRANT privileges and ALTER attributes    100 xp\n",
    "    Add a user role to a group role    100 xp\n",
    "    Table partitioning    50 xp\n",
    "    Reasons to partition    50 xp\n",
    "    Partitioning and normalization    100 xp\n",
    "    Creating vertical partitions    100 xp\n",
    "    Creating horizontal partitions    100 xp\n",
    "    Data integration    50 xp\n",
    "    Data integration do's and dont's    100 xp\n",
    "    Analyzing a data integration plan    50 xp\n",
    "    Picking a Database Management System (DBMS)    50 xp\n",
    "    SQL versus NoSQL    50 xp\n",
    "    Choosing the right DBMS    100 xp \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2c8a4e-2a35-4541-8190-a51d96700e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e41e95a-473a-4137-a2cf-131c92c4cf47",
   "metadata": {},
   "source": [
    "## OLTP and OLAP\n",
    "\n",
    "\n",
    "\n",
    "Hello, my name is Lis, I'm a Curriculum Manager here at DataCamp.  This course will be talking about database design.  So what does that entail exactly?  To put it simply, in this course we are asking the question: [How should we organize and manage data?]  To answer this, we have to consider the different schemas, management options, and objects that make up a database.  Some examples are listed here, and they are covered throughout the course.  These topics all affect the way data is stored and accessed.  Some enable faster query speeds.  Some take up less memory than others.  And notablely, some cost more money than others.  \n",
    "\n",
    "\n",
    "    Schemas: How should my daya be logically organized?\n",
    "    Normalization: Should my data have minimal dependency and redundancy?\n",
    "    Views: What joins will be done most often?\n",
    "    Access control: Should all users of the data have the same level of access?\n",
    "    DBMS: How do I pick between all the SQL and noSQL options?\n",
    "    and more?\n",
    "    \n",
    "And as we will soon find out in this course, there is no one right answer to this motivating question.  It will come down to how the data will be used.  Now lets dive in.  \n",
    "\n",
    "\n",
    "OLTP and OLAP are approaches to processing data, and they will be referenced throughout this course.  They help define the way data is going to flow, be structured, and stored.  If you figure out which fits your business case, desiging your database will be much easier.  [OLTP stands for Online Transaction Processing].  [OLAP stands for Online Analytical Processing].  As the name hint, the OLTP approach is oriented around transactions, while the other is oriented around analytics.  Before going into formal definitions, lets look at some use case of each.  Say you are in charge of data management at a bookstore.  You would use an OLTP approach to keep track of the prices of books, while to analyze the most profitable books, an OLAP approach woule be more appropriate.  [To keep track all customer transactions, you would use an OLTP approach to insert sales as customers finish paying].  However, if you wanted to do sophisticated analysis on sales, like most loyal customers - you would use OLAP.  An OLTP database would be used to track when when employees have worked, while to run an analysis on who deserves employee of the month, you would need to switch over to OLAP.  Are you starting to see the diff?  [OLTP focus on supporting day-to-day operations, while OLAP tasks are vaguer and focus on business decision making]  \n",
    "\n",
    "[The OLTP systems are application-oriented, like for bookkeeping for example.  OLAP systems are oriented around a certain subject that's under analysis, like last quarter's book sales].  The data in OLTP systems can be seen as a current snapshot of transactions that are archived often.  The data in OLAP systems tend to be data from over a large period of time that has been consolidated for long-term analysis.  This means OLAP tends to have more data than OLTP.  As we saw in the bookstore example, the commonly executed OLTP queries are simpler and require a quick query or update.  On the other hand, OLAP systems used for analysis require more complex queries.  In terms of how these approaches are being used, OLTP systems are used by more people throughout a company and even a company's customers, while OLAP systems are typically used by only analysts and data scientists at a company.  \n",
    "\n",
    "\n",
    "[OLTP and OLAP systems work together, in fact, they need each other.  OLTP data is usually stored in an operational database that is pulled and cleaned to create an OLAP data warehouse].(But how, we want to tract the user experience, we want to track the business functions)  We'll get into data warehouses and other storage solutions in the next video.  Without transactional data, no analyses can be done in the first place.  Analyses from OLAP systems are used to inform business practice and day-to-day activity, thereby influencing the OLTP databases.  \n",
    "\n",
    "\n",
    "To wrap up, here is what you should take away from this video.  Before implementing anything, figure out your business requirements because there are many design decisions you'll have to make.  The way you set up your database now wil affect how it can be effectively used in the future.  Start by figuring out if you need an OLAP or OLTP approach, or perhaps both.  You should now be comfortable with the differences between both.  These are the two most common approaches.  However, they are not exhaustive, but they are an excellent start to get you on the right path to designing your database.  In later videos, we'll learn more about the technical difference between both approaches.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57576fa4-79af-4027-b9bc-273f2e01da68",
   "metadata": {},
   "source": [
    "## OLAP vs. OLTP\n",
    "\n",
    "You should now be familiar with the differences between OLTP and OLAP. In this exercise, you are given a list of cards describing a specific approach which you will categorize between OLAP and OLTP.\n",
    "Instructions\n",
    "100XP\n",
    "\n",
    "    Categorize the cards into the approach that they describe best.\n",
    "\n",
    "OLAP:\n",
    "    Typically use a data warehouse\n",
    "    Helps businesses with decision making and problem solving\n",
    "    Queries a large amount of data\n",
    "\n",
    "OLTP:\n",
    "    Data is inserted and updated more often\n",
    "    Typically use an operational database\n",
    "    Most likely to have data from the past hour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d99099-8ca0-400f-bd61-667746453b2d",
   "metadata": {},
   "source": [
    "## Which is better?\n",
    "\n",
    "The city of Chicago receives many 311 service requests throughout the day. 311 service requests are non-urgent community requests, ranging from graffiti removal to street light outages. Chicago maintains a data repository of all these services organized by type of requests. In this exercise, Potholes has been loaded as an example of a table in this repository. It contains pothole reports made by Chicago residents from the past week.\n",
    "\n",
    "Explore the dataset. What data processing approach is this larger repository most likely using?\n",
    "Instructions\n",
    "50 XP\n",
    "Instructions\n",
    "50 XP\n",
    "Possible Answers\n",
    "\n",
    "    OLTP because this table could not be used for any analysis.\n",
    "    OLAP because each record has a unique service request number.\n",
    "#    OLTP because this table's structure appears to require frequent updates.\n",
    "    OLAP because this table focuses on pothole requests only.\n",
    "    \n",
    "    Hint\n",
    "\n",
    "    Run SELECT * FROM Potholes to take look at the data contained in this table.\n",
    "[    The rows current_status and most_recent_action indicate that this table expects updates on records.]\n",
    "    Does this data look like transactional day-to-day data?\n",
    "\n",
    "Incorrect submission\n",
    "This data could be used for analysis, it just wouldn't be as efficient as using an OLAP approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea352640-e33f-4070-bf03-ea829b58e72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "select * from Potholes;\n",
    "\n",
    "creation_date\tcurrent_status\tcompletion_date\tservice_request_id\ttype_of_service\tmost_recent_action\tstreet_address\tzip\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03380123\tPothole in Street\tnull\t10300 S WALLACE ST\t60628.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03388180\tPothole in Street\tnull\t4100 S WESTERN BLVD\t60609.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03388493\tPothole in Street\tnull\t5230 S NEW ENGLAND AVE\t60638.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03386050\tPothole in Street\tnull\t1053 E 92ND ST\t60619.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03382135\tPothole in Street\tnull\t4756 W 85TH ST\t60652.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03382643\tPothole in Street\tnull\t3623 N NORA AVE\t60634.0\n",
    "2018-12-17T00:00:00.000\tCompleted\t2018-12-18T00:00:00.000\t18-03378491\tPothole in Street\tCompleted Upon Arrival\t719 N DRAKE AVE\t60624.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03380795\tPothole in Street\tnull\t1900 S HAMLIN AVE\t60623.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03388657\tPothole in Street\tnull\t1899 S KOMENSKY AVE\t60623.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03383604\tPothole in Street\tnull\t1646 N NATOMA AVE\t60707.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03387491\tPothole in Street\tnull\t3530 W GOVERNORS PKWY\t60624.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03381825\tPothole in Street\tnull\t7558 W DEVON AVE\t60631.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03381945\tPothole in Street\tnull\t2411 N NEWCASTLE AVE\t60707.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03388109\tPothole in Street\tnull\t6300 S NARRAGANSETT AVE\t60638.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03381921\tPothole in Street\tnull\t4400 W BELDEN AVE\t60639.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03389580\tPothole in Street\tnull\t1127 W SCHUBERT AVE\t60614.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03378282\tPothole in Street\tnull\t5606 W BRYN MAWR AVE\t60646.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03378374\tPothole in Street\tnull\t9400 S WOODLAWN AVE\t60619.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03381490\tPothole in Street\tnull\t1105 N CHRISTIANA AVE\t60651.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03383626\tPothole in Street\tnull\t10359 S FAIRFIELD AVE\t60655.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03382580\tPothole in Street\tnull\t5508 S WHIPPLE ST\t60629.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03388378\tPothole in Street\tnull\t1358 S KEDZIE AVE\t60623.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03382611\tPothole in Street\tnull\t3021 N NEWLAND AVE\t60634.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03384843\tPothole in Street\tnull\t4711 N VIRGINIA AVE\t60625.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03383522\tPothole in Street\tnull\t7926 S COTTAGE GROVE AVE\t60619.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03378054\tPothole in Street\tnull\t4142 N WESTERN AVE\t60618.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03382506\tPothole in Street\tnull\t7300 W WAVELAND AVE\t60634.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03382926\tPothole in Street\tnull\t2550 N PULASKI RD\t60639.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03381396\tPothole in Street\tnull\t3116 N SACRAMENTO AVE\t60618.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03381474\tPothole in Street\tnull\t2410 W GRAND AVE\t60612.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03388778\tPothole in Street\tnull\t8247 S GREEN ST\t60620.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03385654\tPothole in Street\tnull\t4155 W 82ND PL\t60652.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03379525\tPothole in Street\tnull\t5300 S FRANCISCO AVE\t60632.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03381588\tPothole in Street\tnull\t6231 W GUNNISON ST\t60630.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03382660\tPothole in Street\tnull\t1517 W WEBSTER AVE\t60614.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03377810\tPothole in Street\tnull\t945 W DIVERSEY PKWY\t60614.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03384615\tPothole in Street\tnull\t3707 E 116TH ST\t60617.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03380164\tPothole in Street\tnull\t10000 S LOWE AVE\t60628.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03385464\tPothole in Street\tnull\t5001 S LA CROSSE AVE\t60638.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03386292\tPothole in Street\tnull\t2356 E 71ST ST\t60649.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03378165\tPothole in Street\tnull\t1121 N LATROBE AVE\t60651.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03383170\tPothole in Street\tnull\t6644 S STONY ISLAND AVE\t60637.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03382675\tPothole in Street\tnull\t2640 W MONROE ST\t60612.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03388384\tPothole in Street\tnull\t1959 S TROY ST\t60623.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03382923\tPothole in Street\tnull\t5046 N AVERS AVE\t60625.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03389579\tPothole in Street\tnull\t2600 N LAKEWOOD AVE\t60614.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03383524\tPothole in Street\tnull\t4834 N LONG AVE\t60630.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03379830\tPothole in Street\tnull\t2305 W 34TH ST\t60608.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03389578\tPothole in Street\tnull\t2640 N SHEFFIELD AVE\t60614.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03385110\tPothole in Street\tnull\t4180 W 82ND ST\t60652.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03380063\tPothole in Street\tnull\t6030 N NAGLE AVE\t60646.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03379292\tPothole in Street\tnull\t854 E 40TH ST\t60653.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03383449\tPothole in Street\tnull\t3411 S EMERALD AVE\t60616.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03378454\tPothole in Street\tnull\t4100 S WESTERN AVE\t60609.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03387506\tPothole in Street\tnull\t1145 S WASHTENAW AVE\t60612.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03379518\tPothole in Street\tnull\t10200 S HALSTED ST\t60628.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03388112\tPothole in Street\tnull\t5400 N WESTERN AVE\t60625.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03381370\tPothole in Street\tnull\t2300 N WESTERN AVE\t60647.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03385374\tPothole in Street\tnull\t10800 S SANGAMON ST\t60643.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03384523\tPothole in Street\tnull\t300 S HOMAN AVE\t60624.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03380286\tPothole in Street\tnull\t3542 N PULASKI RD\t60641.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03387600\tPothole in Street\tnull\t2747 N LARAMIE AVE\t60639.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03379342\tPothole in Street\tnull\t4830 S KEATING AVE\t60632.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03387505\tPothole in Street\tnull\t1145 S FAIRFIELD AVE\t60612.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03387458\tPothole in Street\tnull\t908 S CAMPBELL AVE\t60612.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03378386\tPothole in Street\tnull\t2138 N DAYTON ST\t60614.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03386479\tPothole in Street\tnull\t1819 W HOWARD ST\t60626.0\n",
    "2018-12-18T00:00:00.000\tOpen\tnull\t18-03387502\tPothole in Street\tnull\t2421 W 21ST PL\t60608.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03382321\tPothole in Street\tnull\t8350 W BELMONT AVE\t60634.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03382231\tPothole in Street\tnull\t1344 E 88TH ST\t60619.0\n",
    "2018-12-17T00:00:00.000\tOpen\tnull\t18-03378466\tPothole in Street\tnull\t1405 W BELLE PLAINE AVE\t60613.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9427a7-8c48-449a-9b36-89c3031c4d16",
   "metadata": {},
   "source": [
    "## Storing data\n",
    "\n",
    "\n",
    "\n",
    "Lets discuss the different ways you can store data.  Data can be stored in 3 different levels.  \n",
    "\n",
    "The first is [structured data], which is usually defined by schemas.  \n",
    "[Data types and tables are not only defined, but relationships between tables are also defined, using concepts like foreign keys].  \n",
    "\n",
    "The second is [unstructured data], which is schemaless and data in its rawest form, meaning its not clean.  Most data n the world is unstructured.  Examples includes media files and raw text.  \n",
    "\n",
    "The third is [semi-structured] data, which does not follow a larger schema, rather it has an ad-hoc self-describing structure.  Therefore, it has some structure.  This is an inherently vague definition as there can be a lot of variation between structured and unstructured data.  Examples includes NoSQL, XML, and JSON.  \n",
    "\n",
    "Because its clean and organized, structured data is easier to analyze.  However, its not as flexible because it need to follow a schema, which makes it less scalable.  These are trade-offs to consider as you move between structured and unstructured data.  You should already be familiar with traditional databases.  They generally follow relational schemas.  Operational databases, which are used for OLTP, are an example of traditional database.  Decades ago, traditional databases used to be enoughfor data storage.  Then as data analytics took off, data warehouses were popularized for OLAP approaches.  And now in the age of big data, we need to analyze and store even more data, which is where the data lake comes in.  \n",
    "\n",
    "I used to term \"traditional databases\" because many people consider data warehouse and lakes to be a type of database.  Data warehouses are optimized for read-only analytics.  They combine data from multiple sources and use massively parallel processing for faster queries.  In their database design, they typically use dimensional modeling and a denormalized schema.  We will walk through both of these terms later in the course.  \n",
    "\n",
    "\n",
    "Data warehouse:\n",
    "    Optimized for analytics - OLAP\n",
    "        Organized for reading/aggregating data\n",
    "        Usually read-only\n",
    "    Contains data from multiple sources\n",
    "    Massively Parallel Processing (MPP)\n",
    "    Typically uses a denormalized schema and dimensional modeling\n",
    "Data marts\n",
    "    Subset of data warehouse\n",
    "\n",
    "\n",
    "Amazon, Google, and Microsoft all often data warehouse solutions, know ar Redshift, Big Query, and Azure SQL Data Warehouse, respectively.  [A data mart is a subset of a data warehouse dedicated to a specific topic].  Data marts allow departments to have easier access to the data that matters to them.  Technically, traditional databases and warehouses can store unstructured data, but not cost-effectively.  Data Lake storage is cheaper because it uses object storage as opposed to the traditional block or file storage.  This allows massive amounts of data to be stored effectivey of all types, from streaming data to operational databases.  Lakes are massive because they store all the data that might be used.  \n",
    "\n",
    "Data lakes are often petabytes in size - that's 1000 terabytes.  Unstructured data is the most scable, which permits this size.  Lakes are schema-on-read, meaning the schema is created as data is read.  Warehouses and traditional databases are classified as schema-on-write because the schema is predefined.  Data lakes have to be organized and cataloged well, otherwise it becomes an aptly named \"data swamp\".  Data lakes aren't  only limited to storage.  Its becoming popular to run analytics on data lakes.  This is especially true for tasks like deep learning and data discovery, which needs a lot of data that doesn't need to be that clean.  Again the big 3 cloud providers all ofer a data lake solution.  \n",
    "\n",
    "When we think about where to store data, we have to think about how data will get there and in what form.  Extract Transform Load and Extract Load Transform are 2 different approaches for describing data flows.  They get into the intricacies of building data pipelines, which we will not get into.  ETL is the more traditional approach for warehousing and smaller-scale analutics.  But ELT has become more common with big data projects.  In ETL data is transformed before loading into storage, usually to follow the storages' schema, as in the case with warehouse.  In ELT, the data is stored in its native form in a storage solution like a data lake.  The portions of data are transformed for different purposes, from building a data warehouse to doing deep learning.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2adaf7-dfda-4173-932f-b6114c84bcce",
   "metadata": {},
   "source": [
    "## Name that data type!\n",
    "\n",
    "In the previous video, you learned about structured, semi-structured, and unstructured data. Structured data is the easiest to analyze because it is organized and cleaned. On the other hand, unstructured data is schemaless, but scales well. In the middle we have semi-structured data for everything in between.\n",
    "Instructions\n",
    "100XP\n",
    "\n",
    "Each of these cards hold a type of data. Place them in the correct category.\n",
    "\n",
    "Unstructured:\n",
    "    To-do list in a text editor\n",
    "    Zip file of all text messages ever received\n",
    "    Images in your photo library\n",
    "\n",
    "Semi-structured:\n",
    "    JSON object of tweets outputted in real-time by the Twitter API\n",
    "    <note><from>Lis</from><heading>Thanks Ruanne!</heading><body>You rock</body></note>    \n",
    "\n",
    "Structured:\n",
    "    A relational database with latest withdrawals and deposits made by clients\n",
    "    CSVs of open data downloaded from your local government websites  [Semi-structured]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c89834b-0db3-4ebe-ac0c-5df4ec687527",
   "metadata": {},
   "source": [
    "## Ordering ETL Tasks\n",
    "\n",
    "You have been hired to manage data at a small online clothing store. Their system is quite outdated because their only data repository is a traditional database to record transactions.\n",
    "\n",
    "You decide to upgrade their system to a data warehouse after hearing that different departments would like to run their own business analytics. You reason that an ELT approach is unnecessary because there is relatively little data (< 50 GB).\n",
    "Instructions\n",
    "100XP\n",
    "\n",
    "In the ETL flow you design, different steps will take place. Place the steps in the most appropriate order.\n",
    "\n",
    "eCommerce API outputs real time data of transactions\n",
    "Python script drops null rows and clean data into pre-determined columns\n",
    "Resulting dataframes is written into an AWS Redshift Warehouse\n",
    "\n",
    "Hint\n",
    "\n",
    "[    Python scripts are often used to transform data to match a specified schema.]\n",
    "    Data marts are a subset of data warehouses.\n",
    "    In ETL, you need to clean data before writing it to a data warehouse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aad440f-6d8d-4464-80a7-e54da8687be7",
   "metadata": {},
   "source": [
    "## Recommend a storage solution\n",
    "\n",
    "When should you choose a data warehouse over a data lake?\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "\n",
    "    To train a machine learning model with a 150 GB of raw image data.\n",
    "    1\n",
    "    To store real-time social media posts that may be used for future analysis\n",
    "    2\n",
    "    To store customer data that needs to be updated regularly\n",
    "    3\n",
    "#    To create accessible and isolated data repositories for other analysts\n",
    "    4\n",
    "    \n",
    "Hint\n",
    "\n",
    "    Data lakes store unstructured data, which is data in its native form.\n",
    "    Data lakes are better equipped to handle big data problems.\n",
    "    The main use of warehouses is not to store data, but to run analysis.\n",
    "\n",
    "That's right! Analysts will appreciate working in a data warehouse more because of its organization of structured data that make analysis easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fcfac8-871d-4233-9fd6-ee87ed7c42f9",
   "metadata": {},
   "source": [
    "## Database design\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now lets learn more about database design means.  [Database design determines how data is logically stored].  This is crucial because it affects how the database will be queried, whether for reading data or updating data.  There are 2 important concepts to know when it comes to database design: [Database models and schemas].  Database models are high-level specifications for database structure.  [The relational model, which is the most popular, is the model used to make relational databases].  It defines rows as records and columns as attributes.  It calls for rules such as each row having unique keys.  There are other models that exist that do not enforce the same rules.  \n",
    "\n",
    "A schema is a database's blueprint.  In other words, the implementation of the database model.  It takes the logical structure more granularly by defining the specific tables, fields, relationships, indexes, and views a database will have.  Schemas must be respected when inserting structured data into a relational database.  [The first step to database design is data modeling].  __This is the abstract design phase, where we define a data model for the data to be stored.  There are 3 levels to a data model: \n",
    "\n",
    "[    A conceptual data model] describes what the database contains, such as its entities, relationships, sttributes. \n",
    "[    A logical data model] decides how these entities and relationships map to tables.  \n",
    "[    A physical data model] looks at how data will be physically stored at the lowest level of abstraction.  \n",
    "These 3 levels of a data model ensure consistency and provide a plan for implementation and use.  \n",
    "\n",
    "Here is a simplified example of where we want to store songs.  In this case, the entities are songs, albums, and artists with various [] attributes.  Their relationships are denoted by blue rhombuses (<>).  \n",
    "\n",
    "\n",
    "[Conceptual - ER diagram]\n",
    "    Song[song_id, title, length] --N--<>has-- Album[album_id, title, num_songs] --N--<>creates--1-- Artist[artist_id, label, ganre]\n",
    "\n",
    "\n",
    "Here we have a conceptual idea of the data we want to store.  And here is the a corresponding schema using the relational model.  The fastest way to create a schema is to translate the entities into tables.  But just because its the easiest, doesn't mean its the best.  \n",
    "\n",
    "\n",
    "[Logical - schema]\n",
    "   Songs\n",
    "    song_id     bigint\n",
    "    title       char\n",
    "    length      float\n",
    "    album_id    bigint  ----\n",
    "                           | \n",
    "   Albums                  |\n",
    "    album_id    bigint  ----\n",
    "    title       char\n",
    "    num_songs   int\n",
    "    artist_id   bigint  ----\n",
    "                           |\n",
    "   Artists                 |\n",
    "    artist_id   bigint  ----\n",
    "    genre       char\n",
    "    label       char\n",
    "\n",
    "\n",
    "Lets look at some other ways this ER diagram could be converted.  For example, you could opt to have one table because you don't want to run so many joins to get song information.  Or you could add tables for genre and label.  Many songs share these attributes, and having one place for them helps with data integrity.  [The biggest difference here is how the tables are determined].  There are different pros and cons to these 3 examples shown below.  The next chapter on normalization and denormalization will expand on this.  From the prerequisites, you should be familiar with the relational model.  \n",
    "\n",
    "\n",
    "   Songs\n",
    "    song_id          bigint\n",
    "    song_title       char\n",
    "    length           float\n",
    "    album_title      bigint\n",
    "    num_songs_album  int\n",
    "    artist_name      char\n",
    "    genre            char\n",
    "    label            char\n",
    "\n",
    "\n",
    "   Songs\n",
    "    song_id     bigint\n",
    "    title       char\n",
    "    length      float\n",
    "    album_id    bigint  ----\n",
    "                           | \n",
    "   Albums                  |\n",
    "    album_id    bigint  ----\n",
    "    title       char\n",
    "    num_songs   int\n",
    "    artist_id   bigint  ----\n",
    "                           |\n",
    "   Artists                 |\n",
    "    artist_id   bigint  ----\n",
    "    genre_id    bigint  -----------------------------|\n",
    "    label_id    bigint  ------Label                Genre\n",
    "                               label_id  bigint     genre_id  bigint\n",
    "                               label     char       genre     char\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "[Dimensional modeling is an adaptation of the relational model specifically for data warehouse].  Its optimized for OLAP type of queries that aim to analyze rather than update.  To do this, it uses the [star schema].  In the next chapter, we'll dive into that more.  [The schema of a dimentional model tends to be easy to interpret and extend].  This is a big plus for analysts working on the data warehouse.  Dimensional models are made up of 2 types of tables: [fact and dimension tables].  __What the fact table holds is decided by the business use-case.  It contains records of key metric, and this metric changes often.__  Fact tables also hold foreign keys to dimension tables.  __Dimension tables hold descriptions of specific attributes and these do not change as often.__  \n",
    "\n",
    "So what does that means?  Back to the Song Analysis example, The center (turquoise) table is a fact table called songs, It contains foreign keys to surrounding (purple) dimension tables.  These dimension table expand on the attributes of a fact table, such as the album it is in and the artist who mdade it.  The records in fact tables often change as new songs get inserted.  Albums, labels, artists and genres will be shared by more than one song - hence records in dimenstion tables won't change as much.  \n",
    "\n",
    "\n",
    "Dimension table               Fact table               Dimension table\n",
    "  Album                         Songs                    Artist\n",
    "    album_id                      album_id                 artist_id\n",
    "    album_title                   artist_id                artist_name\n",
    "    num_songs                     label_id                 gender\n",
    "    release_date                  genre_id                 age\n",
    "                                  song_title\n",
    "Dimension table                   song_length          Simension table\n",
    "  Label                                                  Genre\n",
    "    label_id                                               genre_id\n",
    "    label_name                                             genre_name\n",
    "    address\n",
    "\n",
    "\n",
    "Summing it up, to detect a fact table in a dimention model, consider what is being analyzed and how often entities change.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae14438-07be-4582-bc22-c416da5e5ce1",
   "metadata": {},
   "source": [
    "## Classifying data models\n",
    "\n",
    "In the previous video, we learned about three different levels of data models: conceptual, logical, and physical.\n",
    "Instructions\n",
    "100XP\n",
    "\n",
    "Each of these cards hold a tool or concept that fits into a certain type of data model. Place the cards in the correct category.\n",
    "\n",
    "Conceptual Data Model:\n",
    "    Gathers business requirements\n",
    "    Relational model   [Logical Data Model]\n",
    "    \n",
    "Logical Data Model:\n",
    "    Entities, attributes, and relationships   [Conceptual Data Model]\n",
    "    Determining tables and columns\n",
    "    \n",
    "Physical Data Model:\n",
    "    File structure of data storage\n",
    "    \n",
    "Hint\n",
    "\n",
    "    Database design is a step in determining the logical structure of a database.\n",
    "    Entity-Relation diagrams are useful creating conceptual data models.\n",
    "    Physical data models focus on how data will be physically stored.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36967b74-725c-4f01-91e8-ee93f56cb8f2",
   "metadata": {},
   "source": [
    "## Deciding fact and dimension tables\n",
    "\n",
    "Imagine that you love running and data. It's only natural that you begin collecting data on your weekly running routine. You're most concerned with tracking how long you are running each week. You also record the route and the distances of your runs. You gather this data and put it into one table called Runs with the following schema:\n",
    "  runs\n",
    "    duration_mins - float\n",
    "    week - int\n",
    "    month - varchar(160)\n",
    "    year - int\n",
    "    park_name - varchar(160)\n",
    "    city_name - varchar(160)\n",
    "    distance_km - float\n",
    "    route_name - varchar(160)\n",
    "\n",
    "After learning about dimensional modeling, you decide to restructure the schema for the database. Runs has been pre-loaded for you.\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "\n",
    "    Question 1\n",
    "    Out of these possible answers, what would be the best way to organize the fact table and dimensional tables?\n",
    "Possible Answers\n",
    "\n",
    "#    A fact table holding duration_mins and foreign keys to dimension tables holding route details and week details, respectively.\n",
    "    A fact table holding week,month, year and foreign keys to dimension tables holding route details and duration details, respectively.\n",
    "    A fact table holding route_name,park_name, distance_km,city_name, and foreign keys to dimension tables holding week details and duration details, respectively.\n",
    "    \n",
    "Incorrect submission\n",
    "Try again. route_name,park_name, distance_km, and city_name would be better in it's own dimension table. We're more interested in the run itself, than where it took place.\n",
    "\n",
    "\n",
    "    Question 2\n",
    "    Create a dimension table called route that will hold the route information.\n",
    "    Create a dimension table called week that will hold the week information.\n",
    "\n",
    "Hint\n",
    "\n",
    "[    Route information includes the columns park_name, city_name, distance_km, and route_name.\n",
    "    Week information includes the columns week, month, and year.\n",
    "    Make sure to look at the schema diagram for the data type of each columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419501d4-385d-46de-b11a-c13ab48ac822",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT * FROM Runs;\n",
    "\n",
    "\n",
    "duration_mins\tweek\tmonth\tyear\tpark_name\tcity_name\tdistance_km\troute_name\n",
    "24.5\t3\tMay\t2019\tProspect Park\tBrooklyn\t5\tSimple Loop\n",
    "61\t3\tMay\t2019\tCentral Park\tNew York City\t8\tResevoir Loop\n",
    "24.5\t3\tMay\t2019\tCentral Park\tNew York City\t5\tLake Loop\n",
    "24.5\t4\tMay\t2019\tCentral Park\tNew York City\t5\tLake Loop\n",
    "48\t4\tMay\t2019\tProspect Park\tBrooklyn\t10\tGrove Run\n",
    "23\t4\tMay\t2019\tProspect Park\tBrooklyn\t5\tSimple Loop\n",
    "24.5\t1\tJune\t2019\tCentral Park\tNew York City\t5\tLake Loop\n",
    "54.96\t1\tJune\t2019\tPennypack Park\tPhiladelphia\t12\tPenny Trail Extended\n",
    "38.4\t1\tJune\t2019\tCentral Park\tNew York City\t8\tResevoir Loop\n",
    "23.75\t1\tJune\t2019\tCentral Park\tNew York City\t5\tLake Loop\n",
    "57.6\t2\tJune\t2019\tPennypack Park\tPhiladelphia\t12\tPenny Trail Extended\n",
    "31.8\t2\tJune\t2019\tPennypack Park\tPhiladelphia\t6\tPenny Trail\n",
    "49\t2\tJune\t2019\tLiberty State Park\tJersey City\t10\tWater Front Run\n",
    "23.35\t2\tJune\t2019\tProspect Park\tBrooklyn\t5\tSimple Loop\n",
    "39.2\t3\tJune\t2019\tCentral Park\tNew York City\t8\tResevoir Loop\n",
    "27.48\t3\tJune\t2019\tPennypack Park\tPhiladelphia\t6\tPenny Trail\n",
    "28.8\t3\tJune\t2019\tPennypack Park\tPhiladelphia\t6\tPenny Trail\n",
    "47.5\t3\tJune\t2019\tLiberty State Park\tJersey City\t10\tWater Front Run\n",
    "24\t4\tJune\t2019\tCentral Park\tNew York City\t5\tLake Loop\n",
    "53\t4\tJune\t2019\tProspect Park\tBrooklyn\t10\tGrove Run\n",
    "24.5\t4\tJune\t2019\tProspect Park\tBrooklyn\t5\tSimple Loop\n",
    "37.36\t4\tJune\t2019\tCentral Park\tNew York City\t8\tResevoir Loop\n",
    "28.8\t1\tJuly\t2019\tPennypack Park\tPhiladelphia\t6\tPenny Trail\n",
    "23\t1\tJuly\t2019\tProspect Park\tBrooklyn\t5\tSimple Loop\n",
    "49\t1\tJuly\t2019\tLiberty State Park\tJersey City\t10\tWater Front Run\n",
    "45.8\t1\tJuly\t2019\tLiberty State Park\tJersey City\t10\tWater Front Run\n",
    "24\t2\tJuly\t2019\tProspect Park\tBrooklyn\t5\tSimple Loop\n",
    "47.5\t2\tJuly\t2019\tProspect Park\tBrooklyn\t10\tGrove Run\n",
    "24\t2\tJuly\t2019\tCentral Park\tNew York City\t5\tLake Loop\n",
    "53\t2\tJuly\t2019\tLiberty State Park\tJersey City\t10\tWater Front Run\n",
    "24.5\t3\tJuly\t2019\tCentral Park\tNew York City\t5\tLake Loop\n",
    "37.36\t3\tJuly\t2019\tCentral Park\tNew York City\t8\tResevoir Loop\n",
    "24.5\t3\tJuly\t2019\tProspect Park\tBrooklyn\t5\tSimple Loop\n",
    "\n",
    "\n",
    "\n",
    "-- Create a route dimension table\n",
    "CREATE TABLE ___(\n",
    "\troute_id INTEGER PRIMARY KEY,\n",
    "    ___ VARCHAR(160) NOT NULL,\n",
    "    ___ VARCHAR(160) NOT NULL,\n",
    "    distance_km ___ NOT NULL,\n",
    "    ___ VARCHAR(160) NOT NULL\n",
    ");\n",
    "-- Create a week dimension table\n",
    "CREATE TABLE ___(\n",
    "\tweek_id INTEGER PRIMARY KEY,\n",
    "    week ___ NOT NULL,\n",
    "    ___ VARCHAR(160) NOT NULL,\n",
    "    ___ ___ NOT NULL\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14dfe04-8663-48f1-8bbc-decb0cd60018",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Create a route dimension table\n",
    "CREATE TABLE route(\n",
    "\troute_id INTEGER PRIMARY KEY,\n",
    "    route_name VARCHAR(160) NOT NULL,\n",
    "    park_name VARCHAR(160) NOT NULL,\n",
    "    distance_km float NOT NULL,\n",
    "    city_name VARCHAR(160) NOT NULL\n",
    ");\n",
    "-- Create a week dimension table\n",
    "CREATE TABLE week(\n",
    "\tweek_id INTEGER PRIMARY KEY,\n",
    "    week int NOT NULL,\n",
    "    month VARCHAR(160) NOT NULL,\n",
    "    year int NOT NULL\n",
    ");\n",
    "\n",
    "\n",
    "\n",
    "Solution ------------------------------------------------------------------------------------------------------------\n",
    "-- Create a route dimension table\n",
    "CREATE TABLE route (\n",
    "\troute_id INTEGER PRIMARY KEY,\n",
    "    park_name VARCHAR(160) NOT NULL,\n",
    "    city_name VARCHAR(160) NOT NULL,\n",
    "    distance_km FLOAT NOT NULL,\n",
    "    route_name VARCHAR(160) NOT NULL\n",
    ");\n",
    "-- Create a week dimension table\n",
    "CREATE TABLE week(\n",
    "\tweek_id INTEGER PRIMARY KEY,\n",
    "    week INTEGER NOT NULL,\n",
    "    month VARCHAR(160) NOT NULL,\n",
    "    year INTEGER NOT NULL\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a9484a-e2c0-4896-a11d-eb3c6c922847",
   "metadata": {},
   "source": [
    "## Querying the dimensional model\n",
    "\n",
    "Here it is! The schema reorganized using the dimensional model:\n",
    "\n",
    "route_dim             runs_fact            week_dim\n",
    "  route_id              route_id             week_id\n",
    "  park_name             week_id              week\n",
    "  city_name             duration_mins        month\n",
    "  distance_km                                year\n",
    "  route_name\n",
    "\n",
    "Let's try to run a query based on this schema. How about we try to find the number of minutes we ran in July, 2019? We'll break this up in two steps. First, we'll get the total number of minutes recorded in the database. Second, we'll narrow down that query to week_id's from July, 2019.\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "\n",
    "    Question 1\n",
    "    Calculate the sum of the duration_mins column.\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    Join week_dim and runs_fact.\n",
    "    Get all the week_id's from July, 2019.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e54c60-db86-4cb6-9036-eb94089224f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT \n",
    "\t-- Select the sum of the duration of all runs\n",
    "\tsum(duration_mins)\n",
    "FROM \n",
    "\truns_fact;\n",
    "    \n",
    "sum\n",
    "1172.1599999999999\n",
    "\n",
    "\n",
    "\n",
    "SELECT \n",
    "\t-- Get the total duration of all runs\n",
    "\tSUM(duration_mins)\n",
    "FROM \n",
    "\truns_fact\n",
    "-- Get all the week_id's that are from July, 2019\n",
    "INNER JOIN ___ ON ___.___ = ___.___\n",
    "WHERE ___ = 'July' and ___ = '2019';\n",
    "\n",
    "\n",
    "SELECT \n",
    "\t-- Get the total duration of all runs\n",
    "\tSUM(duration_mins)\n",
    "FROM \n",
    "\truns_fact\n",
    "-- Get all the week_id's that are from July, 2019\n",
    "INNER JOIN week_dim ON runs_fact.week_id = week_dim.week_id\n",
    "WHERE month = 'July' and year = '2019';\n",
    "\n",
    "sum\n",
    "381.46000000000004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917dd0d5-efd0-4254-b8e7-c0b1dcae68f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_fact\n",
    "\n",
    "duration_mins\tweek_id\troute_id\n",
    "24.5\t601\t101\n",
    "61\t601\t103\n",
    "24.5\t601\t104\n",
    "24.5\t602\t104\n",
    "48\t602\t102\n",
    "23\t602\t101\n",
    "24.5\t603\t104\n",
    "54.96\t603\t106\n",
    "38.4\t603\t103\n",
    "23.75\t603\t104\n",
    "57.6\t604\t106\n",
    "31.8\t604\t105\n",
    "49\t604\t107\n",
    "23.35\t604\t101\n",
    "39.2\t605\t103\n",
    "27.48\t605\t105\n",
    "28.8\t605\t105\n",
    "47.5\t605\t107\n",
    "24\t606\t104\n",
    "53\t606\t102\n",
    "24.5\t606\t101\n",
    "37.36\t606\t103\n",
    "28.8\t607\t105\n",
    "23\t607\t101\n",
    "49\t607\t107\n",
    "45.8\t607\t107\n",
    "24\t608\t101\n",
    "47.5\t608\t102\n",
    "24\t608\t104\n",
    "53\t608\t107\n",
    "24.5\t609\t104\n",
    "37.36\t609\t103\n",
    "24.5\t609\t101\n",
    "\n",
    "\n",
    "week_dim\n",
    "\n",
    "week_id\tweek\tmonth\tyear\n",
    "601\t3\tMay\t2019\n",
    "602\t4\tMay\t2019\n",
    "603\t1\tJune\t2019\n",
    "604\t2\tJune\t2019\n",
    "605\t3\tJune\t2019\n",
    "606\t4\tJune\t2019\n",
    "607\t1\tJuly\t2019\n",
    "608\t2\tJuly\t2019\n",
    "609\t3\tJuly\t2019\n",
    "\n",
    "\n",
    "route_dim\n",
    "route_id\tpark_name\tcity_name\tdistance_km\troute_name\n",
    "101\tProspect Park\tBrooklyn\t5\tSimple Loop\n",
    "102\tProspect Park\tBrooklyn\t10\tGrove Run\n",
    "103\tCentral Park\tNew York City\t8\tResevoir Loop\n",
    "104\tCentral Park\tNew York City\t5\tLake Loop\n",
    "105\tPennypack Park\tPhiladelphia\t6\tPenny Trail\n",
    "106\tPennypack Park\tPhiladelphia\t12\tPenny Trail Extended\n",
    "107\tLiberty State Park\tJersey City\t10\tWater Front Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c5c5f0-8e83-429a-a077-cb6ed87da01b",
   "metadata": {},
   "source": [
    "## Star and snowflake schema\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Congrats on finishing the first chapter.  We're now going to jump in where we left off with the [star schema]. The star schema is the simplest form of the dimensional model.  Some use the term \"star schema\" and \"dimensional model\" interchangeably.  Remember that the star schema is made up of 2 tables: [fact and dimension tables].  Fact tables hold records of metrics that are described further by dimension tables.  Throughout this chapter, we are going to use another bookstore example.  However, this time, you work for a company that sells books in bulk to bookstores across the US and Canada.  You have a database to keep track of book sales.  Lets take a look at the star schema for this database.  \n",
    "\n",
    "                                        dim_book_star\n",
    "                                          book_is    int           PK\n",
    "                                          title      varchar(256)\n",
    "                                          author     varchar(256)\n",
    "                                          publisher  varchar(256)\n",
    "                                          genre      varchar(128)\n",
    "                                                  |\n",
    "                                                 |||\n",
    "dim_store_star                          fact_booksales                   dim_time_star\n",
    "  store_id       int           PK         sales_id      int    PK          time_id      int    PK\n",
    "  store_address  varchar(256)         -   book_id       int    FK -        day          int\n",
    "  city           varchar(128)   -------   time_id       int    FK ------   month        int\n",
    "  state          varchar(128)         -   store_id      int    FK -        quarter      int\n",
    "  country        varchar(128)             sales_amount  float              year         int\n",
    "                                          quantity      int\n",
    "\n",
    "\n",
    "Excluding primary and foreign keys, the fact table holds the sale amount and quantity of books.  Its connected to dimension tables with details on the books sold, the time the sale took place, and the store buying the books.  You may notice the lines connecting these tables have a special pattern.  These lines represent a one-to-many relationship.  For example, a store can be part of many book sales, but one sale can only belong to one store.  The star schema got its name because it tends to look like a star wit its different extension points.  \n",
    "\n",
    "Now that we have a good grasp of the star schema, lets look at the snowflake schema.  The snowflake schema is an extension of the star schema.  Off the bat, we see that it has more tables.  You may not be able to see all the details in the slide, but don't worry it will be broken down in later slides.  \n",
    "\n",
    "[The snowflake schema looks like above diagram, but with other many-to-one tables linked to the dimension tables]\n",
    "\n",
    "\n",
    "dim_author_sf                           dim_book_sf                      dim_genre_sf\n",
    "  author_id    int          PK        -   book_is    int           PK      genre_id    int          PK\n",
    "  author       varchar(256)     -------   title      varchar(256)  ------  genre       varchar(256)\n",
    "                                      -   author     varchar(256)  -\n",
    "                                          publisher  varchar(256)\n",
    "                                          genre      varchar(128)\n",
    "                                                  |\n",
    "                                                 |||\n",
    "dim_store_sf                            fact_booksales                   dim_time_sf\n",
    "  store_id       int           PK         sales_id      int    PK          time_id      int    PK\n",
    "  store_address  varchar(256)         -   book_id       int    FK -        day          int\n",
    "  city_id        int           FK -----   time_id       int    FK ------   month        int    FK\n",
    "       |||                            -   store_id      int    FK -        quarter      int\n",
    "        |                                 sales_amount  float              year         int\n",
    "dim_city_sf                               quantity      int                  |||\n",
    "  city_id        varchar(128)  PK                                             |\n",
    "  city           varchar(128)                                             dim_month_sf\n",
    "  state          varchar(128)  FK                                           month_id   int   PK\n",
    "                                                                            month      int\n",
    "  \n",
    "# *******************************************************************************************************************\n",
    "The information contained in this schema is the same as the star schema.  In fact, the fact table is the same but the way the dimension tables are structured is different.  We see that they extend more, hence its namesake.  [The star schema extend one dimsion, while the snowflake schema extends over more than one dimension].  This is because the dimension tables are normalized.  So what is normalization?  [Normalization is a technique that divides tables into smaller tables and connects them via relationships]  __The goal is to reduce redundancy and increase data integrity__  So how does this happen?  There are several forms of normalization, which we'll delve into later.  __But the basic idea is to identify repeating groups of data and create new tables for them__  Lets go back to our exampleand see how these tables were normalized.  Here is the book dimension in the star schema.  [What could be repeating here?]  Primary key are inherently unique.  For book titles, although there is possible repeat here, it is not common.  On the other hand, authors often publish more than one book, [publishers definitely publish many books], and a lot of books share genres.  We can create new tables for them, and it results in the following snowflake schema.  Do you see how these repeating groups now have their own table?  On to the store dimension.  [City, states, and countries can definitely have more than one book stores within them].  \n",
    "\n",
    "dim_book_star\n",
    "  book_id     int           PK\n",
    "  title       varchar(256)\n",
    "  author      varchar(256)\n",
    "  publisher   varchar(256)\n",
    "  genre       varchar(256)\n",
    "\n",
    "\n",
    "                                         dim_publisher_sf\n",
    "                                           publisher_id     int          PK\n",
    "                                           publisher        varchar(256)\n",
    "                                               |\n",
    "                                              |||\n",
    "dim_author_sf                            dim_book_sf                     -       dim_genre_sf\n",
    "  author_id   int          PK          -   book_id      int          PK  --------  genre_id       int         PK\n",
    "  author      varchar(256)     ---------   title        varchar()256     -         genre          varchar(128)\n",
    "                                       -   author_id    int          FK\n",
    "                                           publisher_id int          FK\n",
    "                                           genre_id     int          FK\n",
    "\n",
    "\n",
    "dim_store_sf\n",
    "  store_id       int         PK\n",
    "  store_address  varchar(256)\n",
    "  city_id        int         FK\n",
    "|||\n",
    "|\n",
    "dim_city_sf\n",
    "  city_id        int         PK\n",
    "  city           varchar(128)\n",
    "  state_id       int         FK\n",
    "|||\n",
    "|\n",
    "dim_state_sf\n",
    "  state_id       int         PK\n",
    "  state          varchar(128)\n",
    "  country_id     int         FK\n",
    "|||\n",
    "|\n",
    "dim_country_sf\n",
    "  country_id     int         PK\n",
    "  country        varchar(128) \n",
    "\n",
    "We can apply same skills to normalize dimension tables representing the book stores.  Do you notice that the way we structure these repeating groups is a bit different from the book dimension?  [An author can have published in different genres and with various publishers, hence why they were different dimemsions]. However, a city stays in the same state and country; thus they extend each other over 3 dimensions.  The same is done on the time dimension.  A day is a part of a month that is a part of a quarter, and so on.  \n",
    "\n",
    "And then we put all the normalized dimensions together to get the snowflake schema.  Geting the hang of this? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76cff00-4adf-4681-9608-6977c337f4e1",
   "metadata": {},
   "source": [
    "## Running from star to snowflake\n",
    "\n",
    "Remember your running database from last chapter?\n",
    "\n",
    "route_dim             runs_fact            week_dim\n",
    "  route_id              route_id             week_id\n",
    "  park_name             week_id              week\n",
    "  city_name             duration_mins        month\n",
    "  distance_km                                year\n",
    "  route_name\n",
    "\n",
    "After learning about the snowflake schema, you convert the current star schema into a snowflake schema. To do this, you normalize route_dim and week_dim. Which option best describes the resulting new tables after doing this?\n",
    "\n",
    "The tables runs_fact, route_dim, and week_dim have been loaded.\n",
    "Instructions\n",
    "50 XP\n",
    "Instructions\n",
    "50 XP\n",
    "Possible Answers\n",
    "\n",
    "    week_dim is extended two dimensions with new tables for month and year. route_dim is extended one dimension with a new table for city.\n",
    "    week_dim is extended two dimensions with new tables for month and year. route_dim is extended two dimensions with new tables for city and park.\n",
    "#    week_dim is extended three dimensions with new tables for week, month and year. route_dim is extended one dimension with new tables for city and park.\n",
    "\n",
    "Hint\n",
    "\n",
    "    Look through the tables route_dim and week_dim to see what columns hold repeated information.\n",
    "    Remember how dimensions can connect to each other via foreign keys, like the city, state, and country variables in the slides?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4a2639-0273-4a07-997d-358beafeba68",
   "metadata": {},
   "source": [
    "## Adding foreign keys\n",
    "\n",
    "Foreign key references are essential to both the snowflake and star schema. When creating either of these schemas, correctly setting up the foreign keys is vital because they connect dimensions to the fact table. They also enforce a one-to-many relationship, because unless otherwise specified, a foreign key can appear more than once in a table and primary key can appear only once.\n",
    "\n",
    "The fact_booksales table has three foreign keys: book_id, time_id, and store_id. In this exercise, the four tables that make up the star schema below have been loaded. However, the foreign keys still need to be added.\n",
    "\n",
    "                                        dim_book_star\n",
    "                                          book_is    int           PK\n",
    "                                          title      varchar(256)\n",
    "                                          author     varchar(256)\n",
    "                                          publisher  varchar(256)\n",
    "                                          genre      varchar(128)\n",
    "                                                  |\n",
    "                                                 |||\n",
    "dim_store_star                          fact_booksales                   dim_time_star\n",
    "  store_id       int           PK         sales_id      int    PK          time_id      int    PK\n",
    "  store_address  varchar(256)         -   book_id       int    FK -        day          int\n",
    "  city           varchar(128)   -------   time_id       int    FK ------   month        int\n",
    "  state          varchar(128)         -   store_id      int    FK -        quarter      int\n",
    "  country        varchar(128)             sales_amount  float              year         int\n",
    "                                          quantity      int\n",
    "\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    In the constraint called sales_book, set book_id as a foreign key.\n",
    "    In the constraint called sales_time, set time_id as a foreign key.\n",
    "    In the constraint called sales_store, set store_id as a foreign key.\n",
    "\n",
    "Hint\n",
    "\n",
    "    The table with the foreign keys, fact_booksales, is the table that will be altered.\n",
    "    The (___) holds the column name that will serve as the foreign key.\n",
    "    The primary keys of the dimension tables need to be referenced as foreign keys, therefore, a dimension table should come after REFERENCES.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ac6be-b00a-464f-8f8c-b19c89f5fdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Add the book_id foreign key\n",
    "ALTER TABLE ___ ADD CONSTRAINT sales_book\n",
    "    FOREIGN KEY (___) REFERENCES ___ (___);\n",
    "    \n",
    "-- Add the time_id foreign key\n",
    "ALTER TABLE ___ ___ ___ ___\n",
    "    ___ ___ (___) REFERENCES ___ (___);\n",
    "    \n",
    "-- Add the store_id foreign key\n",
    "___ ___ ___ ___ ___ ___\n",
    "    ___ ___ (___) ___ ___ (___);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d93a2-62f1-4bac-b227-68d304168bb2",
   "metadata": {},
   "source": [
    "# *******************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa30b9-33ab-4c77-bc44-bdb261e0dda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Add the book_id foreign key\n",
    "ALTER TABLE fact_booksales ADD CONSTRAINT sales_book\n",
    "    FOREIGN KEY (book_id) REFERENCES dim_book_star (book_id);\n",
    "    \n",
    "-- Add the time_id foreign key\n",
    "ALTER TABLE fact_booksales ADD CONSTRAINT sales_time\n",
    "    FOREIGN KEY (time_id) REFERENCES dim_time_star (time_id);\n",
    "    \n",
    "-- Add the store_id foreign key\n",
    "ALTER TABLE fact_booksales ADD CONSTRAINT sales_store\n",
    "    FOREIGN KEY (Store_id) REFERENCEs dim_store_star (store_id);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5d9706-3bc6-4d49-9236-b34d20882f46",
   "metadata": {},
   "source": [
    "## Extending the book dimension\n",
    "\n",
    "In the video, we saw how the book dimension differed between the star and snowflake schema. The star schema's dimension table for books, dim_book_star, has been loaded and below is the snowflake schema of the book dimension.\n",
    "\n",
    "                                         dim_publisher_sf\n",
    "                                           publisher_id     int          PK\n",
    "                                           publisher        varchar(256)\n",
    "                                               |\n",
    "                                              |||\n",
    "dim_author_sf                            dim_book_sf                     -       dim_genre_sf\n",
    "  author_id   int          PK          -   book_id      int          PK  --------  genre_id       int         PK\n",
    "  author      varchar(256)     ---------   title        varchar()256     -         genre          varchar(128)\n",
    "                                       -   author_id    int          FK\n",
    "                                           publisher_id int          FK\n",
    "                                           genre_id     int          FK\n",
    "\n",
    "\n",
    "In this exercise, you are going to extend the star schema to meet part of the snowflake schema's criteria. Specifically, you will create dim_author from the data provided in dim_book_star.\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "\n",
    "    Question 1\n",
    "    Create dim_author with a column for author.\n",
    "    Insert all the distinct authors from dim_book_star into dim_author.\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    Alter dim_author to have a primary key called author_id.\n",
    "    Output all the columns of dim_author.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74e9327-838b-49fb-b918-bd93ecac08b2",
   "metadata": {},
   "source": [
    "# *******************************************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31505dce-63fd-4ec9-97f5-e14637aced79",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Create dim_author with an author column\n",
    "CREATE TABLE dim_author (\n",
    "    author VARCHAR(256)  NOT NULL\n",
    ");\n",
    "\n",
    "-- Insert authors into the new table\n",
    "INSERT INTO dim_author\n",
    "SELECT DISTINCT author FROM dim_book_star;\n",
    "\n",
    "\n",
    "\n",
    "-- Create a new table for dim_author with an author column\n",
    "CREATE TABLE dim_author(\n",
    "    author varchar(256)  NOT NULL\n",
    ")\n",
    "\n",
    "-- Insert authors\n",
    "INSERT INTO dim_author\n",
    "SELECT DISTINCT author FROM dim_book_star\n",
    "\n",
    "-- Add a primary key ------------------------------------------------------------------------------------------------\n",
    "ALTER TABLE dim_author ADD COLUMN author_id SERIAL PRIMARY KEY\n",
    "\n",
    "-- Output the new table\n",
    "SELECT * FROM dim_author"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87536e2c-b0b1-4ae1-8a9b-59d183b7c406",
   "metadata": {},
   "source": [
    "## Normalized and denormalized databases\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now that we have a grasp on normalization, lets talk about why we would want to normalize a database.  You should be familiar th these 2 schemars by now.  The star schema and the snowflake schema.  They both storing fictional company data on the sales of books in bulk to stores across the US and Canada.  On the left you have the [star schema with denormalized dimension tables].  And on the right, you have the [snowflake schema with normalized dimension tables].  The normalized database looks way more complicated.  Adn it is in some ways.  \n",
    "\n",
    "For example, lets say you wanted to get all the quantity of all books by Octavia E. Bulter sold in Vancouver in Q4 of 2018.   Based on the denormalized schema, you can run the following query to accomplish this.  It composed of 3 joins, which makes sense based on the 3 dimension tables in the starschema.  \n",
    "\n",
    "\n",
    "SELECT SUM(quantity) FROM fact_booksales \n",
    "    INNER JOIN dim_store_star ON fact_booksales.store_id = dim_store_star.store_id\n",
    "    INNER JOIN dim_book_star ON fact_booksales.book_id = dim_book_star.book_id\n",
    "    INNER JOIN dim_time_star ON fact_booksales.time_id = dim_time_star.time_id\n",
    "WHERE \n",
    "    dim_store_star.city = 'Vancouver' AND dim_book_star.author = 'Octavia E. Bulter' AND\n",
    "    dim_time_star.yesr = 2018 and dim_time_star.quarter = 4;\n",
    "\n",
    "\n",
    "And what would the query look like on the normalized schema?  A lot longer.  There is a total of 8 inner joins.  This makes sense based on the snowflake schema diagram.  The normalized snowflake schema has considerable more tables.  this means [more joins, which means slower queries.  \n",
    "\n",
    "\n",
    "SELECT \n",
    "    SUM(fact_booksales.quantity)\n",
    "FROM\n",
    "    fact_booksales\n",
    "INNER JOIN dim_store_sf ON fact_booksales.store_id = dim_store_sf.store_id\n",
    "INNER JOIN dim_city_sf ON dim_store_sf.city_id = dim_city_sf.city_id\n",
    "INNER JOIN dim_book_sf ON fact_booksales.book_id = dim_book_sf\n",
    "INNER JOIN dim_author_sf ON dim_book_sf.author_id = dim_author_sf.author_id\n",
    "INNER JOIN dim_time_sf ON fact_booksales.time_id = dim_time_sf.time_id\n",
    "INNER JOIN dim_month_sf ON dim_time_sf.month_id = dim_month_sf.month_id\n",
    "INNER JOIN dim_quarter_sf ON dim_time_sf.quarter_id = dim_quarter_sf.quarter_id\n",
    "INNER JOIN dim_year_sf ON dim_quarter_sf.year_id = dim_year_sf.year_id\n",
    "Where \n",
    "    dim_store_star.city = 'Vancouver' AND dim_book_star.author = 'Octavia E. Bulter' AND\n",
    "    dim_time_star.yesr = 2018 and dim_time_star.quarter = 4;\n",
    "\n",
    "\n",
    "[So why would we want to normalize a database?  Normalization saves space].  This isn't intuitive seeing how normalized database have more tables.  Lets take a look at the store table in our denormalized database.  Here we see a lot of information in bold - such as [USA], [California], [New York], and [Brooklyn].  This type of denormalized structure enables a lot of data redundancy.  \n",
    "\n",
    "-------------------------------------------------------------------\n",
    "id  | store_address    | city          | state       | country\n",
    "1   | 67 First St      | Brooklyn      | New York    | USA\n",
    "2   | 12 Jefferson Rd  | San Francisco | California  | USA\n",
    "3   | 90 Coolidge St   | Log Angeles   | California  | USA\n",
    "4   | 85 Main Ave      | Brooklyn      | New York    | USA\n",
    "5   | 123 Bedford St   | Brooklyn      | New York    | USA\n",
    "\n",
    "\n",
    "If we normalize that previous schema, we got this: we see that although we are using more tables, there is no data redundancy.  The string \"Brooklyn\" is only stored once.  And the state records are stored separately because many cities share the same state, and country.  We don't need to repeat that information, instead, we can one record holding the string Califoria.  [Here we see how normalization eliminates data redundancy]. \n",
    "\n",
    "\n",
    "dim_store_sf                          dim_city_sf                              dim_state_sf\n",
    "---------------------------------     ------------------------------------     ------------------------------------\n",
    "id  | store_address    | city_id      city_id | city_name      | state_id      state_id  | state       | country_id\n",
    "1   | 67 First St      | 2            2       | Brooklyn       | 43            43        | New York    | 121\n",
    "2   | 12 Jefferson Rd  | 3            3       | San Francisco  | 36            36        | California  | 121\n",
    "3   | 90 Coolidge St   | 4            4       | Log Angeles    | 36                       \n",
    "4   | 85 Main Ave      | 2                                                               \n",
    "5   | 123 Bedford St   | 2                                                               \n",
    "\n",
    "\n",
    "__Normalization ensures better data integrity through its design__.  First it enforces data consistency.  Data entry can get messy, and at times people will fill out fields differently.  For example, when referring to California, someone might initials CA.  [Since the states are already entered in a table, we can ensure naming conventions through referential integrity].  Secondly, because duplicates are reduced, [modification of any data becomes safer and simpler].  Say in the previous example, you wanted to update the spelling of a state - you wouldn't have to find each record referring to the state, instead, you could make that change in the states table by altering one record.  From there, you can be confident that the new spelling will be enacted for all stores in that state.  Lastly, [since tables are smaller and organized more by object, its easier to alter the database schema].  You can extend a smaller table without having to alter a larger table holding all the vital data.  \n",
    "\n",
    "\n",
    "\n",
    "To recap, here are the pros and cons of normalization.  Now normalization seems sppealing, especially for database maintenance.  [However, normalization requires a lot more joins making queries more complicated], which can make indexing and reading of data slower.  Deciding between normalization and denormalization comes down to how read- or write- intensive your database is going to be.  Remember OLTP and OLAP?  Can you guess which [OLTP] prefers normalization?  \n",
    "\n",
    "\n",
    "[OLTP is write-intensive meaning we're updating and writing often.  Normalization makes sense because we want to add data quickly and consistently].  OLAP is read-intensive because we're running analytics on the data.  This means we want to prioritize quicker read queries.  Remember how much more joins the normalized query had over the denormalized query?  8 joins to 3 joins.  OLAP should try to avoid that.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282a269b-27d3-4180-bc63-74f43d9e292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT COUNT(a.book_id) \n",
    "FROM fact_booksales a LEFT JOIN dim_store_star b ON a.store_id = b.store_id\n",
    "LEFT JOIN dim_time_star c ON a.time_id = c.time_id\n",
    "LEFT JOIN dim_book_star d ON a.book_id = d.book_id\n",
    "WHERE d.publisher = 'Octavia E. Bulter' AND b.city = 'Vancouver' AND c.year = 2018 AND c.quarter = 4\n",
    "\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "SELECT SUM(quantity) FROM fact_booksales \n",
    "    INNER JOIN dim_store_star ON fact_booksales.store_id = dim_store_star.store_id\n",
    "    INNER JOIN dim_book_star ON fact_booksales.book_id = dim_book_star.book_id\n",
    "    INNER JOIN dim_time_star ON fact_booksales.time_id = dim_time_star.time_id\n",
    "WHERE \n",
    "    dim_store_star.city = 'Vancouver' AND dim_book_star.author = 'Octavia E. Bulter' AND\n",
    "    dim_time_star.yesr = 2018 and dim_time_star.quarter = 4;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f6e245-86d1-44f2-97e1-724bd51586e4",
   "metadata": {},
   "source": [
    "## Querying the star schema\n",
    "\n",
    "The novel genre hasn't been selling as well as your company predicted. To help remedy this, you've been tasked to run some analytics on the novel genre to find which areas the Sales team should target. To begin, you want to look at the total amount of sales made in each state from books in the novel genre.\n",
    "\n",
    "Luckily, you've just finished setting up a data warehouse with the following star schema:\n",
    "\n",
    "                                        dim_book_star\n",
    "                                          book_is    int           PK\n",
    "                                          title      varchar(256)\n",
    "                                          author     varchar(256)\n",
    "                                          publisher  varchar(256)\n",
    "                                          genre      varchar(128)\n",
    "                                                  |\n",
    "                                                 |||\n",
    "dim_store_star                          fact_booksales                   dim_time_star\n",
    "  store_id       int           PK         sales_id      int    PK          time_id      int    PK\n",
    "  store_address  varchar(256)         -   book_id       int    FK -        day          int\n",
    "  city           varchar(128)   -------   time_id       int    FK ------   month        int\n",
    "  state          varchar(128)         -   store_id      int    FK -        quarter      int\n",
    "  country        varchar(128)             sales_amount  float              year         int\n",
    "                                          quantity      int\n",
    "                                          \n",
    "\n",
    "The tables from this schema have been loaded.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "[    Select \"state\" from the appropriate table and the total \"sales_amount\".]\n",
    "    Complete the JOIN on book_id.\n",
    "    Complete the JOIN to connect the dim_store_star table\n",
    "    Conditionally select for books with the genre novel.\n",
    "    Group the results by state.\n",
    "\n",
    "Hint\n",
    "\n",
    "    The JOINs are done on the shared foreign keys between tables. In this schema, these keys end with _id.\n",
    "    The fact_booksales table needs to be joined with dim_store_star and dim_book_star.\n",
    "    The values of genre and state are held in dim_book_star and dim_store_star, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f79147-e356-4fdf-a80e-2042df98d5f2",
   "metadata": {},
   "source": [
    "<img src=\"https://assets.datacamp.com/production/repositories/5311/datasets/75bc5e6de9085df105fd4cd1af69752786096617/book-star.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de615d0d-21e0-4dcd-a20d-d7901f878ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Output each state and their total sales_amount\n",
    "SELECT dim_store_star.state, SUM(sales_amount)\n",
    "FROM fact_booksales\n",
    "\t-- Join to get book information\n",
    "    JOIN dim_book_star on fact_booksales.book_id = dim_book_star.book_id\n",
    "\t-- Join to get store information\n",
    "    JOIN dim_store_star on dim_store_star.store_id = fact_booksales.store_id\n",
    "-- Get all books with in the novel genre\n",
    "WHERE  \n",
    "    dim_book_star.genre = 'novel'\n",
    "-- Group results by state\n",
    "GROUP BY\n",
    "    dim_store_star.state;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e66d38-b6b2-4132-bf79-9af59a6f8bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Output each state and their total sales_amount\n",
    "SELECT dim_store_star.state, sum(sales_amount)\n",
    "FROM fact_booksales\n",
    "\t-- Join to get book information\n",
    "    JOIN dim_book_star on fact_booksales.book_id = dim_book_star.book_id\n",
    "\t-- Join to get store information\n",
    "    JOIN dim_store_star on fact_booksales.store_id = dim_store_star.store_id\n",
    "-- Get all books with in the novel genre\n",
    "WHERE  \n",
    "    dim_book_star.genre = 'novel'\n",
    "-- Group results by state\n",
    "GROUP BY\n",
    "    dim_store_star.state;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bf065e-04f4-49f4-a347-f817c4f21caf",
   "metadata": {},
   "source": [
    "## Querying the snowflake schema\n",
    "\n",
    "Imagine that you didn't have the data warehouse set up. Instead, you'll have to run this query on the company's operational database, which means you'll have to rewrite the previous query with the following snowflake schema:\n",
    "\n",
    "\n",
    "dim_author_sf                           dim_book_sf                      dim_genre_sf\n",
    "  author_id    int          PK        -   book_is    int           PK      genre_id    int          PK\n",
    "  author       varchar(256)     -------   title      varchar(256)  ------  genre       varchar(256)\n",
    "                                      -   author     varchar(256)  -\n",
    "                                          publisher  varchar(256)\n",
    "                                          genre      varchar(128)\n",
    "                                                  |\n",
    "                                                 |||\n",
    "dim_store_sf                            fact_booksales                   dim_time_sf\n",
    "  store_id       int           PK         sales_id      int    PK          time_id      int    PK\n",
    "  store_address  varchar(256)         -   book_id       int    FK -        day          int\n",
    "  city_id        int           FK -----   time_id       int    FK ------   month        int    FK\n",
    "       |||                            -   store_id      int    FK -        quarter      int\n",
    "        |                                 sales_amount  float              year         int\n",
    "dim_city_sf                               quantity      int                  |||\n",
    "  city_id        varchar(128)  PK                                             |\n",
    "  city           varchar(128)                                             dim_month_sf\n",
    "  state          varchar(128)  FK                                           month_id   int   PK\n",
    "                                                                            month      int\n",
    "  \n",
    "\n",
    "The tables in this schema have been loaded. Remember, our goal is to find the amount of money made from the novel genre in each state.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Select state from the appropriate table and the total sales_amount.\n",
    "    Complete the two JOINS to get the genre_id's.\n",
    "    Complete the three JOINS to get the state_id's.\n",
    "    Conditionally select for books with the genre novel.\n",
    "    Group the results by state.\n",
    "\n",
    "Hint\n",
    "\n",
    "    The JOINs are done on the shared foreign keys between tables. In this schema, these keys end with _id.\n",
    "    To get the store information, we need to connect fact_booksales with dim_store_sf, dim_city_sf, and dim_state_sf.\n",
    "    The actual values of genre and state are held in dim_genre_sf and dim_state_sf, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411fabf6-aadc-4e97-8fa2-71459c98bf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Output each state and their total sales_amount\n",
    "SELECT dim_state_sf.state, SUM(fact_booksales.sales_amount)\n",
    "FROM fact_booksales\n",
    "    -- Joins for genre\n",
    "    JOIN dim_book_sf on fact_booksales.book_id = dim_book_sf.book_id\n",
    "    JOIN dim_genre_sf on dim_book_sf.genre_id = dim_genre_sf.genre_id\n",
    "    -- Joins for state \n",
    "    JOIN dim_store_sf on fact_booksales.store_id = dim_store_sf.store_id \n",
    "    JOIN dim_city_sf on dim_store_sf.city_id = dim_city_sf.city_id\n",
    "\tJOIN dim_state_sf on  dim_city_sf.state_id = dim_state_sf.state_id\n",
    "-- Get all books with in the novel genre and group the results by state\n",
    "WHERE  \n",
    "    dim_genre_sf.genre = 'novel'\n",
    "GROUP BY\n",
    "    dim_state_sf.state;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85175baa-b69b-4ade-aa00-f62e99b180b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Output each state and their total sales_amount\n",
    "SELECT dim_state_sf.state, sum(sales_amount)\n",
    "FROM fact_booksales\n",
    "    -- Joins for the genre\n",
    "    JOIN dim_book_sf on fact_booksales.book_id = dim_book_sf.book_id\n",
    "    JOIN dim_genre_sf on dim_book_sf.genre_id = dim_genre_sf.genre_id\n",
    "    -- Joins for the state \n",
    "    JOIN dim_store_sf on fact_booksales.store_id = dim_store_sf.store_id \n",
    "    JOIN dim_city_sf on dim_store_sf.city_id = dim_city_sf.city_id\n",
    "\tJOIN dim_state_sf on  dim_city_sf.state_id = dim_state_sf.state_id\n",
    "-- Get all books with in the novel genre and group the results by state\n",
    "WHERE  \n",
    "    dim_genre_sf.genre = 'novel'\n",
    "GROUP BY\n",
    "   dim_state_sf.state;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ce69d5-9ebd-4f29-a9f6-f3b9334836a3",
   "metadata": {},
   "source": [
    "<img src=\"https://assets.datacamp.com/production/repositories/5311/datasets/0c83b37648663810b40ef2b13503e64a56bbb856/book-snowflake-copy.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0c91b2-022d-4efb-b09b-ac5ffffbe53d",
   "metadata": {},
   "source": [
    "## Updating countries\n",
    "\n",
    "Going through the company data, you notice there are some inconsistencies in the store addresses. These probably occurred during data entry, where people fill in fields using different naming conventions. This can be especially seen in the [country] field, and you decide that countries should be represented by their abbreviations. The only countries in the database are Canada and the United States, which should be represented as [USA] and [CA].\n",
    "\n",
    "In this exercise, you will compare the records that need to be updated in order to do this task on the star and snowflake schema. [dim_store_star] and [dim_country_sf] have been loaded.\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "\n",
    "    Question 1\n",
    "    Output all the records that need to be updated in the star schema so that countries are represented by their abbreviations.\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    How many records would need to be updated in the snowflake schema?\n",
    "Possible Answers\n",
    "    - 18 records\n",
    "    - 2 records\n",
    "    - 1 record\n",
    "    - 0 records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7ba76f-2d01-4b93-b31a-b2ca5854253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Output records that need to be updated in the star schema\n",
    "SELECT * FROM ___\n",
    "WHERE country != 'USA' AND country !='CA';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ce32ab-6fb9-4cd8-aef8-1e031aed30f9",
   "metadata": {},
   "source": [
    "## Extending the snowflake schema\n",
    "\n",
    "The company is thinking about extending their business beyond bookstores in Canada and the US. Particularly, they want to expand to a new continent. In preparation, you decide a continent field is needed when storing the addresses of stores.\n",
    "\n",
    "[Luckily, you have a snowflake schema in this scenario. As we discussed in the video, the snowflake schema is typically faster to extend while ensuring data consistency]. Along with dim_country_sf, a table called dim_continent_sf has been loaded. It contains the only continent currently needed, North America, and a primary key. In this exercise, you'll need to extend dim_country_sf to reference dim_continent_sf.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "[    Add a `continent_id` column to \"dim_country_sf\" with a default value of 1. Note thatNOT NULL DEFAULT(1) constrains a value from being null and defaults its value to 1.\n",
    "    Make that new column a foreign key reference to dim_continent_sf's continent_id.\n",
    "\n",
    "Hint\n",
    "\n",
    "    Because there are only stores in Canada and the US, we can default the value to be 1.\n",
    "    All the alterations are being done to dim_country_sf.\n",
    "    dim_country_sf needs to have a foreign key reference to dim_continent_sf's continent_id.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1f644a-3b0c-4bf2-b919-42d28d048394",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Add a continent_id column with default value of 1\n",
    "ALTER TABLE dim_country_sf\n",
    "ADD continent_id int NOT NULL DEFAULT(1);\n",
    "\n",
    "-- Add the foreign key constraint -----------------------------------------------------------------------------------\n",
    "ALTER TABLE dim_country_sf ADD CONSTRAINT country_continent\n",
    "   FOREIGN KEY (continent_id) REFERENCES dim_continent_sf(continent_id);\n",
    "#####################################################################################################################\n",
    "\n",
    "-- Output updated table\n",
    "SELECT * FROM dim_country_sf;\n",
    "\n",
    "\n",
    "\n",
    "ALTER TABLE dim_author ADD COLUMN author_id SERIAL PRIMARY KEY   ----------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea387c1d-8e11-462b-a923-bdc0870be660",
   "metadata": {},
   "source": [
    "## Normal forms\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Earlier we introduced the concept of normalization.  We reviewed the simplified definition (So what is normalization?  [Normalization is a technique that divides tables into smaller tables and connects them via relationships]  __The goal is to reduce redundancy and increase data integrity__  So how does this happen?  There are several forms of normalization, which we'll delve into later.  __But the basic idea is to identify repeating groups of data and create new tables for them__  ).  Here is a more formal definition provided by Andrienne Watt.  \n",
    "\n",
    "[    Identify repeating groups of data and create new tables for them\n",
    "    The goal of normalization are to:\n",
    "    Be able to characterize the level of redundancy in a relational schema\n",
    "    Provide mechanism for transforming schemas in order to remove redundancy]\n",
    "\n",
    "So what are these levels?  There are different extents to which you can normalize.  These are called normal forms.  Below is a list of them from least to most normalized.  Each has its own set of rules, and some build on top of each other.  We'll only cover the first 3 normal forms.  \n",
    "\n",
    "First normal form (1NF),                    Forth normal form (4NF)\n",
    "Second normal form (2NF),                   Essential tuple normal form (ETNF),\n",
    "Third normal form (3NF),                    Fifth normal form (5NF)\n",
    "Elementary key normal form (EKNF),          Domain-key normal form (DKNF),\n",
    "Boyce-codd normal form (DCNF).              Sixth normal form (6NF),\n",
    "\n",
    "\n",
    "Lets begain with first normal form (1NF).  [To comply with 1NF, each record must be unique and each cell must hold one value].  Below is a simple table with student ids and emails, along with courses completed.  All these rows are unique, but the courses_completed column has more than one course in two records.  \n",
    "\n",
    "------------------------------------------------------------------------------------\n",
    "Student_id  | Student_Email     | Courses_Completed\n",
    "235         | jim@gmail.com     | Introduction to Python, Intermediate Python\n",
    "455         | kelly@yahoo.com   | Cleaning Data in R\n",
    "767         | amy@hotmail.com   | Machine Learning Toolbox, Deep Learning in Python\n",
    "\n",
    "[To rectify this, we can split the original table as such.  Now all the records are unique and each column has one value.  ]\n",
    "\n",
    "----------------------------------\n",
    "Student_id  | Student_Email\n",
    "235         | jim@gmail.com   \n",
    "455         | kelly@yahoo.com \n",
    "767         | amy@hotmail.com \n",
    "\n",
    "-----------------------------------------\n",
    "Student_id  | Completed\n",
    "235         | Introduction to Python\n",
    "235         | Intermediate Python\n",
    "455         | Cleaning Data in R\n",
    "767         | Machine Learning Toolbox\n",
    "767         | Deep Learning in Python\n",
    "\n",
    "\n",
    "[Next is 2NF, which is must satisfy 1NF.  Beyond that, if the promary key is one column, the the table in 2NF].  __A composite primary key is when a primary key is made up of two or more columns__.  If the table has a composite promary key, then each non-key column must be dependent on all the keys.  Lets look at a concrete example.  __In below table, we have the student and course is as a composite primary key.  We then review the other columns and their dependence on these two keys.  First is instructor, which isn't dependent on the student_id - only the course_id.  Meaning an instructor solely depends on the course, not the students who take the course.  The same goes for the instructor_id column.  However the rcent completedin dependet on both the studen and the course_id__.  \n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "Student_id (PK)  | Course_id (PK) | Instructor_id | Instructor     | Progress\n",
    "235              | 2001           | 560           | Nick Carchedi  | 0.55\n",
    "455              | 2345           | 658           | Ginger Grant   | 0.10\n",
    "767              | 6584           | 999           | Chester Ismay  | 1.00\n",
    "\n",
    "To convert it, we can create two new tables that satisfy the conditions of 2NF.  \n",
    "\n",
    "----------------------------------------------\n",
    "Student_id (PK)  | Course_id (PK) | Progress\n",
    "235              | 2001           | 0.55\n",
    "455              | 2345           | 0.10\n",
    "767              | 6584           | 1.00\n",
    "\n",
    "---------------------------------------------\n",
    " Course_id (PK) | Instructor_id | Instructor\n",
    " 2001           | 560           | Nick Carchedi\n",
    " 2345           | 658           | Ginger Grant\n",
    " 6584           | 999           | Chester Ismay\n",
    "\n",
    "\n",
    "On to 3NF, where you might've correctly guessed require 2NF to be satisfied.  3NF doesn't allow transitive dependencies.  This means that non-primary key columns can't depend on other non-primary key columns.  Lets take a look at an example.  __Course_id is the primary key so we can ignore this column.  [Isstructor_id and instructor definitely depend on each other].  Teach doesn't depend on the instructor as an instructor can teach different technologies__.  We can replace the table from before into these two tables to meet 3NF criteria.  \n",
    "\n",
    "------------------------------------------------------------\n",
    "Course_id (PK) | Instructor_id | Instructor    | Teach\n",
    "2001           | 560           | Nick Carchedi | Python\n",
    "2345           | 658           | Ginger Grant  | SQL\n",
    "6584           | 999           | Chester Ismay | R\n",
    "\n",
    "\n",
    "--------------------------------------------\n",
    "Course_id (PK) | Instructor    | Teach\n",
    "2001           | Nick Carchedi | Python\n",
    "2345           | Ginger Grant  | SQL\n",
    "6584           | Chester Ismay | R\n",
    "\n",
    "------------------------------\n",
    " Instructor_id | Instructor   \n",
    " 560           | Nick Carchedi\n",
    " 658           | Ginger Grant \n",
    " 999           | Chester Ismay\n",
    "\n",
    "[These tables have no transitive dependencies and they also meet 3NF as there are no composite primary keys].  We've covered these first 3 normal forms that increase in normalization.  Its time to consider why we would want to put effect into normalizing a database even more.  Why isn't 1NF enough?  A database that isn't normalized enough is prone to three of anomaly errors: update, insertion and deletion.  \n",
    "\n",
    "An update anomaly is a data inconsistency that can arise when updating a database with redundancy.  For example, take this simple table.  It holds the ids and emails of students, and their enrolled courses.  __If we want to update the email of student 520, we would have to update multiple records__.  It may sound easy to update multiple records, but its risky because it depends on the user updating- if they remember this redundancy.  And this is a simple example - as we scale, its harder to keep track of these redundancies.  \n",
    "An insertion anomaly is when you are unable to add a new record due to missing attributes.  For example, [if a student signs up for DataCamp but doesn't start any courses, they cannot be put into this database].  The only excaption is if the enrolled_in column can accept nulls.  The dependency between columns in the same table unintentionally restricts what can be inserted into the table.  \n",
    "[A deletion anomaly happens when you delete a record and unintentionally delete other data].  For example, if you were to delete any of these students, you would loose the course information provided in the columns enrolled_in and taught_by.  This coule be resolved if we put that information in another table.  \n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "Student_ID | Student_Email   | Enrolled_in            | Taught_by\n",
    "230        | lisa@gmail.com  | Cleaning Data in R     | Maggie Matsui\n",
    "367        | bob@hotmail.com | Data Visualizationin R | Ronald Pearson\n",
    "520        | ken@yahoo.com   | Introduction to Python | Hugo Bowne-Andreson\n",
    "520        | ken@yahoo.com   | Arima Models in R      | David Stoffer\n",
    "\n",
    "\n",
    "The more normalized the database, the less prone it will be to these anomalies.  For example, most 3NF tables cant have an update, insertion and deletion anomalies.  This makes normalization sound great.  But dont forget the downsides of normalization from the last video.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec870309-3dd6-4550-bb88-74d26b3d7981",
   "metadata": {},
   "source": [
    "## Converting to 1NF\n",
    "\n",
    "In the next three exercises, you'll be working through different tables belonging to a car rental company. Your job is to explore different schemas and gradually increase the normalization of these schemas through the different normal forms. At this stage, we're not worried about relocating the data, but rearranging the tables.\n",
    "\n",
    "A table called \"customers\" has been loaded, which holds information about customers and the cars they have rented.\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "\n",
    "    Question 1\n",
    "    Does the customers table meet 1NF criteria?\n",
    "Possible Answers\n",
    "\n",
    "    Yes, all the records are unique.\n",
    "    No, because there are multiple values in cars_rented and invoice_id\n",
    "    No, because the non-key columns such as don't depend on customer_id, the primary key.\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    \"cars_rented\" holds one or more \"car_ids\" and \"invoice_id\" holds multiple values. Create a new table to hold individual \"car_ids\" and \"invoice_ids\" of the \"customer_ids\" who've rented those cars.\n",
    "    Drop two columns from customers table to satisfy 1NF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173c2c66-b1d4-4a10-bc25-a7dc348f1245",
   "metadata": {},
   "outputs": [],
   "source": [
    " [To comply with 1NF, each record must be unique and each cell must hold one value]----------------------------------\n",
    "\n",
    "\n",
    "-- Create a new table to hold the cars rented by customers  ---------------------------------------------------------\n",
    "CREATE TABLE cust_rentals (\n",
    "  customer_id INT NOT NULL,\n",
    "  car_id VARCHAR(128) NULL,\n",
    "  invoice_id VARCHAR(128) NULL\n",
    ");\n",
    "\n",
    "\n",
    "-- Drop column from customers table to satisfy 1NF  -----------------------------------------------------------------\n",
    "ALTER TABLE customers\n",
    "DROP COLUMN cars_rented,\n",
    "DROP COLUMN invoice_id;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab35885-b1ff-4b60-b239-2246027998e0",
   "metadata": {},
   "source": [
    "## Converting to 2NF\n",
    "\n",
    "Let's try normalizing a bit more. In the last exercise, you created a table holding \"customer_ids\" and \"car_ids\". This has been expanded upon and the resulting table, \"customer_rentals\", has been loaded for you. Since you've got 1NF down, it's time for 2NF.\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "\n",
    "    Question 1\n",
    "    Why doesn't customer_rentals meet 2NF criteria?\n",
    "Possible Answers\n",
    "\n",
    "    Because the end_date doesn't depend on all the primary keys.\n",
    "    Because there can only be at most two primary keys.\n",
    "    Because there are non-key attributes describing the car that only depend on one primary key, car_id.\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    Create a new table for the non-key columns that were conflicting with 2NF criteria.\n",
    "    Drop those non-key columns from customer_rentals.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7693f2-95aa-4dcc-ab18-7b372553da4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[Next is 2NF, which is must satisfy 1NF.  Beyond that, if the promary key is one column, the the table in 2NF]-------\n",
    "\n",
    "\n",
    "-- Create a new table to satisfy 2NF\n",
    "CREATE TABLE cars (\n",
    "  car_id VARCHAR(256) NULL,\n",
    "  model VARCHAR(128),\n",
    "  manufacturer VARCHAR(128),\n",
    "  type_car VARCHAR(128),\n",
    "  condition VARCHAR(128),\n",
    "  color VARCHAR(128)\n",
    ");\n",
    "\n",
    "-- Drop columns in customer_rentals to satisfy 2NF\n",
    "ALTER TABLE customer_rentals \n",
    "DROP COLUMN model,\n",
    "DROP COLUMN manufacturer, \n",
    "DROP COLUMN type_car,\n",
    "DROP COLUMN condition,\n",
    "DROP COLUMN color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce68677-5b4d-4643-aaca-2633e3699ea1",
   "metadata": {},
   "source": [
    "## Converting to 3NF\n",
    "\n",
    "Last, but not least, we are at 3NF. In the last exercise, you created a table holding car_idss and car attributes. This has been expanded upon. For example, car_id is now a primary key. The resulting table, rental_cars, has been loaded for you.\n",
    "\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "\n",
    "    Question 1\n",
    "    Why doesn't rental_cars meet 3NF criteria?\n",
    "Possible Answers\n",
    "\n",
    "    Because there are two columns that depend on the non-key column, model.\n",
    "    Because there are two columns that depend on the non-key column, color.\n",
    "    Because 2NF criteria isn't satisfied.\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    Create a new table for the non-key columns that were conflicting with 3NF criteria.\n",
    "    Drop those non-key columns from rental_cars.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac85077-12d8-445a-b3e8-751c38b456f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "[3NF doesn't allow transitive dependencies.  \n",
    " This means that non-primary key columns can't depend on other non-primary key columns.]\n",
    "\n",
    "\n",
    " \n",
    "-- Create a new table to satisfy 3NF\n",
    "CREATE TABLE car_model(\n",
    "  model VARCHAR(128),\n",
    "  manufacturer VARCHAR(128),\n",
    "  type_car VARCHAR(128)\n",
    ");\n",
    "\n",
    "-- Drop columns in rental_cars to satisfy 3NF\n",
    "ALTER TABLE rental_cars\n",
    "DROP COLUMN manufacturer,\n",
    "DROP COLUMN type_car; \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02438922-e44f-4ffa-934e-0b802026a2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1e91fd8-053c-4780-af33-a4e39019635c",
   "metadata": {},
   "source": [
    "## Database views\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In this chapter, we'll learn about database views.  So what are views?  Wikipedia provides the following definition: __In a database, a view is the result set of stored query on the data, which the database users can query just as they would in a persistent collection object__.  Essentially, [views are virtual tables that are not part of the physical schema].  A view isn't stored in physical memory; instead, the query to create the view is.  The data in a view comes from data in tables of the same database.  Once a view is created, you can query it like a regular table.  [The benefit of a view is that you don't need to retype common queries].  It allows you to add virtual tables without altering the database's schema.  \n",
    "\n",
    "Views are simple to create.  You take the query of interest and add a line before it to name the view, as such.  \n",
    "\n",
    "    CREATE VIEW view_name AS\n",
    "    SELECT col1, col2\n",
    "    FROM table_name\n",
    "    WHERE condition;\n",
    "\n",
    "Here is an example, this is part of last chapter's snowflake schema.  \n",
    "\n",
    "                                 dim_publisher_sf\n",
    "                                   publisher_id  int         PK\n",
    "                                   publisher     varchar(256)\n",
    "                                       |\n",
    "                                      |||\n",
    "dim_author_sf                    dim_book_sf                        dim_genre_sf\n",
    "  author_id   int         PK    -  book_id       int         PK -     genre_id    int         PK\n",
    "  author      varchar(256)    ---  title         varchar(256)   ---   genre       varchar(128)\n",
    "                                -  author_id     int         FK -\n",
    "                                   publisher_id  int         FK\n",
    "                                   genre_id      int         FK\n",
    "\n",
    "Lets say analysts at your company are often running analytics on the science fiction genre.  To help their workflow, you want to create a view specifically dedicated to the science fiction genre and its associated book titles and authors.  The query would look something like this.  \n",
    "\n",
    "SELECT title, author, genre\n",
    "FROM dim_book_sf\n",
    "JOIN dim_genre_sf ON dim_genre_sf.genre_id = dim_book_sf.genre_id\n",
    "JOIN dim_author_sf ON dim_author_sf.author_id = dim_book_sf.author_id\n",
    "WHERE dim_genre_sf.genre = 'science fiction';\n",
    "\n",
    "[To convert this query into a view, you would add a CREATE VIEW statement like so.  ]\n",
    "\n",
    "\n",
    "CREATE VIEW scifi_books AS  =========================================================================================\n",
    "SELECT title, author, genre\n",
    "FROM dim_book_sf\n",
    "JOIN dim_genre_sf ON dim_genre_sf.genre_id = dim_book_sf.genre_id\n",
    "JOIN dim_author_sf ON dim_author_sf.author_id = dim_book_sf.author_id\n",
    "WHERE dim_genre_sf.genre = 'science fiction';\n",
    "\n",
    "\n",
    "After executing the code from the last slide, you can query the view.  The [scifi_books] isn't a real table with physical memory.  When we run this SELECT statement, the following query is actually being run.  Its important to keep track of the views in your database.  To get all the views in your database, you can run a query on the INFORMATION_SCHEMA.views table.  Note that this command is specific to PostgreSQL.  If you are using another DBMS, look at its documentation to find the equivalent command.  \n",
    "\n",
    "[SELECT * FROM INFORMATION_SCHEMA.views;]\n",
    "\n",
    "If you run this command, you will get a long list of views.  Thats because DBMS's have their own built-in views.  To exclude views and to get to views you've created, use this query.  It excludes views from pg_catalog and information_schema which are built-in view categories.  \n",
    "\n",
    "SELECT * FROM INFORMATION_schema.views\n",
    "WHERE table_schema NOT IN ('pg_catalog', 'information_schema');\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Can you see why views are useful?  First off, a view doesn't take up any storage except for the query statement, which is minimal.  [Views act as a form of access control].  For example, instead of giving a user access to columns that may have sensitive information, you can restrict what they can see via a view.  [Perhaps most importantly, views mask the complexity of queries].  Remember those snowflake schema from the last chapter and their joins?  Views are handy for views normalized past the 2NF.  You can make those common joins - such as aggregating dates or genres - into views.  The users of your database will thank you for views because they won't have to spend as much time thinking about how to join tables.  \n",
    "\n",
    "In this chapter' exercises, we'll be using a database of Pitchfork reviews from Kaggle.  Pitchfork is a music magazine that publish magazine that publishes reviews.  The database schema looks like this.  The main table Reviews holds the url of the review, the title of the work being reviewed, and the score it received.  It has more information on the author of the review and date of publication.  The reviewid field is a foreign key to tables: Content, Genres, Artist, and Labels.  Content holds the text of the review.  \n",
    "\n",
    "\n",
    "Content               Reviews                          Artist\n",
    "  reviewid  INT         reviewid        INT              reviewid  INT\n",
    "  content   TEXT        title           TEXT             artist    TEXT\n",
    "                        url             TEXT   \n",
    "                        score           REAL\n",
    "Genres                  best_new_music  INT            Labels\n",
    "  reviewid  INT         author          TEXT             reviewid  INT\n",
    "  genre     TEXT        author_type     TEXT             label     TEXT\n",
    "                        pub_date        TEXT\n",
    "                        pub_weekday     INT\n",
    "                        pub_day         INT\n",
    "                        pub_month       INT\n",
    "                        pub_year        INT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365c1f7b-2648-4394-becf-fa2cbb70382b",
   "metadata": {},
   "source": [
    "## Tables vs. views\n",
    "\n",
    "Views have been described as \"virtual tables\". It's true that views are similar to tables in certain aspects, but there are key differences. In this exercise, you will organize these differences and similarities.\n",
    "Instructions\n",
    "100XP\n",
    "\n",
    "    Organize these characteristics into the category that they describe best.\n",
    "\n",
    "Hint\n",
    "\n",
    "    Views store queries, while tables store records.\n",
    "\n",
    "Only Tables:\n",
    "    Part of the physical schema of a database\n",
    "    Contains rows and columns    [Views & Tables]\n",
    "\n",
    "Views & Tables:\n",
    "    Can be queried\n",
    "\n",
    "Only Views:\n",
    "    Has access control    [Views & Tables]\n",
    "    Takes up less memory\n",
    "    Always definedby a query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679003d9-e332-41f5-a3d6-6a6929bafce7",
   "metadata": {},
   "source": [
    "## Viewing views\n",
    "\n",
    "Because views are very useful, it's common to end up with many of them in your database. It's important to keep track of them so that database users know what is available to them.\n",
    "\n",
    "The goal of this exercise is to get familiar with viewing views within a database and interpreting their purpose. This is a skill needed when writing database documentation or organizing views.\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "\n",
    "    Question 1\n",
    "    Query the information schema to get views.\n",
    "    Exclude system views in the results.\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    What does view1 do?\n",
    "Possible Answers\n",
    "    -Returns the content records with reviewids that have been viewed more than 4000 times.\n",
    "    -Returns the content records that have reviews of more than 4000 characters.\n",
    "    -Returns the first 4000 records in content.\n",
    "Answer : Returns the content records that have reviews of more than 4000 characters.\n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "    What does view2 do?\n",
    "Possible Answers\n",
    "    -Returns 10 random reviews published in 2017.\n",
    "    -Returns the top 10 lowest scored reviews published in 2017.\n",
    "    -Returns the top 10 highest scored reviews published in 2017.\n",
    "Answer : Returns the top 10 highest scored reviews published in 2017.\n",
    "\n",
    "Hint\n",
    "\n",
    "    A system view has a table_schema with the value pg_catalog or information_schema.\n",
    "    information_schema.views is the table that needs to be queried.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2720cc7-b726-4cbe-9212-f891bc7d5462",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Get all non-systems views\n",
    "SELECT * FROM INFORMATION_SCHEMA.views\n",
    "WHERE table_schema NOT IN ('pg_catalog', 'information_schema');\n",
    "\n",
    "\n",
    "\n",
    "table_catalog\ttable_schema\ttable_name\tview_definition\t        check_option\tis_updatable\tis_insertable_into\tis_trigger_updatable\tis_trigger_deletable\tis_trigger_insertable_into\n",
    "dataarchpost\tpublic\tview1\t SELECT content.reviewid,\n",
    "                              content.content\n",
    "                              FROM content\n",
    "                              WHERE (length(content.content) > 4000);\tNONE\tYES\tYES\tNO\tNO\tNO\n",
    "\n",
    "dataarchpost\tpublic\tview2\t SELECT reviews.reviewid,\n",
    "                              reviews.title,\n",
    "                              reviews.score\n",
    "                              FROM reviews\n",
    "                              WHERE (reviews.pub_year = 2017)\n",
    "                              ORDER BY reviews.score DESC\n",
    "                              LIMIT 10;\t                              NONE\tNO\tNO\tNO\tNO\tNO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77061d5a-2ae3-4e36-b698-f05d16085330",
   "metadata": {},
   "source": [
    "## Creating and querying a view\n",
    "\n",
    "[Have you ever found yourself running the same query over and over again? Maybe, you used to keep a text copy of the query in your desktop notes app, but that was all before you knew about views!]\n",
    "\n",
    "In these Pitchfork reviews, we're particularly interested in high-scoring reviews and if there's a common thread between the works that get high scores. In this exercise, you'll make a view to help with this analysis so that we don't have to type out the same query often to get these high-scoring reviews.\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "\n",
    "    Question 1\n",
    "    Create a view called high_scores that holds reviews with scores above a 9.\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    Count the number of records in high_scores that are self-released in the label field of the labels table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6adaa0-2058-4055-babe-da33e15cba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Create a view for reviews with a score above 9\n",
    "CREATE VIEW high_scores AS\n",
    "SELECT * FROM Reviews\n",
    "WHERE score > 9;\n",
    "\n",
    "\n",
    "-- Create a view for reviews with a score above 9\n",
    "CREATE VIEW high_scores AS\n",
    "SELECT * FROM REVIEWS\n",
    "WHERE score > 9\n",
    "\n",
    "-- Count the number of self-released works in high_scores  ----------------------------------------------------------\n",
    "SELECT COUNT(*) FROM high_scores\n",
    "INNER JOIN labels ON labels.reviewid = high_scores.reviewid\n",
    "WHERE label = 'self-released'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf6a011-a184-4cae-bffb-b147d943785a",
   "metadata": {},
   "source": [
    "## Managing views\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now that we've got a grasp on what views are, lets dig deeper.  We've kept our views simple.  Its worth pointing out that views can get as complicated and creative as you choose.  Think of the aggregation JOIN, and conditional functions and clauses available to you in SQL.  Of course, the query to create the view still has to run, so you need to be aware of long query execution time.  \n",
    "\n",
    "[\n",
    "Aggregation:  SUM(), AVG(), COUNT(), MIN(), MAX(), GROUP BY, etc\n",
    "Joins:        INNER JOIN, LEFT JOIN, RIGHT JOIN, FULL JOIN\n",
    "Conditionals: WHERE, HAVING, UNIQUE, NOT NULL, AND, OR, <, >, etc\n",
    "]\n",
    "\n",
    "In the last video, we talked about how views are helpful for access control.  To give and remove user premissions, we use the SQL [GRANT and REVOKE] command.  The syntax is as follows.  First you list the relevant privileges after the GRANT and the REVOKE command.  There are several types of privileges users can execute, and here we list the most common.  Then you indicate on which object and for which role.  You use the TO clause and FROM clause, respectively for GRANT and REVOKE.  In chapter4, we'll go more in-depth about different types of roles and privieges.  For now, we just need a high-level understanding.  \n",
    "\n",
    "[\n",
    "Privileges:  SELECT, INSERT, UPDATE, DELETE, etc\n",
    "Objects:     table, view, schema, etc\n",
    "Roles:       a database user or a group of database users\n",
    "]\n",
    "\n",
    "GRANT privilege(s) or REVOKE privige(s)\n",
    "ON object\n",
    "TO role or FROM role\n",
    "\n",
    "\n",
    "Here is an example.  The update priviege on an object called \"ratings\" is being granted to public.  PUBLIC is a SQL term that encompasses all users.  __All users can now use the UPDATE command on the ratings object__.  In the second line, the db_user will no longer be able to INSERT on the object films.  \n",
    "\n",
    "[\n",
    "GRANT UPDATE ON ratings TO public;\n",
    "][\n",
    "REVOKE INSERT ON films FROM db_user;\n",
    "]\n",
    "\n",
    "\n",
    "A user can update a view if they have the necessary priviege.  If you need a refresher, here is a simple example of the UPDATE command.  You may be wondering, how is it even possible to update a view?  That's a good question because if you remember correctly, a view isn't a physical table.  [Therefore, when you run an UPDATE, you are updating the tables behind the view.  Hence, only particular views are updatable.]  There are criteria for viewo be considered updatable.  The criteria depend on the type of SQL being used.  Generally, the view needs to be made up of one table and can't rely on a window or aggregate function.  \n",
    "\n",
    "[\n",
    "UPDATE films SET kind='Dramatic' WHERE kind='Drama'\n",
    "]\n",
    "\n",
    "\n",
    "The INSERT command is in a similar case as the UPDATE command.  When you run an INSERT command into a view, you're again really inserting into the table behind it.  The criteria for inserting is usually very similar to updatable views.  Generally, [avoid modifying data through views].  Its usually a good idea to use views for read-only purposes only.  Dropping a view is straightforward with the DROP command.  There are 2 useful parameters to know about: CASCADE and RESTRICT.  Sometimes there are SQL objects that depend on views.  For example, its not unusual for views to build off of other views in larger databases.  __The [RESTRICT] parameter is the default and returns an error when you try to drop a view that other objects depend on__.  The [CASCADE] parameter will drop the view and any object that depends on that view.  \n",
    "\n",
    "[\n",
    "DROP VIEW view_name [CASCADE | RESTRICT];\n",
    "]\n",
    "\n",
    "Say you want to change the query a view is defined by.  To do this, you can use the CREATE OR REPLACE command.  If a view_name exsits, it is replaced by the new_query specified.  __However, there are limitations to this.  The new query must generate the same column names, column order, and column data types as the existing query__.  The column output may be different, as long as those conditions are met.  [New columns may be added at the end].  If this criteria can't be met, the solution is to drop the existing view and create a new one.  \n",
    "\n",
    "[\n",
    "CREATE OR REPLACE VIEW view_name AS new_query\n",
    "]\n",
    "\n",
    "\n",
    "Last but not least, the auxiliary properties of a view can be altered.  We list the various options here.  [This includes changing the name, owner, and schema of a view.  ]\n",
    "\n",
    "[\n",
    "ALTER VIEW [IF EXIST] name ALTER [COLUMN] column_name SET DEFAULT expression\n",
    "ALTER VIEW [IF EXIST] name ALTER [COLUMN] column_name DROP DEFAULT \n",
    "ALTER VIEW [IF EXIST] name OWNER TO new_owner\n",
    "ALTER VIEW [IF EXIST] name RENAME TO new_name\n",
    "ALTER VIEW [IF EXIST] name SET SCHEMA new_schema\n",
    "ALTER VIEW [IF EXIST] name SET (view_option_name [=view_option_value] [, ...])\n",
    "ALTER VIEW [IF EXIST] name RESET (view_option_name [, ...])\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1408feed-f39b-4ca1-99cf-00022d2e5be5",
   "metadata": {},
   "source": [
    "## Creating a view from other views\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Views can be created from queries that include other views. This is useful when you have a complex schema, potentially due to normalization, because it helps reduce the JOINS needed. The biggest concern is keeping track of dependencies, specifically how any modifying or dropping of a view may affect other views.\n",
    "\n",
    "In the next few exercises, we'll continue using the Pitchfork reviews data. There are two views of interest in this exercise. top_15_2017 holds the top 15 highest scored reviews published in 2017 with columns reviewid,title, and score. artist_title returns a list of all reviewed titles and their respective artists with columns reviewid, title, and artist. From these views, we want to create a new view that gets the highest scoring artists of 2017.\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "\n",
    "    Question 1\n",
    "[    Create a view called \"top_artists_2017\" with artist from \"artist_title\".\n",
    "    To only return the highest scoring artists of 2017, join the views \"top_15_2017\" and \"artist_title\" on \"reviewid\".]\n",
    "    Output top_artists_2017.\n",
    "    \n",
    "    Hint\n",
    "\n",
    "    The top_artists_2017 view should be created solely from the two other views top_15_2017 and artist_title described in the context.\n",
    "    Query the top_artists_2017 and artist_title view to see their contents.\n",
    "    reviewid is a shared field between top_15_2017 and artist_title.\n",
    "\n",
    "    \n",
    "    Question 2\n",
    "    Which is the DROP command that would drop both top_15_2017 and top_artists_2017?\n",
    "Possible Answers\n",
    "    - DROP VIEW top_15_2017 CASCADE;\n",
    "    - DROP VIEW top_15_2017 RESTRICT;\n",
    "    - DROP VIEW top_artists_2017 RESTRICT;\n",
    "#    - DROP VIEW top_artists_2017 CASCADE;    Recall what we learned before, what the 2 parameters? and its functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8efce97-1be7-4f31-ba5a-ad5cc7c0329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Create a view with the top artists in 2017\n",
    "CREATE VIEW top_artists_2017 AS\n",
    "-- with only one column holding the artist field\n",
    "SELECT artist_title.artist FROM artist_title\n",
    "INNER JOIN top_15_2017\n",
    "ON artist_title.reviewid = top_15_2017.reviewid;\n",
    "\n",
    "-- Output the new view\n",
    "SELECT * FROM top_artists_2017;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6416c1e6-c45f-4cdc-8cf5-603fe25a680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Create a view with the top artists in 2017\n",
    "CREATE VIEW top_artists_2017 AS\n",
    "-- with only one column holding the artist field\n",
    "SELECT artist_title.artist FROM artist_title\n",
    "INNER JOIN top_15_2017\n",
    "ON artist_title.reviewid = top_15_2017.reviewid;\n",
    "\n",
    "-- Output the new view\n",
    "SELECT * FROM top_artists_2017;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929a7b14-dbeb-4bf6-8fe7-e1cbb3f01c6e",
   "metadata": {},
   "source": [
    "## Granting and revoking access\n",
    "\n",
    "Access control is a key aspect of database management. Not all database users have the same needs and goals, from analysts, clerks, data scientists, to data engineers. As a general rule of thumb, write access should never be the default and only be given when necessary.\n",
    "\n",
    "In the case of our Pitchfork reviews, we don't want all database users to be able to write into the long_reviews view. Instead, the editor should be the only user able to edit this view.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Revoke all database users' update and insert privileges on the long_reviews view.\n",
    "    Grant the editor user update and insert privileges on the long_reviews view.\n",
    "\n",
    "Hint\n",
    "\n",
    "    Use PUBLIC to indicate all database users.\n",
    "    REVOKE and GRANT use different clauses, TO and FROM, to indicate which role's privilege is being modified.\n",
    "    The two queries in this exercise affect different users. We want to revoke privileges from PUBLIC and grant privileges to editor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27e3315-3951-4f4a-b1ed-e917a6444672",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Revoke everyone's update and insert privileges\n",
    "REVOKE UPDATE, INSERT ON long_reviews FROM PUBLIC;   ----------------------------------------------------------------\n",
    "\n",
    "-- Grant the editor update and insert privileges \n",
    "GRANT UPDATE, INSERT ON long_reviews TO editor;   -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2b41c2-2c39-4ead-83a4-382bb6ff75fa",
   "metadata": {},
   "source": [
    "## Updatable views\n",
    "\n",
    "In a previous exercise, we've used the [information_schema.views] to get all the views in a database. If you take a closer look at this table, you will notice a column that indicates whether the view is updatable.\n",
    "\n",
    "Which views are updatable?\n",
    "Instructions\n",
    "50 XP\n",
    "Possible Answers\n",
    "\n",
    "    long_reviews and top_25_2017\n",
    "    top_25_2017\n",
    "    long_reviews\n",
    "    top_25_2017 and artist_title\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be884a57-35af-4687-bfaf-93a436abf664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1981/142799945.py:19: SADeprecationWarning: The Engine.table_names() method is deprecated and will be removed in a future release.  Please refer to Inspector.get_table_names(). (deprecated since: 1.4)\n",
      "  print(engine.table_names()[:10])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#Tableau_48_sid_00000FD8_2_Connect_CheckSelectIntoCap', 'AB_Test_Participants', 'AB_Tests', 'AB_Tests_Unlock_Per_Level_18', 'AB_Tests_Variants', 'AB_Tests_Variants_Values', 'CaixaAposta', 'Jogadas', 'Jogos', 'Letras']\n",
      "(169, 10)\n",
      "    TABLE_CATALOG      TABLE_SCHEMA  \\\n",
      "0             def  gms_analytics_db   \n",
      "1             def  gms_analytics_db   \n",
      "2             def  gms_analytics_db   \n",
      "3             def  gms_analytics_db   \n",
      "4             def  gms_analytics_db   \n",
      "..            ...               ...   \n",
      "164           def               sys   \n",
      "165           def               sys   \n",
      "166           def               sys   \n",
      "167           def               sys   \n",
      "168           def               sys   \n",
      "\n",
      "                                    TABLE_NAME  \\\n",
      "0    vw_ab_test_participants_math_wo_yesterday   \n",
      "1       vw_ab_test_participants_math_yesterday   \n",
      "2         vw_ab_test_participants_wo_yesterday   \n",
      "3            vw_ab_test_participants_yesterday   \n",
      "4     vw_club_category_change_log_wo_yesterday   \n",
      "..                                         ...   \n",
      "164       x$wait_classes_global_by_avg_latency   \n",
      "165           x$wait_classes_global_by_latency   \n",
      "166                 x$waits_by_host_by_latency   \n",
      "167                 x$waits_by_user_by_latency   \n",
      "168                  x$waits_global_by_latency   \n",
      "\n",
      "                                       VIEW_DEFINITION CHECK_OPTION  \\\n",
      "0    select `gms_analytics_db`.`ab_test_participant...         NONE   \n",
      "1    select `gms_analytics_db`.`ab_test_participant...         NONE   \n",
      "2    select `gms_analytics_db`.`AB_Test_Participant...         NONE   \n",
      "3    select `gms_analytics_db`.`AB_Test_Participant...         NONE   \n",
      "4    select `gms_analytics_db`.`club_category_chang...         NONE   \n",
      "..                                                 ...          ...   \n",
      "164  select substring_index(`performance_schema`.`e...         NONE   \n",
      "165  select substring_index(`performance_schema`.`e...         NONE   \n",
      "166  select if(isnull(`performance_schema`.`events_...         NONE   \n",
      "167  select if(isnull(`performance_schema`.`events_...         NONE   \n",
      "168  select `performance_schema`.`events_waits_summ...         NONE   \n",
      "\n",
      "    IS_UPDATABLE              DEFINER SECURITY_TYPE CHARACTER_SET_CLIENT  \\\n",
      "0            YES                app@%       DEFINER                 utf8   \n",
      "1            YES                app@%       DEFINER                 utf8   \n",
      "2            YES                app@%       DEFINER                 utf8   \n",
      "3            YES                app@%       DEFINER                 utf8   \n",
      "4            YES                app@%       DEFINER                 utf8   \n",
      "..           ...                  ...           ...                  ...   \n",
      "164           NO  mysql.sys@localhost       INVOKER                 utf8   \n",
      "165           NO  mysql.sys@localhost       INVOKER                 utf8   \n",
      "166          YES  mysql.sys@localhost       INVOKER                 utf8   \n",
      "167          YES  mysql.sys@localhost       INVOKER                 utf8   \n",
      "168          YES  mysql.sys@localhost       INVOKER                 utf8   \n",
      "\n",
      "    COLLATION_CONNECTION  \n",
      "0        utf8_general_ci  \n",
      "1        utf8_general_ci  \n",
      "2        utf8_general_ci  \n",
      "3        utf8_general_ci  \n",
      "4        utf8_general_ci  \n",
      "..                   ...  \n",
      "164      utf8_general_ci  \n",
      "165      utf8_general_ci  \n",
      "166      utf8_general_ci  \n",
      "167      utf8_general_ci  \n",
      "168      utf8_general_ci  \n",
      "\n",
      "[169 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, insert, MetaData, Table, Column, Integer, Float, Boolean, String\n",
    "\n",
    "\n",
    "host='.us-east-1.rds.amazonaws.com'\n",
    "port=3306\n",
    "dbname='_db'\n",
    "user='p'\n",
    "password='jm90'\n",
    "\n",
    "import urllib.parse\n",
    "password = urllib.parse.quote_plus(\"90\")  \n",
    "################################################################\n",
    "\n",
    "\n",
    "engine = create_engine(f\"mysql+pymysql://{user}:{password}@{host}:{port}/{dbname}\")\n",
    "\n",
    "print(engine.table_names()[:10])\n",
    "\n",
    "\n",
    "query = \"\"\"SELECT * FROM INFORMATION_SCHEMA.views\"\"\"\n",
    "\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\n",
    "        query\n",
    "    )\n",
    "    df = pd.DataFrame(rs.fetchall())\n",
    "    df.columns = rs.keys()\n",
    "\n",
    "print(df.shape)\n",
    "print(df)\n",
    "\n",
    "# WHERE DATEDIFF(GET_DATE(), a2.time) <=7  /*a2.time<=NOW() AND a2.time>=DATE_SUB(NOW(), INTERVAL 7 DAY)*/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879cfe16-e0ef-4f86-ad3f-dfcbbd42bcfa",
   "metadata": {},
   "source": [
    "## Redefining a view\n",
    "\n",
    "Unlike inserting and updating, redefining a view doesn't mean modifying the actual data a view holds. Rather, it means modifying the underlying query that makes the view. In the last video, we learned of two ways to redefine a view: (1) CREATE OR REPLACE and (2) DROP then CREATE. CREATE OR REPLACE can only be used under certain conditions.\n",
    "\n",
    "The artist_title view needs to be appended to include a column for the label field from the labels table.\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "\n",
    "    Question 1\n",
    "    Can the CREATE OR REPLACE statement be used to redefine the artist_title view?\n",
    "Possible Answers\n",
    "\n",
    "    Yes, as long as the label column comes at the end.\n",
    "    No, because the new query requires a JOIN with the labels table.\n",
    "    No, because a new column that did not exist previously is being added to the view.\n",
    "    Yes, as long as the label column has the same data type as the other columns in artist_title\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    Redefine the \"artist_title\" view to include a column for the \"label\" field from the \"labels\" table.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28505ce-c58c-4e4d-93f8-92744f103e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Redefine the artist_title view to have a label column\n",
    "CREATE OR REPLACE  VIEW artist_title AS\n",
    "SELECT reviews.reviewid, reviews.title, artists.artist, labels.label\n",
    "FROM reviews\n",
    "INNER JOIN artists\n",
    "ON artists.reviewid = reviews.reviewid\n",
    "INNER JOIN labels\n",
    "ON labels.reviewid = reviews.reviewid;\n",
    "\n",
    "SELECT * FROM artist_title;\n",
    "\n",
    "reviewid\ttitle\tartist\tlabel\n",
    "22703\tmezzanine\tmassive attack\tvirgin\n",
    "22721\tprelapsarian\tkrallice\thathenter\n",
    "22659\tall of them naturals\turanium club\tfashionable idiots\n",
    "22659\tall of them naturals\turanium club\tstatic shock\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c108852-8833-472b-a370-baa01b645fdd",
   "metadata": {},
   "source": [
    "## Materialized views\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You now know what views are and how to manage them.  In this video, we'll introduce [materialized views].  There are 2 types of views.  When you come across the term \"view\" plainly, it is most likely referring to non-materialized views.  Accordingly, in the past 2 videos covering views, we defined views as non-materialized views.  In this video, we will look into materialized views.  As the names begin to hint, [materialized views are physically materialized, while non-materialized remain virtual].  What does this mean?  \n",
    "\n",
    "Instead of storing a query, __a materialized view stores the query results__.  These query results are stored on disk.  This means the query becomes precomputed via the view.  When you query a materialized view, it accesses the stored query results on the disk, rather than running the query like non-materialized view and creating a virtual table.  [Materialized views are refreshed or rematerialized when prompted].  By refreshed or rematerialized, it means that the query is run and the stored query results are updated (__recall what we did on Tableau QSL query customized table, and using that table to create the sheet and chart to dashboard, we can set it as extract or live__).  \n",
    "\n",
    "This can be scheduled depending on how often you expect the underlying query results are changing.  __At DataCamp, some of the views are refreshed once-a-day during non-working hours, and others are refreshed every hour__.  [Materialized views are great if you have queries with long execution time.  Some querys takes hours to complete if you are crunching a lot of data or have complex joins].  \n",
    "\n",
    "Materialized views allow data scientists and analysts to run long queries and get results very quickly.  The caveat is the data is only as up-to-date as the last time the view was refreshed.  So you shouldn't use materialized views on data that is being updated often (table locking can be caused by many, such as frequent disk IO, its a big topic [Google it later on]), because then analyses will be run too often on out-of-date data.  __Materialized views are particularly useful in data warehouses__(not data lake remember).  Data warehouses are typically used for OLAP, meaning more for analysis than writing to data.  This means less worry about out-of-date data.  \n",
    "\n",
    "[Furthermore, the same queries are often run in data warehouses, and the computational cost of them can add up].  Creating materialized views is very similar to creating non-materialized views, except that you specify \"MATERIALIZED\" in the SQL statement.  \n",
    "\n",
    "[\n",
    "CREAT MATERIALIZED VIEW my_mv AS \n",
    "SELECT * FROM existing_table;\n",
    "]\n",
    "[\n",
    "REFRESH MATERIALIZED VIEW my_mv; \n",
    "]\n",
    "\n",
    "There isn't a PostgreSQL command to schedule refreshing views.  However, [there are several ways to do so, like using cron jobs] [Google this topic].  We won't get into details of cron as it is outside of the scope of this course, but cron is a UNIX based job scheduler.  As we learned in the last video, its common to build views from other views.  The same can be said about materialized views.  Unlike non-materialized views, __you need to manage when you refresh materialized views when you have dependencies__.  For example, say we have 2 materialized views, X and Y.  Y uses X in its query(How to write this query statement, can you recall this, we did this on previous non-materialized views): meaning Y depends on X.  and X doesn't depend on Y as it doesn't use Y in its query.  [Lets say X has more time-consuming query].  If Y is refreshed before X's refresh is completed, the Y now has out-of-date data.  This creates a dependency chain when refreshing views.  Scheduling when to refresh is not trivial.  Refreshing them all at the same time is not the most efficient when you consider query time and dependencies.  \n",
    "\n",
    "__Companies that have many materialized views, use directed acyclic graphs to track dependencies and pipeline scheduler tools, like Airflow and Luigi, to schedule and run REFRESH statements__.  A directed acyclic graph, also know as DAG, is a finite directed graph with no cycles.  Here you can see an example where the directed arrows reflect a dependency in a certain direction where one node depends on another.  The no cycles part is important because two views can't depend on each other - only one can rely on another\n",
    "\n",
    "--------------\n",
    "|             \\\n",
    "* ---- * ---- * ---- * ---- * ---- *\n",
    "       |             /\n",
    "       --------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d068929-b914-4a36-9d09-ae4f95f3c197",
   "metadata": {},
   "source": [
    "## Materialized versus non-materialized\n",
    "\n",
    "Materialized and non-materialized are two distinct categories of views. In this exercise, you will organize their differences and similarities.\n",
    "Instructions\n",
    "100XP\n",
    "\n",
    "    Organize these characteristics into the category that they describe best.\n",
    "\n",
    "Non-Materialized Views:\n",
    "    Always returns up-to-date data\n",
    "\n",
    "\n",
    "Non-Materialized & Materialized Views:\n",
    "    Can be used in a data warehouse\n",
    "    Helps reduce the overhead of writing queries\n",
    "\n",
    "Materialized Views:\n",
    "    Consumes more storage\n",
    "    Better to use on write-intensive databases   [Non-materialized Views]\n",
    "    Stores the query result on disk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0606e98d-b679-45dc-8c24-3dca8c2c5c29",
   "metadata": {},
   "source": [
    "## Creating and refreshing a materialized view\n",
    "\n",
    "[The syntax for creating materialized and non-materialized views are quite similar] because they are both defined by a query. One key difference is that we can refresh materialized views, while no such concept exists for non-materialized views(__like a live connecting between your SQL query customized table to your database in Tableau__). It's important to know how to refresh a materialized view, otherwise the view will remain a snapshot of the time the view was created (__should be at least daily update__).\n",
    "\n",
    "In this exercise, you will create a materialized view from the table genres. A new record will then be inserted into genres. To make sure the view has the latest data, it will have to be refreshed.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Create a materialized view called genre_count that holds the number of reviews for each genre.\n",
    "    Refresh genre_count so that the view is up-to-date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fa6384-6aba-49ff-9d23-464b775d7036",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Create a materialized view called genre_count \n",
    "CREATE MATERIALIZED VIEW genre_count AS\n",
    "SELECT genre, COUNT(*) \n",
    "FROM genres\n",
    "GROUP BY genre;\n",
    "\n",
    "INSERT INTO genres\n",
    "VALUES (50000, 'classical');\n",
    "\n",
    "-- Refresh genre_count\n",
    "REFRESH MATERIALIZED VIEW genre_count;\n",
    "\n",
    "SELECT * FROM genre_count;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f2fceb-3955-46ee-8ad2-295137da24b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Create a materialized view called genre_count \n",
    "CREATE MATERIALIZED VIEW genre_count AS\n",
    "SELECT genre, COUNT(*) \n",
    "FROM genres\n",
    "GROUP BY genre;\n",
    "\n",
    "INSERT INTO genres\n",
    "VALUES (50000, 'classical');\n",
    "\n",
    "-- Refresh genre_count\n",
    "REFRESH MATERIALIZED VIEW genre_count;\n",
    "\n",
    "SELECT * FROM genre_count;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ea2d3a-27c4-4ea8-80bf-5ae19a97b347",
   "metadata": {},
   "source": [
    "## Managing materialized views\n",
    "\n",
    "Why do companies use pipeline schedulers, such as Airflow and Luigi, to manage materialized views?\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "\n",
    "    To set up a data warehouse and make sure tables have the most up-to-date data.\n",
    "    1\n",
    "#    To refresh materialized views with consideration to dependences between views.\n",
    "    2\n",
    "    To convert non-materialized views to materialized views.\n",
    "    3\n",
    "    To prevent the creation of new materialized views when there are too many dependencies.\n",
    "    4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d81292b-24f4-4ac4-a8c3-5b9f4b6c68f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e9daf6b-b59b-4026-84a9-0cbce8ac60b8",
   "metadata": {},
   "source": [
    "## Database roles and access control\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Welcome back.  In this chapter, we're going to cover different topics within database management, beginning with database roles and access control.  Recall last chapter, we learned how views are helpful for access control.  We granted and revoked a user's access to a view (or any object).  We also briefly discussed roles and privileges in the context of access control.  Now we'll dive deeper.  \n",
    "\n",
    "[\n",
    "GRANT privilege(s)   or   REVOKE privilege(s)\n",
    "ON object\n",
    "TO role    or   FROM role\n",
    "]\n",
    "[\n",
    "Privileges: SELECT, INSERT, UPDATE, DELETE, etc.\n",
    "Object:     table, view, schema, etc.\n",
    "Role:       a database user or a group of database users\n",
    "]\n",
    "\n",
    "GRANT UPDATE ON ratings TO PUBLIC;\n",
    "REVOKE INSERT ON films FROM db_user;\n",
    "\n",
    "\n",
    "First, __database roles.  Roles are used to manage database access permissions__.  A database role is an entity that contains information that.  Firstly define its privileges, like whether that role can login, create databases, and many more.  Secondly, interact with the client authentication system, like what the role's password is.  [Roles can be assigned to one or more users].  Since roles are global, you can reference roles across all individual databases in your cluster (WHat she's talking about?  the system role like root or database user? I mean the SQLite user cant be also the MySQL user?).  Say you are about to hire a bunch of data analysts.  You can create the data_analyst role with the [CREATE ROLE] SQl command.  The information that defines what the data_analyst role can do is currently empty.  We can also set some, but not all, of this information when creating a role too (__Recall what we leant in previous videos, GRAND and REVOKE command__).  \n",
    "\n",
    "[\n",
    "CREATE data_analyst;\n",
    "]\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Say you're hiring an intern whose internship ends at the end of the year.  You can create the roleintern, specify the password and valid untill data attribute.  Once second into 2023, the password is no longer valid.  Say you want to create an admin role with the ability to \n",
    "\n",
    "[\n",
    "CREATE ROLE intern WITH PASSWORD 'InternPassword123' VALID UNTIL '2023-01-01'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f0547c-e715-42e9-b37e-de4e18b4339b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6bbef3-c91a-47aa-9a41-88da0841306a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf74579-a58a-4357-8ccf-a8f76c16ca98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b6f5a7-9c71-4bf2-8d72-3fb30b67f0b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a934687-2538-46e4-9c07-abf68489f5d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651d6342-4ea0-4022-9e6a-0951272bcef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee0b74f-d4a0-4585-8e44-1dc640c5a2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b8cd87-b8a9-42cd-bc45-002140922c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9694616c-d3e7-42be-932d-8b44a32e0f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47f2dfb-cc03-471e-8151-82a4ce9927bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc543945-3361-49f9-ad49-542e42cc7755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cc60a6-5501-4d6e-81a5-3b871ae85fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b429e1f-9b9a-4237-9241-3d834299b1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec48346c-3ab7-44ec-9fd7-4ba19b98d537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e9354c-3d87-4286-91c2-db1ad4338171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ec1553-55cf-4c22-bd3a-3536b39ba808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c71ca2-a02e-4a39-93b4-fb84d74c7423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647549cf-c3f9-4a74-90ef-5dff2fa7d43b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed703d43-a5a9-443e-8fce-67de40b8f4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f3c0e3-e2fb-44a1-98a9-9822c83f0ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552eded5-7b1f-4ea8-bb4e-03f04e3d9d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa2248f-6f95-421a-8eb3-153428979729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffd0d71-55a8-4dcf-9fe9-5f42dc7adfa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f875bd88-be41-47e9-90f6-fe44968a6bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b8ae6b-c83d-47d6-a603-145d02546404",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7852dc31-179b-4e0e-a497-aee946fc2d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8cedb0-99d6-4d0d-ae08-a4c5f3edf6af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9276286-028e-4c5e-b443-e95e03fd2a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9639c2d-3f89-4e6b-a4cd-e628a53302aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d91633-347d-488a-80bf-c4e09057d6cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2d368d-ca9a-4ce6-8314-c3507f47caa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef9a601-edca-4c21-bef0-4ba260e3ecc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5f135f-dbcf-4d99-90b5-889cf3812ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d12be5-1c13-45e8-a99e-8937b318fc96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37feb076-875b-40a1-81d1-8a006e9b6163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61493f8c-83eb-4639-abc3-85ffa60280df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9038bec1-1c45-4a91-9f53-2dd2b802236c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cc0d2e-6154-4bef-bdd0-d565a7c4f167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782fcf0f-4f5c-4a47-85a8-edd9ba4b9ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2778f39d-5aa1-4e44-a709-3d2043d2510b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee257bf-bb35-40fc-9a12-bf4e8abdf9de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb972b5-5a64-4da3-b4d1-4df8324ecf24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5495c8-1308-4055-ad02-6508569ee800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12c9f40-2d8c-44a2-ae40-d6008e89afb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c06d14-c188-4cc6-bc3d-8bf72c647edc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdca797c-8074-4de7-8160-dcf0e33e9b11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215b51f5-da6a-42c0-a2fe-77cc8e339c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fa7527-1924-415a-aaea-b1875d9e8f66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1904ab37-9bc3-4129-a67b-a42d9b6a5940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d8bfcf-e92b-4602-8616-833d90d50a53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6825074f-be6f-43a3-b360-9ed88f0e7b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe41589-00fa-4f22-a698-abe66f60ffd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d75cfb-94ab-4c81-9181-d2c4e2c88679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcb097e-51dd-418a-bdf5-0195012a8f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc30f03-afe2-44a8-9580-41f4e56c7991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3870bf8-b60d-40fa-9573-186675e7a79f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f214e7-6f7d-472b-b9ba-048e8e5ff1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff09158c-9356-4d0c-8654-64dcdf9124a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdaf51a-6e23-46b0-9a97-0edbee64add2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ff3e22-e42a-44ed-91c9-c85bb817937c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d523c4-dad0-4891-97a0-c743e389955a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d83fece-4fd1-4fa3-9490-99aeb7ca2fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddae06c-2abe-4d93-8d2e-de15c98ece7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ef480a-04d6-4651-b4b0-9ba567ea8af8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a918d718-9468-4b5e-b253-df25ee9587b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dff143-2dfc-40dc-be2b-e2c847c6209b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57e79b5-107b-4e72-a9e5-9a758b40a607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af8288e-f0d9-4b15-9c4f-0307b8de736e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661f2bdc-73b8-4f23-9ea7-7116231f82c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ddaf1b-524e-49d8-84f9-891043d8f435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0760a6-06a8-4ae0-be87-c92b0aa2ae93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0be024-c175-4a9b-9d76-6b40ad8705f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
