{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5979dc4a-cbce-4841-b0d6-233720f7a93c",
   "metadata": {},
   "source": [
    "## Data Processing in Shell\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72466dcd-ec1d-4659-8e93-2bd45937d7d2",
   "metadata": {},
   "source": [
    "## Course Description\n",
    "\n",
    "We live in a busy world with tight deadlines. As a result, we fall back on what is familiar and easy, favoring GUI interfaces like Anaconda and RStudio. However, taking the time to learn data analysis on the command line is a great long-term investment because it makes us stronger and more productive data people.\n",
    "\n",
    "In this course, we will take a practical approach to learn simple, powerful, and data-specific command-line skills. Using publicly available Spotify datasets, we will learn how to download, process, clean, and transform data, all via the command line. We will also learn advanced techniques such as command-line based SQL database operations. Finally, we will combine the powers of command line and Python to build a data pipeline for automating a predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3f0354-567d-45f2-91f4-cd0979631c9d",
   "metadata": {},
   "source": [
    "##  Downloading Data on the Command Line\n",
    "Free\n",
    "0%\n",
    "\n",
    "In this chapter, we learn how to download data files from web servers via the command line. In the process, we also learn about documentation manuals, option flags, and multi-file processing.\n",
    "\n",
    "    Downloading data using curl    50 xp\n",
    "    Using curl documentation    50 xp\n",
    "    Downloading single file using curl    100 xp\n",
    "    Downloading multiple files using curl    100 xp\n",
    "    Downloading data using Wget    50 xp\n",
    "    Installing Wget    50 xp\n",
    "    Downloading single file using wget    100 xp\n",
    "    Advanced downloading using Wget    50 xp\n",
    "    Setting constraints for multiple file downloads    50 xp\n",
    "    Creating wait time using Wget    100 xp\n",
    "    Data downloading with Wget and curl    100 xp \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a52087-c821-4826-9234-3a2425168d12",
   "metadata": {},
   "source": [
    "##  Data Cleaning and Munging on the Command Line\n",
    "0%\n",
    "\n",
    "We continue our data journey from data downloading to data processing. In this chapter, we utilize the command line library csvkit to convert, preview, filter and manipulate files to prepare our data for further analyses.\n",
    "\n",
    "    Getting started with csvkit    50 xp\n",
    "    Installation and documentation for csvkit    100 xp\n",
    "    Converting and previewing data with csvkit    100 xp\n",
    "    File conversion and summary statistics with csvkit    100 xp\n",
    "    Filtering data using csvkit    50 xp\n",
    "    Printing column headers with csvkit    100 xp\n",
    "    Filtering data by column with csvkit    100 xp\n",
    "    Filtering data by row with csvkit    100 xp\n",
    "    Stacking data and chaining commands with csvkit    50 xp\n",
    "    Stacking files with csvkit    100 xp\n",
    "    Chaining commands using operators    100 xp\n",
    "    Data processing with csvkit    100 xp \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da0d1d8-c0bb-4d63-80b6-cce48c1a38ab",
   "metadata": {},
   "source": [
    "##  Database Operations on the Command Line\n",
    "0%\n",
    "\n",
    "In this chapter, we dig deeper into all that csvkit library has to offer. In particular, we focus on database operations we can do on the command line, including table creation, data pull, and various ETL transformation.\n",
    "\n",
    "    Pulling data from database    50 xp\n",
    "    Using sql2csv documentation    50 xp\n",
    "    Understand sql2csv connectors    50 xp\n",
    "    Practice pulling data from database    100 xp\n",
    "    Manipulating data using SQL syntax    50 xp\n",
    "    Applying SQL to a local CSV file    100 xp\n",
    "    Cleaner scripting via shell variables    100 xp\n",
    "    Joining local CSV files using SQL    100 xp\n",
    "    Pushing data back to database    50 xp\n",
    "    Practice pushing data back to database    100 xp\n",
    "    Database and SQL with csvkit    100 xp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724bd0dd-666b-43f0-a5ee-0a99d9c9eec9",
   "metadata": {},
   "source": [
    "##  Data Pipeline on the Command Line\n",
    "0%\n",
    "\n",
    "In the last chapter, we bridge the connection between command line and other data science languages and learn how they can work together. Using Python as a case study, we learn to execute Python on the command line, to install dependencies using the package manager pip, and to build an entire model pipeline using the command line.\n",
    "\n",
    "    Python on the command line    50 xp\n",
    "    Finding Python version on the command line    50 xp\n",
    "    Executing Python script on the command line    100 xp\n",
    "    Python package installation with pip    50 xp\n",
    "    Understanding pip's capabilities    50 xp\n",
    "    Installing Python dependencies    100 xp\n",
    "    Running a Python model   100 xp\n",
    "    Data job automation with cron    50 xp\n",
    "    Understanding cron scheduling syntax    50 xp\n",
    "    Scheduling a job with crontab    100 xp\n",
    "    Model production on the command line    100 xp\n",
    "    Course recap    50 xp \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcf13b7-649f-4f3e-aeb2-63d58baf47d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1a28e0a-ad88-4a3f-b236-34527b025385",
   "metadata": {},
   "source": [
    "## Downloading data using curl\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Welcome to Intermediate Shell.  My name is Susan Sun, and I do data work.  I'm looking forward to learning with you in this course.  In data, many of us bypass the command line in favor of GUI interfaces like Anaconda and RStudio because that is what we are familiar with.  However, taking the time to learn data science on the command line is a great long term investment that will, ultimately, make us better and more productive data people.  \n",
    "\n",
    "\n",
    "In this course, we take a practical approach and learn command line tools useful for everyday data processing and analyses.  First, lets learn how to download data files using curl.  The \"curl\" is short for Client for URLs, is a UNIX command line tool for transferring data to and from a server.  It is often used to download data from HTTP sites and FTP servers.  To check if \"curl\" has properly installed, type the following in the command line: \"man curl\".  If \"curl\" has not been installed, you will see: \"curl command not found\".  To install curl, Google it.  If \"curl\" is installed, your console will look like normal man help pages.  You can keep pressing Enter to scroll through the curl manual.  To exit and return to your console, press q.  \n",
    "\n",
    "The basic syntax for curl has the following structure: \"curl [optional flags] [URL]\".  The URL is required  for the command to run successfully.  The \"curl\" supports a large number of protocal calls.  (including HTTP, HTTPS, FTP, SFTP etc).  For the full list using the \"curl --help\".  Lets download a single file stored at this hypothetical URL using curl.  To save the file with its original name \"datafilename.txt\", use the optional flag \"-O\" (dash uppercase O).  This reads \"curl -O URL\".  To save the file under a different name, replace -O (dash uppercase O) with -o (dash lowercase o) and new file name.  Now it reads \"curl -o newname URL\".  \n",
    "\n",
    "Often times, a server will host multiple data files, with similar filenames.  Like with different ending values.  Instead of curl each file individually, we can use wildcards (do you remember what we learned in introduction to shell course) to download all the files at once.  To download every file hostedon this server that starts with datafilename and end in \".txt\", we use: \"curl -o URLsomething*.txt\".  \n",
    "\n",
    "Another option is to increment using a globbing parser.  The following will download every files sequentially starting with data \"filename001.txt\" ane ending with data \"filename100.txt\".  Note that the end of the command that reads: open square bracket zero zero one dash one hundread close square bracket dot txt.  That is the globbing at work.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# curl -O https://websitename.com/datafilename[001-100].txt\n",
    "#                                             *********\n",
    "\n",
    "\n",
    "We can increment through the files and download every Nth file.  For example, to download every 10th file, we can modify the globbing parser to read: open square bracket zero zero one dash one hundred colon ten close square bracket dot txt.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# curl -O https://websitename.com/datafilename[001-100:10].txt\n",
    "#                                             ************\n",
    "\n",
    "\n",
    "# Sometimes internet can time out.  To make sure that our download progress is not lost, \n",
    "# *******************************************************************************************************************\n",
    "curl has these two flags: \n",
    "\"-L\" redirects the HTTP URL if a 300 error code occurs.  \n",
    "\"-C\" resumes a previous file transfer if it times out before completion.  \n",
    "Putting everything together.  Note that all option flags come before URL, but the order of the flags does not matter.  \n",
    "\n",
    "\n",
    "\n",
    "In this lesson, we learned how to download files using curl.  Lets put our new knowledge to practice.  Happy crul.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408bf61d-3275-468e-826b-1d8bd2569781",
   "metadata": {},
   "outputs": [],
   "source": [
    "jhu@debian:~$ curl -O https://assets.datacamp.com/production/repositories/4180/datasets/513986f5ea7ed9a8565bba20d088d21c10e099dc/Spotify_MusicAttributes.csv > ~/Downloads/Spotify_MusicAttributes.csv\n",
    "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
    "                                 Dload  Upload   Total   Spent    Left  Speed\n",
    "100  1717  100  1717    0     0   1382      0  0:00:01  0:00:01 --:--:--  1382\n",
    "jhu@debian:~$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b86aa54-6f85-4d13-a275-2414f7247f2f",
   "metadata": {},
   "source": [
    "## Using curl documentation\n",
    "\n",
    "As you work with command line tools you will often need to consult the documentation to remind yourself of the syntax or of some of the available functionality. In this exercise, you'll consult curl's documentation to answer this question:\n",
    "\n",
    "Based on the information in the curl manual, which of the following is NOT a supported file protocol:\n",
    "Instructions\n",
    "50 XP\n",
    "Possible Answers\n",
    "\n",
    "    LDAP\n",
    "    FTPS\n",
    "    HTTPS\n",
    "#    OFTP\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb0b824-a4d8-4867-a1af-f2c81cb76700",
   "metadata": {},
   "outputs": [],
   "source": [
    "jhu@debian:~$ curl --help\n",
    "Usage: curl [options...] <url>\n",
    " -d, --data <data>   HTTP POST data\n",
    " -f, --fail          Fail silently (no output at all) on HTTP errors\n",
    " -h, --help <category> Get help for commands\n",
    " -i, --include       Include protocol response headers in the output\n",
    " -o, --output <file> Write to file instead of stdout\n",
    " -O, --remote-name   Write output to a file named as the remote file\n",
    " -s, --silent        Silent mode\n",
    " -T, --upload-file <file> Transfer local FILE to destination\n",
    " -u, --user <user:password> Server user and password\n",
    " -A, --user-agent <name> Send User-Agent <name> to server\n",
    " -v, --verbose       Make the operation more talkative\n",
    " -V, --version       Show version number and quit\n",
    "\n",
    "This is not the full help, this menu is stripped into categories.\n",
    "Use \"--help category\" to get an overview of all categories.\n",
    "For all options use the manual or \"--help all\".\n",
    "jhu@debian:~$ man curl\n",
    "DESCRIPTION\n",
    "       curl  is  a tool to transfer data from or to a server, using one of the\n",
    "       supported protocols (DICT, FILE, FTP, FTPS, GOPHER, HTTP, HTTPS,  IMAP,\n",
    "       IMAPS,  LDAP,  LDAPS,  MQTT, POP3, POP3S, RTMP, RTMPS, RTSP, SCP, SFTP,\n",
    "       SMB, SMBS, SMTP, SMTPS, TELNET and TFTP). The command  is  designed  to\n",
    "       work without user interaction.\n",
    "\n",
    "       curl offers a busload of useful tricks like proxy support, user authen‐\n",
    "       tication, FTP upload, HTTP post, SSL connections, cookies, file  trans‐\n",
    "       fer  resume,  Metalink,  and more. As you will see below, the number of\n",
    "       features will make your head spin!\n",
    "\n",
    "       curl is powered by  libcurl  for  all  transfer-related  features.  See\n",
    "       libcurl(3) for details.\n",
    "\n",
    "PROTOCOLS\n",
    "       curl supports numerous protocols, or put in URL  terms:  schemes.  Your\n",
    "       particular build may not support them all.\n",
    "\n",
    "       DICT   Lets you lookup words using online dictionaries.\n",
    "\n",
    "       FILE   Read  or  write  local  files.  curl  does not support accessing\n",
    "              file:// URL remotely, but when running on Microsft Windows using\n",
    "              the native UNC approach will work.\n",
    "\n",
    "       FTP(S) curl  supports  the  File Transfer Protocol with a lot of tweaks\n",
    "              and levers. With or without using TLS.\n",
    "\n",
    "       GOPHER Retrieve files.\n",
    "\n",
    "       HTTP(S)\n",
    "              curl supports HTTP with numerous options and variations. It  can\n",
    "              speak HTTP version 0.9, 1.0, 1.1, 2 and 3 depending on build op‐\n",
    "              tions and the correct command line options.\n",
    "\n",
    "       IMAP(S)\n",
    "              Using the mail reading protocol, curl can \"download\" emails  for\n",
    "              you. With or without using TLS.\n",
    "\n",
    "       LDAP(S)\n",
    "              curl can do directory lookups for you, with or without TLS.\n",
    "\n",
    "       MQTT   curl supports MQTT version 3. Downloading over MQTT equals \"sub‐\n",
    "              scribe\" to a topic while uploading/posting equals \"publish\" on a\n",
    "              topic.  MQTT  support  is experimental and TLS based MQTT is not\n",
    "              supported (yet).\n",
    "\n",
    "       POP3(S)\n",
    "              Downloading from a pop3 server means getting  a  mail.  With  or\n",
    "              without using TLS.\n",
    "\n",
    "       RTMP(S)\n",
    "              The  Realtime  Messaging  Protocol  is  primarily used to server\n",
    "              streaming media and curl can download it.\n",
    "\n",
    "       RTSP   curl supports RTSP 1.0 downloads.\n",
    "\n",
    "       SCP    curl supports SSH version 2 scp transfers.\n",
    "\n",
    "       SFTP   curl supports SFTP (draft 5) done over SSH version 2.\n",
    "\n",
    "       SMB(S) curl supports SMB version 1 for upload and download.\n",
    "\n",
    "       SMTP(S)\n",
    "              Uploading contents to an SMTP server  means  sending  an  email.\n",
    "              With or without TLS.\n",
    "\n",
    "       TELNET Telling curl to fetch a telnet URL starts an interactive session\n",
    "              where it sends what it reads  on  stdin  and  outputs  what  the\n",
    "              server sends it.\n",
    "\n",
    "       TFTP   curl can do TFTP downloads and uploads.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4298c7-a2df-4dd7-b324-c0ae07485c15",
   "metadata": {},
   "source": [
    "## Downloading single file using curl\n",
    "\n",
    "Let's get some hands on practice for the more commonly used options and flags with curl. \n",
    "# The URL for the hosted file is a shortened URL using tinyurl. Because of that, we need to fill out a flag option that allows for redirected URLs.\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "\n",
    "    Question 1\n",
    "#    Fill in the option flag that allow downloading from a redirected URL.\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    In the same step as the download, add in the necessary syntax to rename the downloaded file as Spotify201812.zip.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7a1c61-32ac-4c50-8aa6-d005d9040656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use curl to download the file from the redirected URL\n",
    "curl -L -o Spotify201812.zip https://assets.datacamp.com/production/repositories/4180/datasets/eb1d6a36fa3039e4e00064797e1a1600d267b135/201812SpotifyData.zip\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec27918-f8cf-466e-8b9b-c2c4c278b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jhu@debian:~$ cd ~/Downloads/\n",
    "jhu@debian:~/Downloads$ ls\n",
    "Spotify_MusicAttributes.csv\n",
    "Training_Machine_Learning_Surrogate_Models_From_a_.pdf\n",
    "jhu@debian:~/Downloads$ curl -L -o Spotify201812.zip https://assets.datacamp.com/production/repositories/4180/datasets/eb1d6a36fa3039e4e00064797e1a1600d267b135/201812SpotifyData.zip\n",
    "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
    "                                 Dload  Upload   Total   Spent    Left  Speed\n",
    "100 1944k  100 1944k    0     0   863k      0  0:00:02  0:00:02 --:--:--  863k\n",
    "jhu@debian:~/Downloads$ ls\n",
    "new_file  Spotify201812.zip  Spotify_MusicAttributes.csv  Training_Machine_Learning_Surrogate_Models_From_a_.pdf\n",
    "jhu@debian:~/Downloads$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997951d2-43c6-4232-a1fa-8d2e6cde6cb4",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Exercise\n",
    "Downloading multiple files using curl\n",
    "\n",
    "We have 100 data files stored in long sequentially named URLs. Scroll right to see the complete URLs.\n",
    "\n",
    "https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile001.txt\n",
    "https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile002.txt\n",
    "......\n",
    "https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile100.txt\n",
    "\n",
    "To minimize having to type the long URLs over and over again, we'd like to download all of these files using a single curl command.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Download all 100 data files using a single curl command.\n",
    "    Print all downloaded files to directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9afc010-a929-48fb-adc0-19868410e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all 100 data files\n",
    "curl -O https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile[001-100].txt\n",
    "#                                                                                                        #########\n",
    "\n",
    "# Print all downloaded files to directory\n",
    "ls datafile*.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7266c9e5-a188-4d3c-85a2-43afcd5b3153",
   "metadata": {},
   "source": [
    "## Downloading data using Wget\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Welcome back, in this lesson, we will introduce another command line tool for downloading data, called Wget.  We will walk through how to install and set up Wget along with some basic usage.  Wget derives its name from World Wide Web and Get.  It is a GNU project native to the Linux system, but is compatible across all operating systems.  It is another command line tool that will help you download files via HTTP and FTP.  \n",
    "\n",
    "\n",
    "# Compared to \"curl\", Wget is more multi-purpose.  It can download a single file, an entire folder, or even a webpage.  \n",
    "Most importantly, it makes multiple file downloads possible recursively.  Aside from using man, another way to check is Wget has been installed correctly, is by using \"which wget\" (just like Bash and Dash?).  This will return the location of where Wget is installed.  For example, in the local user bin: If Wget has not been installed, there will simply be no output.  For official documentation and source code of Wget, Google it.  Unless you are comfortable compiling from the source code, here are some easier alternatives.  \n",
    "\n",
    "For Linux users, it is likely Wget is already installed for you.  If not, run \"sudo apt-get install wget\", just Google it.  For Mac users, use homebrew by running \"brew install wget\".  For Windows users, this will not be a command line install.  Rather, download as part of the gunwin32 package.  Once the installation is complete, use the man command to print the Wget manual.  \n",
    "\n",
    "The basic syntax for Wget has a similar structure to curl: \"wget [optional flags] [URL]\".  The URL is also required for the Wget command to run successfully (isn't that obviously? we are doing URL request).  Wget supports a large number of protocal calls for data stored on servers.  For the full list of the options available, refer to \"wget --help\" or \"man wget\" or ask Google.  \n",
    "\n",
    "\n",
    "# Here are some option flags unique to Wget: \n",
    "\"-b\" allows your download to run in the background. \n",
    "\"-q\" turns off the wget output, which saves some disk spaces. \n",
    "\"-c\" is useful to finish up a previously broken download wheather by Wget or another program. \n",
    "\n",
    "Finally, you can link all the option flags together like this.  Running this command on this hypothetical file location will generate the output: \"Continuing in background, pid 12345.\"  The pid is unique process ID assigned to this particular data download job for your reference, in case you need to cancel the process.    ********************\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "# wget -bqc https://websitename.com/datafilename.txt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In this lesson, we learned another way to download filesin the command line using the tool Wget.  Up next, we will put our new knowledge to practice and learn more advanced Wget use cases.  Happy wget.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540cdc8b-05ca-4a28-92b4-f0142c5c23ae",
   "metadata": {},
   "source": [
    "## Installing Wget\n",
    "\n",
    "# Unlike curl, there are several ways to download and install wget depending on which operating system your machine is running. Which of the following is NOT a way to install wget?\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "\n",
    "    On some Linux systems, Wget is already pre-installed\n",
    "    press\n",
    "    1\n",
    "    On Linux, install using apt-get\n",
    "    press\n",
    "    2\n",
    "    On Windows, install via gnuwin32\n",
    "    press\n",
    "    3\n",
    "#    On MacOS, install using pip       its not a Python package, its a command line program, Mac use brew XXX\n",
    "    press\n",
    "    4\n",
    "    On MacOS, install using homebrew\n",
    "    press\n",
    "    5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b314bf92-b894-4024-bd7c-50448cce80d0",
   "metadata": {},
   "source": [
    "## Downloading single file using wget\n",
    "\n",
    "Let's get some hands on practice for the option flags that make wget such a popular file downloading tool.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "#    Fill in the option flag for resuming a partial download.\n",
    "#    Fill in the option flag for letting the download occur in the background.\n",
    "    Preview the download log file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f2baf7-95e0-40e2-b9ea-dcb87dba0a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the two option flags \n",
    "wget -c -b https://assets.datacamp.com/production/repositories/4180/datasets/eb1d6a36fa3039e4e00064797e1a1600d267b135/201812SpotifyData.zip\n",
    "\n",
    "# Verify that the Spotify file has been downloaded\n",
    "ls \n",
    "\n",
    "# Preview the log file \n",
    "cat ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da662ca2-db99-4be5-9894-b3fe057f4c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "jhu@debian:~/Downloads$ wget -c -b https://assets.datacamp.com/production/repositories/4180/datasets/eb1d6a36fa3039e4e00064797e1a1600d267b135/201812SpotifyData.zip\n",
    "Continuing in background, pid 29917.\n",
    "Output will be written to ‘wget-log’.\n",
    "jhu@debian:~/Downloads$ ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f836824e-519b-49cb-834f-e9866c37a038",
   "metadata": {},
   "source": [
    "## Advanced downloading using Wget\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b4feb8-c8f5-4ba5-b380-6046571deb88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581b5723-7389-422d-ad15-ae3389a44780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f798e21-d704-44bb-84e8-9021bb7655ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92062c51-ebf6-4e02-a961-460aec2d06d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480a6b62-80ce-4109-b8fd-17044aafa1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879b4a4d-03f2-4dea-a026-c48ba578a340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f9dfa6-5afb-4d5c-9215-9b4071442889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5123b40d-6753-47f0-be8e-35b6798a47ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a29048-eb92-43ab-9215-a392509e0bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523ba74d-d926-485f-9a21-8c3561073473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb745ca0-0884-492c-9a57-9e512672c93a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818a4443-610a-4a42-8f33-82d476794c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fff00b-e44d-4fc7-baf1-bfb3b7564007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d73974a-fa81-4427-8cfd-16206b88cdbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
