{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5979dc4a-cbce-4841-b0d6-233720f7a93c",
   "metadata": {},
   "source": [
    "## Data Processing in Shell\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72466dcd-ec1d-4659-8e93-2bd45937d7d2",
   "metadata": {},
   "source": [
    "## Course Description\n",
    "\n",
    "We live in a busy world with tight deadlines. As a result, we fall back on what is familiar and easy, favoring GUI interfaces like Anaconda and RStudio. However, taking the time to learn data analysis on the command line is a great long-term investment because it makes us stronger and more productive data people.\n",
    "\n",
    "In this course, we will take a practical approach to learn simple, powerful, and data-specific command-line skills. Using publicly available Spotify datasets, we will learn how to download, process, clean, and transform data, all via the command line. We will also learn advanced techniques such as command-line based SQL database operations. Finally, we will combine the powers of command line and Python to build a data pipeline for automating a predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3f0354-567d-45f2-91f4-cd0979631c9d",
   "metadata": {},
   "source": [
    "##  Downloading Data on the Command Line\n",
    "Free\n",
    "0%\n",
    "\n",
    "In this chapter, we learn how to download data files from web servers via the command line. In the process, we also learn about documentation manuals, option flags, and multi-file processing.\n",
    "\n",
    "    Downloading data using curl    50 xp\n",
    "    Using curl documentation    50 xp\n",
    "    Downloading single file using curl    100 xp\n",
    "    Downloading multiple files using curl    100 xp\n",
    "    Downloading data using Wget    50 xp\n",
    "    Installing Wget    50 xp\n",
    "    Downloading single file using wget    100 xp\n",
    "    Advanced downloading using Wget    50 xp\n",
    "    Setting constraints for multiple file downloads    50 xp\n",
    "    Creating wait time using Wget    100 xp\n",
    "    Data downloading with Wget and curl    100 xp \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a52087-c821-4826-9234-3a2425168d12",
   "metadata": {},
   "source": [
    "##  Data Cleaning and Munging on the Command Line\n",
    "0%\n",
    "\n",
    "We continue our data journey from data downloading to data processing. In this chapter, we utilize the command line library csvkit to convert, preview, filter and manipulate files to prepare our data for further analyses.\n",
    "\n",
    "    Getting started with csvkit    50 xp\n",
    "    Installation and documentation for csvkit    100 xp\n",
    "    Converting and previewing data with csvkit    100 xp\n",
    "    File conversion and summary statistics with csvkit    100 xp\n",
    "    Filtering data using csvkit    50 xp\n",
    "    Printing column headers with csvkit    100 xp\n",
    "    Filtering data by column with csvkit    100 xp\n",
    "    Filtering data by row with csvkit    100 xp\n",
    "    Stacking data and chaining commands with csvkit    50 xp\n",
    "    Stacking files with csvkit    100 xp\n",
    "    Chaining commands using operators    100 xp\n",
    "    Data processing with csvkit    100 xp \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da0d1d8-c0bb-4d63-80b6-cce48c1a38ab",
   "metadata": {},
   "source": [
    "##  Database Operations on the Command Line\n",
    "0%\n",
    "\n",
    "In this chapter, we dig deeper into all that csvkit library has to offer. In particular, we focus on database operations we can do on the command line, including table creation, data pull, and various ETL transformation.\n",
    "\n",
    "    Pulling data from database    50 xp\n",
    "    Using sql2csv documentation    50 xp\n",
    "    Understand sql2csv connectors    50 xp\n",
    "    Practice pulling data from database    100 xp\n",
    "    Manipulating data using SQL syntax    50 xp\n",
    "    Applying SQL to a local CSV file    100 xp\n",
    "    Cleaner scripting via shell variables    100 xp\n",
    "    Joining local CSV files using SQL    100 xp\n",
    "    Pushing data back to database    50 xp\n",
    "    Practice pushing data back to database    100 xp\n",
    "    Database and SQL with csvkit    100 xp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724bd0dd-666b-43f0-a5ee-0a99d9c9eec9",
   "metadata": {},
   "source": [
    "##  Data Pipeline on the Command Line\n",
    "0%\n",
    "\n",
    "In the last chapter, we bridge the connection between command line and other data science languages and learn how they can work together. Using Python as a case study, we learn to execute Python on the command line, to install dependencies using the package manager pip, and to build an entire model pipeline using the command line.\n",
    "\n",
    "    Python on the command line    50 xp\n",
    "    Finding Python version on the command line    50 xp\n",
    "    Executing Python script on the command line    100 xp\n",
    "    Python package installation with pip    50 xp\n",
    "    Understanding pip's capabilities    50 xp\n",
    "    Installing Python dependencies    100 xp\n",
    "    Running a Python model   100 xp\n",
    "    Data job automation with cron    50 xp\n",
    "    Understanding cron scheduling syntax    50 xp\n",
    "    Scheduling a job with crontab    100 xp\n",
    "    Model production on the command line    100 xp\n",
    "    Course recap    50 xp \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcf13b7-649f-4f3e-aeb2-63d58baf47d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1a28e0a-ad88-4a3f-b236-34527b025385",
   "metadata": {},
   "source": [
    "## Downloading data using curl\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Welcome to Intermediate Shell.  My name is Susan Sun, and I do data work.  I'm looking forward to learning with you in this course.  In data, many of us bypass the command line in favor of GUI interfaces like Anaconda and RStudio because that is what we are familiar with.  However, taking the time to learn data science on the command line is a great long term investment that will, ultimately, make us better and more productive data people.  \n",
    "\n",
    "\n",
    "In this course, we take a practical approach and learn command line tools useful for everyday data processing and analyses.  First, lets learn how to download data files using curl.  The \"curl\" is short for Client for URLs, is a UNIX command line tool for transferring data to and from a server.  It is often used to download data from HTTP sites and FTP servers.  To check if \"curl\" has properly installed, type the following in the command line: \"man curl\".  If \"curl\" has not been installed, you will see: \"curl command not found\".  To install curl, Google it.  If \"curl\" is installed, your console will look like normal man help pages.  You can keep pressing Enter to scroll through the curl manual.  To exit and return to your console, press q.  \n",
    "\n",
    "The basic syntax for curl has the following structure: \"curl [optional flags] [URL]\".  The URL is required  for the command to run successfully.  The \"curl\" supports a large number of protocal calls.  (including HTTP, HTTPS, FTP, SFTP etc).  For the full list using the \"curl --help\".  Lets download a single file stored at this hypothetical URL using curl.  To save the file with its original name \"datafilename.txt\", use the optional flag \"-O\" (dash uppercase O).  This reads \"curl -O URL\".  To save the file under a different name, replace -O (dash uppercase O) with -o (dash lowercase o) and new file name.  Now it reads \"curl -o newname URL\".  \n",
    "\n",
    "Often times, a server will host multiple data files, with similar filenames.  Like with different ending values.  Instead of curl each file individually, we can use wildcards (do you remember what we learned in introduction to shell course) to download all the files at once.  To download every file hostedon this server that starts with datafilename and end in \".txt\", we use: \"curl -o URLsomething*.txt\".  \n",
    "\n",
    "Another option is to increment using a globbing parser.  The following will download every files sequentially starting with data \"filename001.txt\" ane ending with data \"filename100.txt\".  Note that the end of the command that reads: open square bracket zero zero one dash one hundread close square bracket dot txt.  That is the globbing at work.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# curl -O https://websitename.com/datafilename[001-100].txt\n",
    "#                                             *********\n",
    "\n",
    "\n",
    "We can increment through the files and download every Nth file.  For example, to download every 10th file, we can modify the globbing parser to read: open square bracket zero zero one dash one hundred colon ten close square bracket dot txt.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# curl -O https://websitename.com/datafilename[001-100:10].txt\n",
    "#                                             ************\n",
    "\n",
    "\n",
    "# Sometimes internet can time out.  To make sure that our download progress is not lost, \n",
    "# *******************************************************************************************************************\n",
    "curl has these two flags: \n",
    "\"-L\" redirects the HTTP URL if a 300 error code occurs.  \n",
    "\"-C\" resumes a previous file transfer if it times out before completion.  \n",
    "Putting everything together.  Note that all option flags come before URL, but the order of the flags does not matter.  \n",
    "\n",
    "\n",
    "\n",
    "In this lesson, we learned how to download files using curl.  Lets put our new knowledge to practice.  Happy crul.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408bf61d-3275-468e-826b-1d8bd2569781",
   "metadata": {},
   "outputs": [],
   "source": [
    "jhu@debian:~$ curl -O https://assets.datacamp.com/production/repositories/4180/datasets/513986f5ea7ed9a8565bba20d088d21c10e099dc/Spotify_MusicAttributes.csv > ~/Downloads/Spotify_MusicAttributes.csv\n",
    "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
    "                                 Dload  Upload   Total   Spent    Left  Speed\n",
    "100  1717  100  1717    0     0   1382      0  0:00:01  0:00:01 --:--:--  1382\n",
    "jhu@debian:~$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b86aa54-6f85-4d13-a275-2414f7247f2f",
   "metadata": {},
   "source": [
    "## Using curl documentation\n",
    "\n",
    "As you work with command line tools you will often need to consult the documentation to remind yourself of the syntax or of some of the available functionality. In this exercise, you'll consult curl's documentation to answer this question:\n",
    "\n",
    "Based on the information in the curl manual, which of the following is NOT a supported file protocol:\n",
    "Instructions\n",
    "50 XP\n",
    "Possible Answers\n",
    "\n",
    "    LDAP\n",
    "    FTPS\n",
    "    HTTPS\n",
    "#    OFTP\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb0b824-a4d8-4867-a1af-f2c81cb76700",
   "metadata": {},
   "outputs": [],
   "source": [
    "jhu@debian:~$ curl --help\n",
    "Usage: curl [options...] <url>\n",
    " -d, --data <data>   HTTP POST data\n",
    " -f, --fail          Fail silently (no output at all) on HTTP errors\n",
    " -h, --help <category> Get help for commands\n",
    " -i, --include       Include protocol response headers in the output\n",
    " -o, --output <file> Write to file instead of stdout\n",
    " -O, --remote-name   Write output to a file named as the remote file\n",
    " -s, --silent        Silent mode\n",
    " -T, --upload-file <file> Transfer local FILE to destination\n",
    " -u, --user <user:password> Server user and password\n",
    " -A, --user-agent <name> Send User-Agent <name> to server\n",
    " -v, --verbose       Make the operation more talkative\n",
    " -V, --version       Show version number and quit\n",
    "\n",
    "This is not the full help, this menu is stripped into categories.\n",
    "Use \"--help category\" to get an overview of all categories.\n",
    "For all options use the manual or \"--help all\".\n",
    "jhu@debian:~$ man curl\n",
    "DESCRIPTION\n",
    "       curl  is  a tool to transfer data from or to a server, using one of the\n",
    "       supported protocols (DICT, FILE, FTP, FTPS, GOPHER, HTTP, HTTPS,  IMAP,\n",
    "       IMAPS,  LDAP,  LDAPS,  MQTT, POP3, POP3S, RTMP, RTMPS, RTSP, SCP, SFTP,\n",
    "       SMB, SMBS, SMTP, SMTPS, TELNET and TFTP). The command  is  designed  to\n",
    "       work without user interaction.\n",
    "\n",
    "       curl offers a busload of useful tricks like proxy support, user authen‐\n",
    "       tication, FTP upload, HTTP post, SSL connections, cookies, file  trans‐\n",
    "       fer  resume,  Metalink,  and more. As you will see below, the number of\n",
    "       features will make your head spin!\n",
    "\n",
    "       curl is powered by  libcurl  for  all  transfer-related  features.  See\n",
    "       libcurl(3) for details.\n",
    "\n",
    "PROTOCOLS\n",
    "       curl supports numerous protocols, or put in URL  terms:  schemes.  Your\n",
    "       particular build may not support them all.\n",
    "\n",
    "       DICT   Lets you lookup words using online dictionaries.\n",
    "\n",
    "       FILE   Read  or  write  local  files.  curl  does not support accessing\n",
    "              file:// URL remotely, but when running on Microsft Windows using\n",
    "              the native UNC approach will work.\n",
    "\n",
    "       FTP(S) curl  supports  the  File Transfer Protocol with a lot of tweaks\n",
    "              and levers. With or without using TLS.\n",
    "\n",
    "       GOPHER Retrieve files.\n",
    "\n",
    "       HTTP(S)\n",
    "              curl supports HTTP with numerous options and variations. It  can\n",
    "              speak HTTP version 0.9, 1.0, 1.1, 2 and 3 depending on build op‐\n",
    "              tions and the correct command line options.\n",
    "\n",
    "       IMAP(S)\n",
    "              Using the mail reading protocol, curl can \"download\" emails  for\n",
    "              you. With or without using TLS.\n",
    "\n",
    "       LDAP(S)\n",
    "              curl can do directory lookups for you, with or without TLS.\n",
    "\n",
    "       MQTT   curl supports MQTT version 3. Downloading over MQTT equals \"sub‐\n",
    "              scribe\" to a topic while uploading/posting equals \"publish\" on a\n",
    "              topic.  MQTT  support  is experimental and TLS based MQTT is not\n",
    "              supported (yet).\n",
    "\n",
    "       POP3(S)\n",
    "              Downloading from a pop3 server means getting  a  mail.  With  or\n",
    "              without using TLS.\n",
    "\n",
    "       RTMP(S)\n",
    "              The  Realtime  Messaging  Protocol  is  primarily used to server\n",
    "              streaming media and curl can download it.\n",
    "\n",
    "       RTSP   curl supports RTSP 1.0 downloads.\n",
    "\n",
    "       SCP    curl supports SSH version 2 scp transfers.\n",
    "\n",
    "       SFTP   curl supports SFTP (draft 5) done over SSH version 2.\n",
    "\n",
    "       SMB(S) curl supports SMB version 1 for upload and download.\n",
    "\n",
    "       SMTP(S)\n",
    "              Uploading contents to an SMTP server  means  sending  an  email.\n",
    "              With or without TLS.\n",
    "\n",
    "       TELNET Telling curl to fetch a telnet URL starts an interactive session\n",
    "              where it sends what it reads  on  stdin  and  outputs  what  the\n",
    "              server sends it.\n",
    "\n",
    "       TFTP   curl can do TFTP downloads and uploads.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4298c7-a2df-4dd7-b324-c0ae07485c15",
   "metadata": {},
   "source": [
    "## Downloading single file using curl\n",
    "\n",
    "Let's get some hands on practice for the more commonly used options and flags with curl. \n",
    "# The URL for the hosted file is a shortened URL using tinyurl. Because of that, we need to fill out a flag option that allows for redirected URLs.\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "\n",
    "    Question 1\n",
    "#    Fill in the option flag that allow downloading from a redirected URL.\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    In the same step as the download, add in the necessary syntax to rename the downloaded file as Spotify201812.zip.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7a1c61-32ac-4c50-8aa6-d005d9040656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use curl to download the file from the redirected URL\n",
    "curl -L -o Spotify201812.zip https://assets.datacamp.com/production/repositories/4180/datasets/eb1d6a36fa3039e4e00064797e1a1600d267b135/201812SpotifyData.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec27918-f8cf-466e-8b9b-c2c4c278b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jhu@debian:~$ cd ~/Downloads/\n",
    "jhu@debian:~/Downloads$ ls\n",
    "Spotify_MusicAttributes.csv\n",
    "Training_Machine_Learning_Surrogate_Models_From_a_.pdf\n",
    "jhu@debian:~/Downloads$ curl -L -o Spotify201812.zip https://assets.datacamp.com/production/repositories/4180/datasets/eb1d6a36fa3039e4e00064797e1a1600d267b135/201812SpotifyData.zip\n",
    "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
    "                                 Dload  Upload   Total   Spent    Left  Speed\n",
    "100 1944k  100 1944k    0     0   863k      0  0:00:02  0:00:02 --:--:--  863k\n",
    "jhu@debian:~/Downloads$ ls\n",
    "new_file  Spotify201812.zip  Spotify_MusicAttributes.csv  Training_Machine_Learning_Surrogate_Models_From_a_.pdf\n",
    "jhu@debian:~/Downloads$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997951d2-43c6-4232-a1fa-8d2e6cde6cb4",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Exercise\n",
    "Downloading multiple files using curl\n",
    "\n",
    "We have 100 data files stored in long sequentially named URLs. Scroll right to see the complete URLs.\n",
    "\n",
    "https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile001.txt\n",
    "https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile002.txt\n",
    "......\n",
    "https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile100.txt\n",
    "\n",
    "To minimize having to type the long URLs over and over again, we'd like to download all of these files using a single curl command.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Download all 100 data files using a single curl command.\n",
    "    Print all downloaded files to directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9afc010-a929-48fb-adc0-19868410e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all 100 data files\n",
    "curl -O https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile[001-100].txt\n",
    "#                                                                                                        #########\n",
    "\n",
    "# Print all downloaded files to directory\n",
    "ls datafile*.txt\n",
    "\n",
    "\n",
    "\n",
    "jhu@debian:~/Downloads$ mkdir NewFiles/\n",
    "jhu@debian:~/Downloads$ ls\n",
    "NewFiles           Training_Machine_Learning_Surrogate_Models_From_a_.pdf\n",
    "Spotify201812.zip\n",
    "jhu@debian:~/Downloads$ cd NewFiles/\n",
    "jhu@debian:~/Downloads/NewFiles$ curl -O https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile[001-030].txt\n",
    "\n",
    "[1/30]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile001.txt --> datafile001.txt\n",
    "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile001.txt\n",
    "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
    "                                 Dload  Upload   Total   Spent    Left  Speed\n",
    "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
    "\n",
    "[2/30]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile002.txt --> datafile002.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7266c9e5-a188-4d3c-85a2-43afcd5b3153",
   "metadata": {},
   "source": [
    "## Downloading data using Wget\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Welcome back, in this lesson, we will introduce another command line tool for downloading data, called Wget.  We will walk through how to install and set up Wget along with some basic usage.  Wget derives its name from World Wide Web and Get.  It is a GNU project native to the Linux system, but is compatible across all operating systems.  It is another command line tool that will help you download files via HTTP and FTP.  \n",
    "\n",
    "\n",
    "# Compared to \"curl\", Wget is more multi-purpose.  It can download a single file, an entire folder, or even a webpage.  \n",
    "Most importantly, it makes multiple file downloads possible recursively.  Aside from using man, another way to check is Wget has been installed correctly, is by using \"which wget\" (just like Bash and Dash?).  This will return the location of where Wget is installed.  For example, in the local user bin: If Wget has not been installed, there will simply be no output.  For official documentation and source code of Wget, Google it.  Unless you are comfortable compiling from the source code, here are some easier alternatives.  \n",
    "\n",
    "For Linux users, it is likely Wget is already installed for you.  If not, run \"sudo apt-get install wget\", just Google it.  For Mac users, use homebrew by running \"brew install wget\".  For Windows users, this will not be a command line install.  Rather, download as part of the gunwin32 package.  Once the installation is complete, use the man command to print the Wget manual.  \n",
    "\n",
    "The basic syntax for Wget has a similar structure to curl: \"wget [optional flags] [URL]\".  The URL is also required for the Wget command to run successfully (isn't that obviously? we are doing URL request).  Wget supports a large number of protocal calls for data stored on servers.  For the full list of the options available, refer to \"wget --help\" or \"man wget\" or ask Google.  \n",
    "\n",
    "\n",
    "# Here are some option flags unique to Wget: \n",
    "\"-b\" allows your download to run in the background. \n",
    "\"-q\" turns off the wget output, which saves some disk spaces. \n",
    "\"-c\" is useful to finish up a previously broken download wheather by Wget or another program. \n",
    "\n",
    "Finally, you can link all the option flags together like this.  Running this command on this hypothetical file location will generate the output: \"Continuing in background, pid 12345.\"  The pid is unique process ID assigned to this particular data download job for your reference, in case you need to cancel the process.    ********************\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "# wget -bqc https://websitename.com/datafilename.txt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In this lesson, we learned another way to download filesin the command line using the tool Wget.  Up next, we will put our new knowledge to practice and learn more advanced Wget use cases.  Happy wget.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540cdc8b-05ca-4a28-92b4-f0142c5c23ae",
   "metadata": {},
   "source": [
    "## Installing Wget\n",
    "\n",
    "# Unlike curl, there are several ways to download and install wget depending on which operating system your machine is running. Which of the following is NOT a way to install wget?\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "\n",
    "    On some Linux systems, Wget is already pre-installed\n",
    "    press\n",
    "    1\n",
    "    On Linux, install using apt-get\n",
    "    press\n",
    "    2\n",
    "    On Windows, install via gnuwin32\n",
    "    press\n",
    "    3\n",
    "#    On MacOS, install using pip       its not a Python package, its a command line program, Mac use brew XXX\n",
    "    press\n",
    "    4\n",
    "    On MacOS, install using homebrew\n",
    "    press\n",
    "    5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b314bf92-b894-4024-bd7c-50448cce80d0",
   "metadata": {},
   "source": [
    "## Downloading single file using wget\n",
    "\n",
    "Let's get some hands on practice for the option flags that make wget such a popular file downloading tool.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "#    Fill in the option flag for resuming a partial download.\n",
    "#    Fill in the option flag for letting the download occur in the background.\n",
    "    Preview the download log file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f2baf7-95e0-40e2-b9ea-dcb87dba0a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the two option flags \n",
    "wget -c -b https://assets.datacamp.com/production/repositories/4180/datasets/eb1d6a36fa3039e4e00064797e1a1600d267b135/201812SpotifyData.zip\n",
    "\n",
    "# Verify that the Spotify file has been downloaded\n",
    "ls \n",
    "\n",
    "# Preview the log file \n",
    "cat ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da662ca2-db99-4be5-9894-b3fe057f4c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "jhu@debian:~/Downloads$ wget -c -b https://assets.datacamp.com/production/repositories/4180/datasets/eb1d6a36fa3039e4e00064797e1a1600d267b135/201812SpotifyData.zip\n",
    "Continuing in background, pid 29917.\n",
    "Output will be written to ‘wget-log’.\n",
    "jhu@debian:~/Downloads$ ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f836824e-519b-49cb-834f-e9866c37a038",
   "metadata": {},
   "source": [
    "## Advanced downloading using Wget\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**So far, we've learned how to install and do basic file downloads using either \"curl\" or \"Wget\".  In this lesson, we'll focus on getting the most out of Wget by going over more advanced techniques for data downloading.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "A common way for data people to handle multiple file downloads is by storing the file locations in a file and pass that meta file to the downloading program like Wget.  In this case, all the URLs for the files we want to download are stored in the file \"URL_list.txt\".  Lets use the cat command to print and preview the URLs first.  After confirming that the URLs are indeed stored in this file, we can now pass this file to Wget.  Note that we need to preface this with \"-i\" option flag, so Wget knows that we are reading URLs from a local or external file.  The command reads \"wget -i URL_list.txt\".  Finally, its worth noting not to insert any option flags in between the \"-i\" and the RUL file.  If other option flags are needed, put it before \"-i\".  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Sometimes, its useful to make sure Wget doesn't consume your entire bandwidth with the file download.  You can set an upper download bandwidth limit using the \"--limit-rate\" option flag.  Set the limit rate equal to a whole number, which will automatically convert to kilobytes per second.  For example \"wget --limit-rate=200k -i RUL_list.txt\" will make sure your download rate will not exceed 200 kilobytes per second as you download the files saved in teh URL list.  For downloading smaller files, enforcing a download bandwidth won't work as well.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "To avoid overtaxing the file hosting server, it is more useful to enforce a mandatory wait time between file downloads using the \"--wait\" option flag.  The default time interval is set to seconds.  For example, in below command \"wget --wait=2.5 --limit-rate=200k -i URL_list.txt\", creates a 2.5 seconds pause between downloading each file stored in the URL list file.  \n",
    "\n",
    "\n",
    "As we round out this chapter, it is helpful to do a quick comparison between the 2 command line program tools \"curl\" and \"wget\".  Although both curl and wget can download files from HTTP, HTTPS, FTP.  The Curl alone can download and uploadrom 20 other protocols.  It is also easier to install across all operating systems, compared to wget.  Wget's advantage is its ability to handle multiple file downloads gracefully.  It can also be used to download just about anything, from a full file directory to a HTML page.  \n",
    "\n",
    "\n",
    "\n",
    "With both curl and wget at your disposal, you're now an expert at codnloading files on the command line.  Lets practice.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2d74e0-d99c-4192-949f-d3b5870ec1f9",
   "metadata": {},
   "source": [
    "## Setting constraints for multiple file downloads\n",
    "\n",
    "Which of the following is NOT the correct way to set download constraints for multiple file downloads using wget?\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "\n",
    "#    Store all URL locations in a text file (e.g. url_list.txt) and iteratively download using wget and option flag i\n",
    "    press\n",
    "    1\n",
    "    Use wget with the --limit-rate option, followed by the download speed in KB/s.\n",
    "    press\n",
    "    2\n",
    "    Use wget with the --wait option, followed by the wait time in seconds.\n",
    "    press\n",
    "    3\n",
    "    \n",
    "Hint\n",
    "\n",
    "    wget -i url_list.txt iterates through the files but does not set any constraints for downloads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1cb67a-b541-48c1-9af2-99e81821495c",
   "metadata": {},
   "source": [
    "## Creating wait time using Wget\n",
    "\n",
    "For download smaller files, enforcing a mandatory wait time between file downloads makes sure we don't overload the server with too many requests. Here, we will using the built in option flag with wget to create a mandatory wait time (in seconds) between downloading each file stored in the URL list file.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Create a mandatory 1 second pause between downloading all files in url_list.txt.\n",
    "\n",
    "Hint\n",
    "\n",
    "    When in doubt, use man wget to find the correct syntax.\n",
    "    The --wait option flag needs to be followed by the time (default is in seconds) with format --wait={insert time in seconds}.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b402185-0148-4bd2-90df-41c827add25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View url_list.txt to verify content\n",
    "cat url_list.txt\n",
    "\n",
    "# Create a mandatory 1 second pause between downloading all files in url_list.txt\n",
    "wget --wait=1 -i url_list.txt\n",
    "\n",
    "# Take a look at all files downloaded\n",
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e3d838-0541-46e4-b702-5c9237c73a8a",
   "metadata": {},
   "source": [
    "## Data downloading with Wget and curl\n",
    "\n",
    "To kick off a data analysis project, it's good practice to first consolidate all of our data into one place. Often times, this means downloading and pulling data from various locations such as HTTP servers and databases.\n",
    "\n",
    "While curl is handy for downloading a single file, it's somewhat unwieldy for handling multiple file downloads. In this capstone exercise, we will use both curl and Wget to download a series of monthly Spotify files, do some minor processing, and consolidate all downloaded files in our local directory.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "#    Download the zipped 201812SpotifyData data saved in the shortened (redirected) URL using curl. In the same step, rename file as Spotify201812.zip.\n",
    "    Unzip Spotify201812.zip, delete the original zipped file, and rename the unzipped file to Spotify201812.csv to stay consistent.\n",
    "    Use url_list.txt and Wget to download all 3 files: Spotify201809.csv, Spotify201810.csv, and Spotify201811.csv in one step, with an upper cap download speed of 2500KB/s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6a0bf3-ed97-43d4-83c4-acc670285390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use curl, download and rename a single file from URL\n",
    "curl -o Spotify201812.zip -L https://assets.datacamp.com/production/repositories/4180/datasets/eb1d6a36fa3039e4e00064797e1a1600d267b135/201812SpotifyData.zip\n",
    "\n",
    "# Unzip, delete, then re-name to Spotify201812.csv\n",
    "unzip Spotify201812.zip && rm Spotify201812.zip\n",
    "mv 201812SpotifyData.csv Spotify201812.csv\n",
    "\n",
    "\n",
    "# View url_list.txt to verify content\n",
    "cat url_list.txt\n",
    "\n",
    "# Use Wget, limit the download rate to 2500 KB/s, download all files in url_list.txt\n",
    "wget --limit-rate=2500k -i url_list.txt\n",
    "\n",
    "# Take a look at all files downloaded\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf8a228-b71d-4cbd-8f9b-e32b3e538475",
   "metadata": {},
   "outputs": [],
   "source": [
    "jhu@debian:~/Downloads$ curl -o Spotify201812.zip -L https://assets.datacamp.com/production/repositories/4180/datasets/eb1d6a36fa3039e4e00064797e1a1600d267b135/201812SpotifyData.zip\n",
    "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
    "                                 Dload  Upload   Total   Spent    Left  Speed\n",
    "100 1944k  100 1944k    0     0   144k      0  0:00:13  0:00:13 --:--:--  152k\n",
    "jhu@debian:~/Downloads$ ls\n",
    "NewFiles           Training_Machine_Learning_Surrogate_Models_From_a_.pdf\n",
    "Spotify201812.zip\n",
    "jhu@debian:~/Downloads$ unzip Spotify201812.zip && rm Spotify201812.zip\n",
    "Archive:  Spotify201812.zip\n",
    "  inflating: 201812SpotifyData.csv   \n",
    "   creating: __MACOSX/\n",
    "  inflating: __MACOSX/._201812SpotifyData.csv  \n",
    "jhu@debian:~/Downloads$ rm -r __MACOSX/\n",
    "jhu@debian:~/Downloads$ ls\n",
    "201812SpotifyData.csv  Training_Machine_Learning_Surrogate_Models_From_a_.pdf\n",
    "NewFiles\n",
    "jhu@debian:~/Downloads$ mv 201812SpotifyData.csv SpotifyData_201812.csv \n",
    "jhu@debian:~/Downloads$ ls\n",
    "NewFiles                Training_Machine_Learning_Surrogate_Models_From_a_.pdf\n",
    "SpotifyData_201812.csv\n",
    "jhu@debian:~/Downloads$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf8764d-1446-4ee8-887d-072582f48035",
   "metadata": {},
   "source": [
    "## Getting started with csvkit\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Welcome back.  In this lesson, we will explore the basics of \"csvkit\" for data processing on the command line.  Data processing on the command line is computationally efficient and also quite simple once you are familiar with the syntax.  Yet generations of data professionals gravitate toward Python since it comes pre-built with libraries specific for data handling that bash commands lack.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# The \"csvkit\" helps to bridge this gap by bringing a suite of data commands to the command line.  \n",
    "Developed by Wireservice using Python, \"csvkit\" offers a variety of data conversion, processng and cleaning capabilities that rivals Python, R and even SQL.  Because the \"csvkit\" is written in Python, it can be installed with the Python package manager pip.  The syntax is \"pip install csvkit\", or \"pip install --upgrade csvkit\" for upgrade.  Google it for more information.  Unlike most command-line tools, csvkit, as a whole, does not respond to the man command.  Documentation is web-based. For each command in the csv-suite, however, this is different.  \n",
    "\n",
    "# \"in2csv\" command to convert Excel and others to CSV file\n",
    "For example \"in2csv\" is a useful command in csvkit suite, that converts tabular data files, like text or Excel, into CSV.  These is both a web-based documentation as well as command-line manual, use \"in2csv --help\" or \"in2csv -h\" to prints the help information.  The syntax involves calling in2csv, followed by the name of the file you wish to convert, in this case, SpotifyData.xlsx.  The redirect operator is followed by the name of the newly created CSV file SpotifyData.csv.  Please note that [in2csv SpotifyData.xlsx] alone just prints console the data on the first Excel sheetand does not generate a new file.  The redirect operator is crucial for redirecting and saving the output in the new file SpotifyData.csv.  \n",
    "\n",
    "#   in2csv SpotifyData.xlsx > SpotifyData.csv\n",
    "\n",
    "# What if the data we want is not in the first sheet? \n",
    "The \"csvkit\" does let us specify which sheet to convert in an Excel file.  First, use the \"--names\" or \"-n\" option flag to print all sheet names in SpotifyData.xlsx.  Than we use the \"--sheet\" to specify that we want to convert \"Worksheet1_popularity\".  Note the quotation marks around the sheet name.  We re-direct the output of the conversion to Spotify_Popularity.csv.  Please remember that \"in2csv\" does not print any logs to console.  For sanity check, we run ls to confirm that the new CSV has been created.  \n",
    "\n",
    "#   in2csv -n\n",
    "#   in2csv --sheet \"Worksheet1_Popularity\" > Spotify_Popularity.csv\n",
    "\n",
    "There are various ways to preview data on the command line, such as cat, less, more.  The \"csvlook\" also in the scvkit suite, prints CSV files to the command line in a Mark-compatible, fixed-width format thats easier on the eye.  For documentation, use csvlook -h.  Lets test this out on our newly created CSV file.  The following command line [csvlook Spotify_Popularity.csv] prints the Pandas DataFrame style in the console.  \n",
    "\n",
    "# Our last command for this lesson is \"csvstat\".  \n",
    "The \"csvstat\" is similar to the \".describe()\" method in Python's Pandas library.  It intelligently desciphrs the data type in each column of the CSV file and prints descriptive summary statistics for each column according to its data type, such as mean, median, and unique value counts.  [And I need to be the guy who can study through reading the documentation, not video tutorials or towardsdatascience articles.  Or at least can use Google to help me in understanding].  Using the popularity data again, this is a portion of the summary statistics \"csvstat\" prints out the first column, track_id, \n",
    "\n",
    "\n",
    "\n",
    "In this lesson, we learned 3 command line tools in the csvkit-suite, in2csv, csvlook, and csvstat.  Now put our new knowledge into practice.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c0db6c-f9e8-4bcf-9f28-ce3683e432e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "jhu@debian:~/.virtual_environments$ ls | grep \".xlsx\"\n",
    "battledeath.xlsx\n",
    "Data_Dictionary_WiFi_Hotspots.xlsx\n",
    "jhu@debian:~/.virtual_environments$ cp *.xlsx ~/Downloads/\n",
    "jhu@debian:~/.virtual_environments$ cd ~/Downloads/\n",
    "jhu@debian:~/Downloads$ ls\n",
    "battledeath.xlsx\n",
    "Data_Dictionary_WiFi_Hotspots.xlsx\n",
    "NewFiles\n",
    "SpotifyData_201812.csv\n",
    "Training_Machine_Learning_Surrogate_Models_From_a_.pdf\n",
    "jhu@debian:~/Downloads$ csvlook Data_Dictionary_WiFi_Hotspots.xlsx \n",
    "bash: csvlook: command not found\n",
    "jhu@debian:~/Downloads$ cd ~/.virtual_environments/\n",
    "jhu@debian:~/.virtual_environments$ csvlook\n",
    "bash: csvlook: command not found\n",
    "jhu@debian:~/.virtual_environments$ source py39/bin/activate\n",
    "(py39) jhu@debian:~/.virtual_environments$ csvlook -h\n",
    "bash: csvlook: command not found\n",
    "(py39) jhu@debian:~/.virtual_environments$ csvsuite\n",
    "bash: csvsuite: command not found\n",
    "(py39) jhu@debian:~/.virtual_environments$ csvkit\n",
    "bash: csvkit: command not found\n",
    "(py39) jhu@debian:~/.virtual_environments$ pip install --upgrade csvkit\n",
    "Collecting csvkit\n",
    "  Downloading csvkit-1.0.6-py2.py3-none-any.whl (42 kB)\n",
    "     |████████████████████████████████| 42 kB 21 kB/s            \n",
    "Collecting agate-dbf>=0.2.0\n",
    "\n",
    "\n",
    "(py39) jhu@debian:~/.virtual_environments$ in2csv -n battledeath.xlsx \n",
    "2002\n",
    "2004\n",
    "(py39) jhu@debian:~/.virtual_environments$ in2csv --sheet \"2002\" battledeath.xlsx > ~/Downloads/2002battledeath.csv \n",
    "/home/jhu/.virtual_environments/py39/lib/python3.9/site-packages/agate/utils.py:285: UnnamedColumnWarning: Column 2 has no name. Using \"c\".\n",
    "/home/jhu/.virtual_environments/py39/lib/python3.9/site-packages/agate/utils.py:285: UnnamedColumnWarning: Column 3 has no name. Using \"d\".\n",
    "/home/jhu/.virtual_environments/py39/lib/python3.9/site-packages/agate/utils.py:285: UnnamedColumnWarning: Column 4 has no name. Using \"e\".\n",
    "(py39) jhu@debian:~/.virtual_environments$ csvlook ~/Downloads/2002battledeath.csv \n",
    "| War, age-adjusted mortality due to |     2002 | c | d | e |\n",
    "| ---------------------------------- | -------- | - | - | - |\n",
    "| Afghanistan                        |  36.084… |   |   |   |\n",
    "| Albania                            |   0.129… |   |   |   |\n",
    "| Algeria                            |  18.314… |   |   |   |\n",
    "| Andorra                            |   0.000… |   |   |   |\n",
    "| Angola                             |  18.965… |   |   |   |\n",
    "\n",
    "\n",
    "\n",
    "(py39) jhu@debian:~/.virtual_environments$ csvstat ~/Downloads/2002battledeath.csv \n",
    "  1. \"War, age-adjusted mortality due to\"\n",
    "\n",
    "\tType of data:          Text\n",
    "\tContains null values:  False\n",
    "\tUnique values:         192\n",
    "\tLongest value:         32 characters\n",
    "\tMost common values:    Afghanistan (1x)\n",
    "\t                       Albania (1x)\n",
    "\t                       Algeria (1x)\n",
    "\t                       Andorra (1x)\n",
    "\t                       Angola (1x)\n",
    "\n",
    "  2. \"2002\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2de980c-8468-4c28-ad04-4ddcdd13a476",
   "metadata": {},
   "source": [
    "## Installation and documentation for csvkit\n",
    "\n",
    "First step in learning about any libraries, tools, or suite of tools is to make sure we are using the latest and most stable version.\n",
    "\n",
    "Second step is to make sure we know how to access the documentation so we know where to go when we get stuck.\n",
    "\n",
    "Let's do both in this exercise for csvkit and the various commands in this suite of data processing command-line tools.\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "\n",
    "    Question 1\n",
    "    Upgrade csvkit to the latest version using Python package manager pip\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    Print the manual for in2csv on the command line.\n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "    Print the manual for csvlook on the command line.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f77dc9-76b0-454d-8ea5-892a116ff0ee",
   "metadata": {},
   "source": [
    "# Now imagine what code to type, and what outcome pops out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c048284-0758-4c59-acf4-94ad38b0f95c",
   "metadata": {},
   "source": [
    "## File conversion and summary statistics with csvkit\n",
    "\n",
    "It's common for Excel data files to have more than one worksheet (tab) of data. The Excel file SpotifyData.xlsx has two sheets named Worksheet1_Popularity and Worksheet2_MusicAttributes. Each sheet should be treated like its own data file, so we will use csvkit's commands here to convert each sheet to its own CSV file. Then, using the power of the commands we already know, print a high level summary for each column in the CSV files.\n",
    "Instructions 1/4\n",
    "25 XP\n",
    "\n",
    "    Question 1\n",
    "#    From SpotifyData.xlsx, convert the sheet \"Worksheet1_Popularity\" to CSV and call it Spotify_Popularity.csv.\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "#    Print the high level summary statistics for each column in Spotify_Popularity.csv.\n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "#    From SpotifyData.xlsx, convert the tab \"Worksheet2_MusicAttributes\" to CSV and call it Spotify_MusicAttributes.csv.\n",
    "    \n",
    "    \n",
    "    Question 4\n",
    "#    Print a preview of Spotify_MusicAttributes.csv using a function in csvkit with Markdown-compatible, fixed-width format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78548fc4-11c4-4d8d-975b-ef366429b829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to confirm name and location of the Excel data file\n",
    "ls\n",
    "\n",
    "# Convert sheet \"Worksheet1_Popularity\" to CSV\n",
    "___ SpotifyData.xlsx ___ \"Worksheet1_Popularity\" > Spotify_Popularity.csv\n",
    "\n",
    "\n",
    "\n",
    "# Think and think, recall what we have used previously? ###########################################################  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf972422-142e-4877-941f-a51fc94d9d03",
   "metadata": {},
   "source": [
    "## Filtering data using csvkit\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**In the previous lesson, we set ourselves up for success by preparing our data using \"csvkit\".  Now we will dig deeper and learn how to use csvkit's data filtering commands.  \n",
    "\n",
    "# Because our data files are tabular, we can filter by creating a subset of the original data by column or row.  \n",
    "We can use the \"csvcut\" command to filter data by column.  And we can use the \"csvgrep\" to filter by row.  \n",
    "\n",
    "The csvcut can filter file and truncate CSV files by either column name or column position.  For full documentation, use [csvcut --help] If you don't have column names or positions memorized, thats perfectly okay.  The \"csvcut\" has a \"--names\" or \"-n\" option flag, which prints a list of the column names and positions.  Using this command line  [csvcut -n Spotify_MusicAttributes.csv], we see that Spotify_MusicAttributes has 3 columns, track_id, dancebility, and duration_ms.  By referring to the output of the [csvcut -n], we can now filter the data more easily.  \n",
    "\n",
    "# Suppose we want to return only the first column track_id, \n",
    "we can do so by referring to that column by position: using this command \"csvcut -c 1 Spotify_MusicAttributes.csv\",  You can generate the same output by referring to the column name by replacing the position \"1\" with the column name, in double parentheses.  By using the below command line [csvcut -c \"track_id\" Spotify_MusicAttributes.csv].  The output as the same as before.  You can specify more than one column for filtering.  To return the second and third columns, separate the column numbers by commas.  Like this command: [csvcut -c 2,3 Spotify_MusicAttributes.csv].  Note that there is no space between the numbers two and three.  Inserting a space will generate an error.  Same as before, we can generate the same output by replacing the column numbers with names.  [csvcut -c \"track_id\",\"duration_ms\" Spotify_MusicAttributes.csv].  Note again, that column names are wrapped in double quotations, and the columns are separated by comma, with no space.  \n",
    "\n",
    "# The \"csvgrep\" is our go-to in csvkit for filtering data by row value.  \n",
    "Despite its name, csvgrep can filter by both extract match or regex fuzzy match.  It is important to remember that csvgrep must be paired with one of these three option flags, \"-m\" [followed by the exact row value to filter], \"-r\" [followed with a regex pattern], or \"-f\" [followed by the path to a file].  We will focus on \"-m\" in this lesson.  As always, please use the \"-h\" for more ducumentations.  Lets say we want to filter by a certain track_id.  We can do so by following command [csvgrep -c \"track_id\" -m 5RCPsfzEpTXMCTNk7wEfQ Spotify_MusicAttributes.csv]  This will return the entire rows that contains this track_id.  Similar to column filtering with csvcut, we can pass the column location instead.  Keeping everything else the same, we replace track_id with the column position number 1 instead.  This will give us the same output.  \n",
    "\n",
    "\n",
    "\n",
    "We just keep adding more csvkit commands to our data processing toolket.  Lets put these skills to good use with some exercises.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535192cf-af7e-4490-be43-8688db25fe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "(py39) jhu@debian:~/.virtual_environments$ csvcut --names ~/Downloads/2002battledeath.csv \n",
    "  1: War, age-adjusted mortality due to\n",
    "  2: 2002\n",
    "  3: c\n",
    "  4: d\n",
    "  5: e\n",
    "(py39) jhu@debian:~/.virtual_environments$ csvcut -n ~/Downloads/2002battledeath.csv \n",
    "  1: War, age-adjusted mortality due to\n",
    "  2: 2002\n",
    "  3: c\n",
    "  4: d\n",
    "  5: e\n",
    "(py39) jhu@debian:~/.virtual_environments$ \n",
    "\n",
    "\n",
    "  -c COLUMNS, --columns COLUMNS\n",
    "                        A comma-separated list of column indices, names or\n",
    "                        ranges to be extracted, e.g. \"1,id,3-5\". Defaults to\n",
    "                        all columns.\n",
    "  -C NOT_COLUMNS, --not-columns NOT_COLUMNS\n",
    "                        A comma-separated list of column indices, names or\n",
    "                        ranges to be excluded, e.g. \"1,id,3-5\". Defaults to no\n",
    "                        columns.\n",
    "  -x, --delete-empty-rows\n",
    "                        After cutting, delete rows which are completely empty.\n",
    "(py39) jhu@debian:~/.virtual_environments$ \n",
    "\n",
    "\n",
    "                                                     #######  If you have intager column name  ######################\n",
    "(py39) jhu@debian:~/.virtual_environments$ csvcut -c \"$2002\" ~/Downloads/2002battledeath.csv \n",
    "2002\n",
    "36.08399\n",
    "0.1289084\n",
    "18.31412\n",
    "0\n",
    "18.96456\n",
    "0\n",
    "0\n",
    "0.1702969\n",
    "0\n",
    "\n",
    "\n",
    "(py39) jhu@debian:~/.virtual_environments$ csvgrep -c 1 -m \"United States\" ~/Downloads/2002battledeath.csv \n",
    "\"War, age-adjusted mortality due to\",2002,c,d,e\n",
    "United States,0.014938,,,\n",
    "(py39) jhu@debian:~/.virtual_environments$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdbb5d2-110e-45c5-ad2c-903bbd7cb318",
   "metadata": {},
   "source": [
    "# Remember what we've learned in Introduction To Shell course?\n",
    "\n",
    "# To get the variable's value, you must put a dollar sign $ in front of it. Typing\n",
    "\n",
    "echo $USER\n",
    "\n",
    "prints\n",
    "\n",
    "repl\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "This is true everywhere: to get the value of a variable called X, you must write $X. (This is so that the shell can tell whether you mean \"a file named X\" or \"the value of a variable named X\".)\n",
    "Instructions\n",
    "100 XP\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1fcb06-ee63-4cc4-92e8-c1ce6f03e944",
   "metadata": {},
   "source": [
    "## Printing column headers with csvkit\n",
    "\n",
    "There are many ways to preview the data within csvkit alone(e.g. csvlook, csvstat, etc). However, if all we want is to find the position and name of the columns in our data, it is easier to simply print a string of column headers. Let's print the column headers for the data file Spotify_MusicAttributes.csv.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Print in console a list of column headers in the data file Spotify_MusicAttributes.csv using a csvkit command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0195f5-bb79-4726-af29-e296bd169b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to confirm name and location of data file\n",
    "ls\n",
    "\n",
    "\n",
    "# Print a list of column headers in data file    ####################################################################\n",
    "___ ___ Spotify_MusicAttributes.csv              # You must think, trying harder to recall what you have learned\n",
    "\n",
    "\n",
    "\n",
    "(py39) jhu@debian:~/.virtual_environments$ csvcut -n ~/Downloads/2002battledeath.csv \n",
    "  1: War, age-adjusted mortality due to\n",
    "  2: 2002\n",
    "  3: c\n",
    "  4: d\n",
    "  5: e\n",
    "(py39) jhu@debian:~/.virtual_environments$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d12729e-4167-4112-9fd9-8f7628dac803",
   "metadata": {},
   "source": [
    "## Filtering data by column with csvkit\n",
    "\n",
    "Let's get some hands-on practice for filtering data column using the csvkit command csvcut. Remember that we can filter columns by referring to the position of the column (e.g. 1st column, 2nd column) or by referring to the exact name of the column as it appears in the data file.\n",
    "Instructions 1/4\n",
    "25 XP\n",
    "\n",
    "    Question 1\n",
    "    Print the first column in Spotify_MusicAttributes.csv by referring to the column by its position in the file.\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    Print the first, third, and fifth column in Spotify_MusicAttributes.csv by referring to them by position.\n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "    Print the first column in Spotify_MusicAttributes.csv by referring to the column by its name.\n",
    "    \n",
    "    \n",
    "    Question 4\n",
    "    Print the first, third, and fifth column in Spotify_MusicAttributes.csv by referring to them by name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59da9453-0874-4776-ad54-478955a40156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a list of column headers in the data  #######################################################################\n",
    "csvcut -n Spotify_MusicAttributes.csv         ### Everytime you saw a line of code, trying to recall what is it\n",
    "                                              ### Push youself forard enough to change something, to be someone\n",
    "\n",
    "\n",
    "# Print the first column, by position\n",
    "csvcut -c 1 Spotify_MusicAttributes.csv\n",
    "\n",
    "\n",
    "csvcut -c 1,2,3 Spotify_MusicAttributes.csv\n",
    "\n",
    "\n",
    "csvcut -c \"track_id\",\"duration_ms\" Spotify_MusicAttributes.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb19df7-bd98-41b7-aaa2-8c38c6d05a4a",
   "metadata": {},
   "source": [
    "## Filtering data by row with csvkit\n",
    "\n",
    "Now it's time get some hands-on practice for filtering data by exact row values using -m. Whether it's text or numeric, csvgrep can help us filter by these values.\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "\n",
    "    Question 1\n",
    "#    Filter Spotify_MusicAttributes.csv and return the row or rows where track_id equals118GQ70Sp6pMqn6w1oKuki.\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "#    Filter Spotify_MusicAttributes.csv and return the row or rows where danceability equals 0.812.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abafc69-51bf-4530-ab80-525889298e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a list of column headers in the data \n",
    "csvcut -n Spotify_MusicAttributes.csv\n",
    "\n",
    "\n",
    "# Filter for row(s) where track_id = 118GQ70Sp6pMqn6w1oKuki\n",
    "csvgrip -c \"track_id\" -m 118GQ70Sp6pMqn6w1oKuki Spotify_MusicAttributes.csv\n",
    "\n",
    "\n",
    "\n",
    "csggrip -c \"danceability\" -m \"0.812\" Spotify_MusicAttributes.csv\n",
    "\n",
    "\n",
    "\n",
    "(py39) jhu@debian:~/.virtual_environments$ csvgrep -c \"$2002\" -m \"0\" ~/Downloads/2002battledeath.csv  ###############\n",
    "\"War, age-adjusted mortality due to\",2002,c,d,e\n",
    "Afghanistan,36.08399,,,\n",
    "Albania,0.1289084,,,\n",
    "Andorra,0,,,\n",
    "Antigua and Barbuda,0,,,\n",
    "Argentina,0,,,\n",
    "Armenia,0.1702969,,,\n",
    "Australia,0,,,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5055d45b-7a47-4ab7-a2f5-8d28ecab1fe3",
   "metadata": {},
   "source": [
    "## Stacking data and chaining commands with csvkit\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "**In this lesson, lets explore some more advanced techniques in csvkit, whether it is to chain multiple commands together, or to process more than one file at a time.  The \"csvstack\" command stacksthe rows form two or more CSV files together.  This is often used when you have files with the same schema but have been downloaded in chunks due to downloading restrictions such as API request restrictions.  \n",
    "\n",
    "First, lets go over some finer points.  Suppose we have two files, Spotify_Rank6.csv and Spotift_Rank7.csv and we want to stack them together to create one file.  The \"csvstack\" can do this, but first, we need to make sure that the input files have the same numberof columns, in the same columns order, with the same data types.  Lets preview Spotify_Rank6.csv and Spotify_Rank7.csv, the output show us the same: column track_id and popularity.  It looks like both files share the same schema.  We are ready for csvstack.  \n",
    "\n",
    "The syntax for stacking is: [csvstack Spotify_Rank6.csv Spotify_Rank7.csv > Spotify_AllRanks.csv].  Assume both files contain 2 rows each, this results in a 4 row file for Spotify_AllRank.csv.  However, its not always clear how to trace back which row in the final stacked file came from which source file.  To keep a record of the source of the data row, use csvstack's option flag \"-g\" followed by a user entered value to create a source column called group.  [csvstack -g \"Rank6\",\"Rank7\" Spotify_Rank6.csv Spotify_Rank7.csv > Spotify_AllRank.csv].  We doing this by inserting the above syntax after csvstack.  Please note that you can use a backslach(\\) to overcome line overflow.  Now when we preview the new Spotify_AllRank.csv, We see that there's a new column, by default named \"group\", that has the value Rank6 for every row that came from Spotify_Rank6.csv.  Finally, if we want to rename the column from group to something else, we can also do so by adding \"-n\" followed by the new column name.  Hwere is our example command: [csvstack -g \"Rank6\",\"Rank7\" -n \"Source\" Spotify_Rank6.csv Spotify_Rank7.csv > Spotify_AlLRank.csv].  \n",
    "\n",
    "\n",
    "The more we use command line tools, the more we start structuring complex commands.  [remember that one to search all the Excel files in pwd, ls | grep \".xlsx\"].  Just like English where we start combining independent sentences with conjunctions to from complex sentences, in command-line, we use operators to chain together commands in the same line.  \n",
    "# The semi-colon (;) operator links and runs multiple commands in a single line in the command line, \n",
    "sequentially.  Here, we will get both csvlook and csvstat output for the same file.  The command is as followed: [csvlook SpotifyData_All.csv; csvstst SpotifyData_All.csv].  \n",
    "# The double AND (&&) operator also links commands together, \n",
    "but the second command will only execute if the first command succeeds.  Here the csvstat will only run after csvlook succeeds.  \n",
    "# We are already familiar with the re-direct (>) operator.  \n",
    "This re-directs the output from the first command and saves it to the location in the second command.  [in2csv SpotifyData.xlsx > SpotifyData.csv].   \n",
    "# The pipe (|) operator uses the output of the first command as input to the second command.  \n",
    "Here, csvcut filters the data and prints columns track_id, and danceability, but notice that the output is not well formated.  By passing the outputof csvcut as input to csvlook using the pipe operator, the results are printed much more neatly.  [csvcut -c \"track_id\",\"danceability\" Spotify_Popularity.csv | csvlook].  The result are printed much more neatly.  \n",
    "\n",
    "\n",
    "\n",
    "We have learned a lot about data processing with various csvkit commands.  Lets practice putting everything together.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3ce50c-94ad-45dd-8fd3-2974418b1c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "(py39) jhu@debian:~/.virtual_environments$ csvlook --max-rows=10 ~/Downloads/2002battledeath.csv \n",
    "| War, age-adjusted mortality due to |    2002 | c | d | e |\n",
    "| ---------------------------------- | ------- | - | - | - |\n",
    "| Afghanistan                        | 36.084… |   |   |   |\n",
    "| Albania                            |  0.129… |   |   |   |\n",
    "| Algeria                            | 18.314… |   |   |   |\n",
    "| Andorra                            |  0.000… |   |   |   |\n",
    "| Angola                             | 18.965… |   |   |   |\n",
    "| Antigua and Barbuda                |  0.000… |   |   |   |\n",
    "| Argentina                          |  0.000… |   |   |   |\n",
    "| Armenia                            |  0.170… |   |   |   |\n",
    "| Australia                          |  0.000… |   |   |   |\n",
    "| Austria                            |  0.000… |   |   |   |\n",
    "| ...                                |     ... | ... | ... | ... |\n",
    "(py39) jhu@debian:~/.virtual_environments$ \n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################################################\n",
    "(py39) jhu@debian:~/.virtual_environments$ csvcut -c 1,2 ~/Downloads/2002battledeath.csv | csvlook --max-row=7\n",
    "| War, age-adjusted mortality due to |    2002 |\n",
    "| ---------------------------------- | ------- |\n",
    "| Afghanistan                        | 36.084… |\n",
    "| Albania                            |  0.129… |\n",
    "| Algeria                            | 18.314… |\n",
    "| Andorra                            |  0.000… |\n",
    "| Angola                             | 18.965… |\n",
    "| Antigua and Barbuda                |  0.000… |\n",
    "| Argentina                          |  0.000… |\n",
    "| ...                                |     ... |\n",
    "(py39) jhu@debian:~/.virtual_environments$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743aa977-8dbb-4a8b-8647-7c24885cbc93",
   "metadata": {},
   "source": [
    "## Stacking files with csvkit\n",
    "\n",
    "SpotifyData_PopularityRank6.csv and SpotifyData_PopularityRank7.csv have the same file format, column order, and overall data schema. However, one file contains information for songs ranked #6, and the other contains information for songs ranked #7. Combine the two files together into one unified file by stacking them.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Stack SpotifyData_PopularityRank6.csv and SpotifyData_PopularityRank7.csv together. Re-direct the output of this stacking and save as a new file called SpotifyPopularity.csv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59e649b-84b0-4b71-b345-b658331c022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the two files and save results as a new file\n",
    "csvstack SpotifyData_PopularityRank6.csv SpotifyData_PopularityRank7.csv ___ SpotifyPopularity.csv\n",
    "\n",
    "\n",
    "# Preview the newly created file \n",
    "csvlook SpotifyPopularity.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf820ea8-7f91-4d36-8ccf-484c54a4020b",
   "metadata": {},
   "source": [
    "## Chaining commands using operators\n",
    "\n",
    "The more we use command-line tools, the more we start stringing complex commands together. Sometimes it's for convenience, but other times, the output of one command can be used as input to another. Let's get some hands on practice with this by filling in the correct chain operators for the circumstances described in the instructions below.\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "\n",
    "    Question 1\n",
    "    Use the chain operator that allows csvlook to run first, and if it succeeds, then run csvstat.\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "#    Use the chain operator that to pass the output of csvsort as input to csvlook.     **pipe will do this job | \n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "#    Use the 2 chain operators that takes the top 15 results from the sorted output and saves it to a new file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eb9291-53a0-4d62-9aac-331c4cbaef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If csvlook succeeds, then run csvstat \n",
    "csvlook Spotify_Popularity.csv && csvstat Spotify_Popularity.csv\n",
    "\n",
    "\n",
    "\n",
    "# Remember we see this before, we have wget the zip file, then \n",
    "unzip filename.zip && rm filename.zip\n",
    "\n",
    "(reverse-i-search)`&&': unzip Spotify201812.zip && rm Spotify201812.zip\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "csvsort - c 2 Spotify_Popularity.csv | head - n 15 > Spotify_Popularity_Top15.csv\n",
    "#####################################################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac9b59c-3cab-40f9-b26f-65c345309ba7",
   "metadata": {},
   "source": [
    "## Data processing with csvkit\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Once we have assembled a dataset, we still need to process and clean the data prior to more advanced analysis such as predictive modeling. In this capstone exercise, let's make use of various commands in csvkit for some common data processing and cleaning.\n",
    "\n",
    "The Excel file Spotify_201809_201810.xlsx contains two sheets (tabs), named Spotify201809 and Spotify201810. First, we will split the Excel file down to its individual sheets, preview summary statistics, remove some columns, and then stack the two sheets back together again to form one single csv file, ready for further analysis.\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "\n",
    "    Question 1\n",
    "    Convert the Spotify201809 sheet into its own csv file named Spotify201809.csv.\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "#    Familiarize ourselves with the column names by printing a preview of the file using a function in csvkit.\n",
    "#    Find the column names for song track and popularity rank. Create a new CSV containing only these 2 columns.\n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "#    Stack Spotify201809_subset.csv and Spotify201810_subset.csv together to form 1 csv file and create a new column with either Sep2018 or Oct2018, depending on original file source. Leave the name of the new column to its default group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde30f79-560f-4a19-a5b3-bb46f52ebd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Spotify201809 sheet into its own csv file \n",
    "___ Spotify_201809_201810.xlsx ___ ___ ___ Spotify201809.csv\n",
    "\n",
    "# Check to confirm name and location of data file\n",
    "ls\n",
    "\n",
    "csvlook Spotify201809.csv\n",
    "\n",
    "# Create a new csv with 2 columns: track_id and popularity\n",
    "csvcut -c \"track_id\",\"popularity\" Spotify201809.csv > Spotify201809_subset.csv\n",
    "\n",
    "# While stacking the 2 files, create a data source column\n",
    "csvstack -g \"Sep2018\",\"Oct2018\" Spotify201809_subset.csv Spotify201810_subset.csv > Spotify_all_rankings.csv\n",
    "#####################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "(py39) jhu@debian:~/Downloads$ csvcut -n SpotifyData_201812.csv \n",
    "  1: artist_name\n",
    "  2: track_id\n",
    "  3: track_name\n",
    "  4: acousticness\n",
    "  5: danceability\n",
    "  6: duration_ms\n",
    "  7: energy\n",
    "  8: instrumentalness\n",
    "  9: key\n",
    " 10: liveness\n",
    " 11: loudness\n",
    " 12: mode\n",
    " 13: speechiness\n",
    " 14: tempo\n",
    " 15: time_signature\n",
    " 16: valence\n",
    " 17: popularity\n",
    "(py39) jhu@debian:~/Downloads$ \n",
    "\n",
    "\n",
    "\n",
    "csvcut - c \"track_id\",\"popularity\" Spotify201809.csv > Spotify201809_subset.csv\n",
    "\n",
    "\n",
    "\n",
    "csvstack -g \"09\",\"10\" -n \"Source\" Spotify201809.csv Spotify201810.csv > Spotify_All.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d99277-ceb6-4ffa-b87f-ec2a0783bf30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27732d1f-7a7b-442e-8b89-20573296ce36",
   "metadata": {},
   "source": [
    "## Pulling data from database\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# *******************************************************************************************************************\n",
    "**Welcome back, In this chapter, we'll learn how csvkit library brings SQL-like functionalities to the command line.  First, lets see how we can pull data from databases using the command \"sql2csv\".  \n",
    "\n",
    "# The \"sql2csv\" is a command in the csvkit library that allows us to access a variety of popular SQL databases and dialects, including Microsoft SQL Server, MySQL, Oracle, PostgreSQL and SQLite.  \n",
    "\n",
    "The beauty of this is that sql2csv gives us access via the command line, without having to go through database clients like PgAdmin or TablePlus.  Like the name auggests, sql2csv executes SQL commands, pulls data from databases, and saves the results as a local CSV file.  For full documentation, use the [sql2csv -h] to access.  The documentation is very short, and you have to be comfortable learning functionality through reading these command doc.  The syntax for sql2csv is fair\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0137d1b9-4cc8-4aa1-a742-5d4cd51dc9fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565ee26a-cdad-4a25-9bde-5ea48a72b553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05faef00-5a04-4ada-8356-7ca5d19a1de5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a382308-eb2f-48ce-af6b-01e2d8ad2745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581b5723-7389-422d-ad15-ae3389a44780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f798e21-d704-44bb-84e8-9021bb7655ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92062c51-ebf6-4e02-a961-460aec2d06d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480a6b62-80ce-4109-b8fd-17044aafa1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879b4a4d-03f2-4dea-a026-c48ba578a340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f9dfa6-5afb-4d5c-9215-9b4071442889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5123b40d-6753-47f0-be8e-35b6798a47ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a29048-eb92-43ab-9215-a392509e0bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523ba74d-d926-485f-9a21-8c3561073473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb745ca0-0884-492c-9a57-9e512672c93a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818a4443-610a-4a42-8f33-82d476794c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fff00b-e44d-4fc7-baf1-bfb3b7564007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d73974a-fa81-4427-8cfd-16206b88cdbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
