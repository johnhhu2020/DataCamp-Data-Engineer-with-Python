{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5979dc4a-cbce-4841-b0d6-233720f7a93c",
   "metadata": {},
   "source": [
    "## Data Processing in Shell\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72466dcd-ec1d-4659-8e93-2bd45937d7d2",
   "metadata": {},
   "source": [
    "## Course Description\n",
    "\n",
    "We live in a busy world with tight deadlines. As a result, we fall back on what is familiar and easy, favoring GUI interfaces like Anaconda and RStudio. However, taking the time to learn data analysis on the command line is a great long-term investment because it makes us stronger and more productive data people.\n",
    "\n",
    "In this course, we will take a practical approach to learn simple, powerful, and data-specific command-line skills. Using publicly available Spotify datasets, we will learn how to download, process, clean, and transform data, all via the command line. We will also learn advanced techniques such as command-line based SQL database operations. Finally, we will combine the powers of command line and Python to build a data pipeline for automating a predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3f0354-567d-45f2-91f4-cd0979631c9d",
   "metadata": {},
   "source": [
    "##  Downloading Data on the Command Line\n",
    "Free\n",
    "0%\n",
    "\n",
    "In this chapter, we learn how to download data files from web servers via the command line. In the process, we also learn about documentation manuals, option flags, and multi-file processing.\n",
    "\n",
    "    Downloading data using curl    50 xp\n",
    "    Using curl documentation    50 xp\n",
    "    Downloading single file using curl    100 xp\n",
    "    Downloading multiple files using curl    100 xp\n",
    "    Downloading data using Wget    50 xp\n",
    "    Installing Wget    50 xp\n",
    "    Downloading single file using wget    100 xp\n",
    "    Advanced downloading using Wget    50 xp\n",
    "    Setting constraints for multiple file downloads    50 xp\n",
    "    Creating wait time using Wget    100 xp\n",
    "    Data downloading with Wget and curl    100 xp \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a52087-c821-4826-9234-3a2425168d12",
   "metadata": {},
   "source": [
    "##  Data Cleaning and Munging on the Command Line\n",
    "0%\n",
    "\n",
    "We continue our data journey from data downloading to data processing. In this chapter, we utilize the command line library csvkit to convert, preview, filter and manipulate files to prepare our data for further analyses.\n",
    "\n",
    "    Getting started with csvkit    50 xp\n",
    "    Installation and documentation for csvkit    100 xp\n",
    "    Converting and previewing data with csvkit    100 xp\n",
    "    File conversion and summary statistics with csvkit    100 xp\n",
    "    Filtering data using csvkit    50 xp\n",
    "    Printing column headers with csvkit    100 xp\n",
    "    Filtering data by column with csvkit    100 xp\n",
    "    Filtering data by row with csvkit    100 xp\n",
    "    Stacking data and chaining commands with csvkit    50 xp\n",
    "    Stacking files with csvkit    100 xp\n",
    "    Chaining commands using operators    100 xp\n",
    "    Data processing with csvkit    100 xp \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da0d1d8-c0bb-4d63-80b6-cce48c1a38ab",
   "metadata": {},
   "source": [
    "##  Database Operations on the Command Line\n",
    "0%\n",
    "\n",
    "In this chapter, we dig deeper into all that csvkit library has to offer. In particular, we focus on database operations we can do on the command line, including table creation, data pull, and various ETL transformation.\n",
    "\n",
    "    Pulling data from database    50 xp\n",
    "    Using sql2csv documentation    50 xp\n",
    "    Understand sql2csv connectors    50 xp\n",
    "    Practice pulling data from database    100 xp\n",
    "    Manipulating data using SQL syntax    50 xp\n",
    "    Applying SQL to a local CSV file    100 xp\n",
    "    Cleaner scripting via shell variables    100 xp\n",
    "    Joining local CSV files using SQL    100 xp\n",
    "    Pushing data back to database    50 xp\n",
    "    Practice pushing data back to database    100 xp\n",
    "    Database and SQL with csvkit    100 xp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724bd0dd-666b-43f0-a5ee-0a99d9c9eec9",
   "metadata": {},
   "source": [
    "##  Data Pipeline on the Command Line\n",
    "0%\n",
    "\n",
    "In the last chapter, we bridge the connection between command line and other data science languages and learn how they can work together. Using Python as a case study, we learn to execute Python on the command line, to install dependencies using the package manager pip, and to build an entire model pipeline using the command line.\n",
    "\n",
    "    Python on the command line    50 xp\n",
    "    Finding Python version on the command line    50 xp\n",
    "    Executing Python script on the command line    100 xp\n",
    "    Python package installation with pip    50 xp\n",
    "    Understanding pip's capabilities    50 xp\n",
    "    Installing Python dependencies    100 xp\n",
    "    Running a Python model   100 xp\n",
    "    Data job automation with cron    50 xp\n",
    "    Understanding cron scheduling syntax    50 xp\n",
    "    Scheduling a job with crontab    100 xp\n",
    "    Model production on the command line    100 xp\n",
    "    Course recap    50 xp \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcf13b7-649f-4f3e-aeb2-63d58baf47d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1a28e0a-ad88-4a3f-b236-34527b025385",
   "metadata": {},
   "source": [
    "## Downloading data using curl\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Welcome to Intermediate Shell.  My name is Susan Sun, and I do data work.  I'm looking forward to learning with you in this course.  In data, many of us bypass the command line in favor of GUI interfaces like Anaconda and RStudio because that is what we are familiar with.  However, taking the time to learn data science on the command line is a great long term investment that will, ultimately, make us better and more productive data people.  \n",
    "\n",
    "\n",
    "In this course, we take a practical approach and learn command line tools useful for everyday data processing and analyses.  First, lets learn how to download data files using curl.  The \"curl\" is short for Client for URLs, is a UNIX command line tool for transferring data to and from a server.  It is often used to download data from HTTP sites and FTP servers.  To check if \"curl\" has properly installed, type the following in the command line: \"man curl\".  If \"curl\" has not been installed, you will see: \"curl command not found\".  To install curl, Google it.  If \"curl\" is installed, your console will look like normal man help pages.  You can keep pressing Enter to scroll through the curl manual.  To exit and return to your console, press q.  \n",
    "\n",
    "The basic syntax for curl has the following structure: \"curl [optional flags] [URL]\".  The URL is required  for the command to run successfully.  The \"curl\" supports a large number of protocal calls.  (including HTTP, HTTPS, FTP, SFTP etc).  For the full list using the \"curl --help\".  Lets download a single file stored at this hypothetical URL using curl.  To save the file with its original name \"datafilename.txt\", use the optional flag \"-O\" (dash uppercase O).  This reads \"curl -O URL\".  To save the file under a different name, replace -O (dash uppercase O) with -o (dash lowercase o) and new file name.  Now it reads \"curl -o newname URL\".  \n",
    "\n",
    "Often times, a server will host multiple data files, with similar filenames.  Like with different ending values.  Instead of curl each file individually, we can use wildcards (do you remember what we learned in introduction to shell course) to download all the files at once.  To download every file hostedon this server that starts with datafilename and end in \".txt\", we use: \"curl -o URLsomething*.txt\".  \n",
    "\n",
    "Another option is to increment using a globbing parser.  The following will download every files sequentially starting with data \"filename001.txt\" ane ending with data \"filename100.txt\".  Note that the end of the command that reads: open square bracket zero zero one dash one hundread close square bracket dot txt.  That is the globbing at work.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# curl -O https://websitename.com/datafilename[001-100].txt\n",
    "#                                             *********\n",
    "\n",
    "\n",
    "We can increment through the files and download every Nth file.  For example, to download every 10th file, we can modify the globbing parser to read: open square bracket zero zero one dash one hundred colon ten close square bracket dot txt.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# curl -O https://websitename.com/datafilename[001-100:10].txt\n",
    "#                                             ************\n",
    "\n",
    "\n",
    "# Sometimes internet can time out.  To make sure that our download progress is not lost, \n",
    "# *******************************************************************************************************************\n",
    "curl has these two flags: \n",
    "\"-L\" redirects the HTTP URL if a 300 error code occurs.  \n",
    "\"-C\" resumes a previous file transfer if it times out before completion.  \n",
    "Putting everything together.  Note that all option flags come before URL, but the order of the flags does not matter.  \n",
    "\n",
    "\n",
    "\n",
    "In this lesson, we learned how to download files using curl.  Lets put our new knowledge to practice.  Happy crul.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408bf61d-3275-468e-826b-1d8bd2569781",
   "metadata": {},
   "outputs": [],
   "source": [
    "jhu@debian:~$ curl -O https://assets.datacamp.com/production/repositories/4180/datasets/513986f5ea7ed9a8565bba20d088d21c10e099dc/Spotify_MusicAttributes.csv > ~/Downloads/Spotify_MusicAttributes.csv\n",
    "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
    "                                 Dload  Upload   Total   Spent    Left  Speed\n",
    "100  1717  100  1717    0     0   1382      0  0:00:01  0:00:01 --:--:--  1382\n",
    "jhu@debian:~$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b86aa54-6f85-4d13-a275-2414f7247f2f",
   "metadata": {},
   "source": [
    "## Using curl documentation\n",
    "\n",
    "As you work with command line tools you will often need to consult the documentation to remind yourself of the syntax or of some of the available functionality. In this exercise, you'll consult curl's documentation to answer this question:\n",
    "\n",
    "Based on the information in the curl manual, which of the following is NOT a supported file protocol:\n",
    "Instructions\n",
    "50 XP\n",
    "Possible Answers\n",
    "\n",
    "    LDAP\n",
    "    FTPS\n",
    "    HTTPS\n",
    "#    OFTP\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb0b824-a4d8-4867-a1af-f2c81cb76700",
   "metadata": {},
   "outputs": [],
   "source": [
    "jhu@debian:~$ curl --help\n",
    "Usage: curl [options...] <url>\n",
    " -d, --data <data>   HTTP POST data\n",
    " -f, --fail          Fail silently (no output at all) on HTTP errors\n",
    " -h, --help <category> Get help for commands\n",
    " -i, --include       Include protocol response headers in the output\n",
    " -o, --output <file> Write to file instead of stdout\n",
    " -O, --remote-name   Write output to a file named as the remote file\n",
    " -s, --silent        Silent mode\n",
    " -T, --upload-file <file> Transfer local FILE to destination\n",
    " -u, --user <user:password> Server user and password\n",
    " -A, --user-agent <name> Send User-Agent <name> to server\n",
    " -v, --verbose       Make the operation more talkative\n",
    " -V, --version       Show version number and quit\n",
    "\n",
    "This is not the full help, this menu is stripped into categories.\n",
    "Use \"--help category\" to get an overview of all categories.\n",
    "For all options use the manual or \"--help all\".\n",
    "jhu@debian:~$ man curl\n",
    "DESCRIPTION\n",
    "       curl  is  a tool to transfer data from or to a server, using one of the\n",
    "       supported protocols (DICT, FILE, FTP, FTPS, GOPHER, HTTP, HTTPS,  IMAP,\n",
    "       IMAPS,  LDAP,  LDAPS,  MQTT, POP3, POP3S, RTMP, RTMPS, RTSP, SCP, SFTP,\n",
    "       SMB, SMBS, SMTP, SMTPS, TELNET and TFTP). The command  is  designed  to\n",
    "       work without user interaction.\n",
    "\n",
    "       curl offers a busload of useful tricks like proxy support, user authen‐\n",
    "       tication, FTP upload, HTTP post, SSL connections, cookies, file  trans‐\n",
    "       fer  resume,  Metalink,  and more. As you will see below, the number of\n",
    "       features will make your head spin!\n",
    "\n",
    "       curl is powered by  libcurl  for  all  transfer-related  features.  See\n",
    "       libcurl(3) for details.\n",
    "\n",
    "PROTOCOLS\n",
    "       curl supports numerous protocols, or put in URL  terms:  schemes.  Your\n",
    "       particular build may not support them all.\n",
    "\n",
    "       DICT   Lets you lookup words using online dictionaries.\n",
    "\n",
    "       FILE   Read  or  write  local  files.  curl  does not support accessing\n",
    "              file:// URL remotely, but when running on Microsft Windows using\n",
    "              the native UNC approach will work.\n",
    "\n",
    "       FTP(S) curl  supports  the  File Transfer Protocol with a lot of tweaks\n",
    "              and levers. With or without using TLS.\n",
    "\n",
    "       GOPHER Retrieve files.\n",
    "\n",
    "       HTTP(S)\n",
    "              curl supports HTTP with numerous options and variations. It  can\n",
    "              speak HTTP version 0.9, 1.0, 1.1, 2 and 3 depending on build op‐\n",
    "              tions and the correct command line options.\n",
    "\n",
    "       IMAP(S)\n",
    "              Using the mail reading protocol, curl can \"download\" emails  for\n",
    "              you. With or without using TLS.\n",
    "\n",
    "       LDAP(S)\n",
    "              curl can do directory lookups for you, with or without TLS.\n",
    "\n",
    "       MQTT   curl supports MQTT version 3. Downloading over MQTT equals \"sub‐\n",
    "              scribe\" to a topic while uploading/posting equals \"publish\" on a\n",
    "              topic.  MQTT  support  is experimental and TLS based MQTT is not\n",
    "              supported (yet).\n",
    "\n",
    "       POP3(S)\n",
    "              Downloading from a pop3 server means getting  a  mail.  With  or\n",
    "              without using TLS.\n",
    "\n",
    "       RTMP(S)\n",
    "              The  Realtime  Messaging  Protocol  is  primarily used to server\n",
    "              streaming media and curl can download it.\n",
    "\n",
    "       RTSP   curl supports RTSP 1.0 downloads.\n",
    "\n",
    "       SCP    curl supports SSH version 2 scp transfers.\n",
    "\n",
    "       SFTP   curl supports SFTP (draft 5) done over SSH version 2.\n",
    "\n",
    "       SMB(S) curl supports SMB version 1 for upload and download.\n",
    "\n",
    "       SMTP(S)\n",
    "              Uploading contents to an SMTP server  means  sending  an  email.\n",
    "              With or without TLS.\n",
    "\n",
    "       TELNET Telling curl to fetch a telnet URL starts an interactive session\n",
    "              where it sends what it reads  on  stdin  and  outputs  what  the\n",
    "              server sends it.\n",
    "\n",
    "       TFTP   curl can do TFTP downloads and uploads.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4298c7-a2df-4dd7-b324-c0ae07485c15",
   "metadata": {},
   "source": [
    "## Downloading single file using curl\n",
    "\n",
    "Let's get some hands on practice for the more commonly used options and flags with curl. \n",
    "# The URL for the hosted file is a shortened URL using tinyurl. Because of that, we need to fill out a flag option that allows for redirected URLs.\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "\n",
    "    Question 1\n",
    "#    Fill in the option flag that allow downloading from a redirected URL.\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    In the same step as the download, add in the necessary syntax to rename the downloaded file as Spotify201812.zip.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7a1c61-32ac-4c50-8aa6-d005d9040656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use curl to download the file from the redirected URL\n",
    "curl -L -o Spotify201812.zip https://assets.datacamp.com/production/repositories/4180/datasets/eb1d6a36fa3039e4e00064797e1a1600d267b135/201812SpotifyData.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec27918-f8cf-466e-8b9b-c2c4c278b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jhu@debian:~$ cd ~/Downloads/\n",
    "jhu@debian:~/Downloads$ ls\n",
    "Spotify_MusicAttributes.csv\n",
    "Training_Machine_Learning_Surrogate_Models_From_a_.pdf\n",
    "jhu@debian:~/Downloads$ curl -L -o Spotify201812.zip https://assets.datacamp.com/production/repositories/4180/datasets/eb1d6a36fa3039e4e00064797e1a1600d267b135/201812SpotifyData.zip\n",
    "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
    "                                 Dload  Upload   Total   Spent    Left  Speed\n",
    "100 1944k  100 1944k    0     0   863k      0  0:00:02  0:00:02 --:--:--  863k\n",
    "jhu@debian:~/Downloads$ ls\n",
    "new_file  Spotify201812.zip  Spotify_MusicAttributes.csv  Training_Machine_Learning_Surrogate_Models_From_a_.pdf\n",
    "jhu@debian:~/Downloads$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997951d2-43c6-4232-a1fa-8d2e6cde6cb4",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Exercise\n",
    "Downloading multiple files using curl\n",
    "\n",
    "We have 100 data files stored in long sequentially named URLs. Scroll right to see the complete URLs.\n",
    "\n",
    "https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile001.txt\n",
    "https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile002.txt\n",
    "......\n",
    "https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile100.txt\n",
    "\n",
    "To minimize having to type the long URLs over and over again, we'd like to download all of these files using a single curl command.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Download all 100 data files using a single curl command.\n",
    "    Print all downloaded files to directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9afc010-a929-48fb-adc0-19868410e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all 100 data files\n",
    "curl -O https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile[001-100].txt\n",
    "#                                                                                                        #########\n",
    "\n",
    "# Print all downloaded files to directory\n",
    "ls datafile*.txt\n",
    "\n",
    "\n",
    "\n",
    "jhu@debian:~/Downloads$ mkdir NewFiles/\n",
    "jhu@debian:~/Downloads$ ls\n",
    "NewFiles           Training_Machine_Learning_Surrogate_Models_From_a_.pdf\n",
    "Spotify201812.zip\n",
    "jhu@debian:~/Downloads$ cd NewFiles/\n",
    "jhu@debian:~/Downloads/NewFiles$ curl -O https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile[001-030].txt\n",
    "\n",
    "[1/30]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile001.txt --> datafile001.txt\n",
    "--_curl_--https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile001.txt\n",
    "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
    "                                 Dload  Upload   Total   Spent    Left  Speed\n",
    "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
    "\n",
    "[2/30]: https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile002.txt --> datafile002.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7266c9e5-a188-4d3c-85a2-43afcd5b3153",
   "metadata": {},
   "source": [
    "## Downloading data using Wget\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Welcome back, in this lesson, we will introduce another command line tool for downloading data, called Wget.  We will walk through how to install and set up Wget along with some basic usage.  Wget derives its name from World Wide Web and Get.  It is a GNU project native to the Linux system, but is compatible across all operating systems.  It is another command line tool that will help you download files via HTTP and FTP.  \n",
    "\n",
    "\n",
    "# Compared to \"curl\", Wget is more multi-purpose.  It can download a single file, an entire folder, or even a webpage.  \n",
    "Most importantly, it makes multiple file downloads possible recursively.  Aside from using man, another way to check is Wget has been installed correctly, is by using \"which wget\" (just like Bash and Dash?).  This will return the location of where Wget is installed.  For example, in the local user bin: If Wget has not been installed, there will simply be no output.  For official documentation and source code of Wget, Google it.  Unless you are comfortable compiling from the source code, here are some easier alternatives.  \n",
    "\n",
    "For Linux users, it is likely Wget is already installed for you.  If not, run \"sudo apt-get install wget\", just Google it.  For Mac users, use homebrew by running \"brew install wget\".  For Windows users, this will not be a command line install.  Rather, download as part of the gunwin32 package.  Once the installation is complete, use the man command to print the Wget manual.  \n",
    "\n",
    "The basic syntax for Wget has a similar structure to curl: \"wget [optional flags] [URL]\".  The URL is also required for the Wget command to run successfully (isn't that obviously? we are doing URL request).  Wget supports a large number of protocal calls for data stored on servers.  For the full list of the options available, refer to \"wget --help\" or \"man wget\" or ask Google.  \n",
    "\n",
    "\n",
    "# Here are some option flags unique to Wget: \n",
    "\"-b\" allows your download to run in the background. \n",
    "\"-q\" turns off the wget output, which saves some disk spaces. \n",
    "\"-c\" is useful to finish up a previously broken download wheather by Wget or another program. \n",
    "\n",
    "Finally, you can link all the option flags together like this.  Running this command on this hypothetical file location will generate the output: \"Continuing in background, pid 12345.\"  The pid is unique process ID assigned to this particular data download job for your reference, in case you need to cancel the process.    ********************\n",
    "# *******************************************************************************************************************\n",
    "\n",
    "# wget -bqc https://websitename.com/datafilename.txt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In this lesson, we learned another way to download filesin the command line using the tool Wget.  Up next, we will put our new knowledge to practice and learn more advanced Wget use cases.  Happy wget.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540cdc8b-05ca-4a28-92b4-f0142c5c23ae",
   "metadata": {},
   "source": [
    "## Installing Wget\n",
    "\n",
    "# Unlike curl, there are several ways to download and install wget depending on which operating system your machine is running. Which of the following is NOT a way to install wget?\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "\n",
    "    On some Linux systems, Wget is already pre-installed\n",
    "    press\n",
    "    1\n",
    "    On Linux, install using apt-get\n",
    "    press\n",
    "    2\n",
    "    On Windows, install via gnuwin32\n",
    "    press\n",
    "    3\n",
    "#    On MacOS, install using pip       its not a Python package, its a command line program, Mac use brew XXX\n",
    "    press\n",
    "    4\n",
    "    On MacOS, install using homebrew\n",
    "    press\n",
    "    5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b314bf92-b894-4024-bd7c-50448cce80d0",
   "metadata": {},
   "source": [
    "## Downloading single file using wget\n",
    "\n",
    "Let's get some hands on practice for the option flags that make wget such a popular file downloading tool.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "#    Fill in the option flag for resuming a partial download.\n",
    "#    Fill in the option flag for letting the download occur in the background.\n",
    "    Preview the download log file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f2baf7-95e0-40e2-b9ea-dcb87dba0a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the two option flags \n",
    "wget -c -b https://assets.datacamp.com/production/repositories/4180/datasets/eb1d6a36fa3039e4e00064797e1a1600d267b135/201812SpotifyData.zip\n",
    "\n",
    "# Verify that the Spotify file has been downloaded\n",
    "ls \n",
    "\n",
    "# Preview the log file \n",
    "cat ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da662ca2-db99-4be5-9894-b3fe057f4c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "jhu@debian:~/Downloads$ wget -c -b https://assets.datacamp.com/production/repositories/4180/datasets/eb1d6a36fa3039e4e00064797e1a1600d267b135/201812SpotifyData.zip\n",
    "Continuing in background, pid 29917.\n",
    "Output will be written to ‘wget-log’.\n",
    "jhu@debian:~/Downloads$ ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f836824e-519b-49cb-834f-e9866c37a038",
   "metadata": {},
   "source": [
    "## Advanced downloading using Wget\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**So far, we've learned how to install and do basic file downloads using either \"curl\" or \"Wget\".  In this lesson, we'll focus on getting the most out of Wget by going over more advanced techniques for data downloading.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "A common way for data people to handle multiple file downloads is by storing the file locations in a file and pass that meta file to the downloading program like Wget.  In this case, all the URLs for the files we want to download are stored in the file \"URL_list.txt\".  Lets use the cat command to print and preview the URLs first.  After confirming that the URLs are indeed stored in this file, we can now pass this file to Wget.  Note that we need to preface this with \"-i\" option flag, so Wget knows that we are reading URLs from a local or external file.  The command reads \"wget -i URL_list.txt\".  Finally, its worth noting not to insert any option flags in between the \"-i\" and the RUL file.  If other option flags are needed, put it before \"-i\".  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Sometimes, its useful to make sure Wget doesn't consume your entire bandwidth with the file download.  You can set an upper download bandwidth limit using the \"--limit-rate\" option flag.  Set the limit rate equal to a whole number, which will automatically convert to kilobytes per second.  For example \"wget --limit-rate=200k -i RUL_list.txt\" will make sure your download rate will not exceed 200 kilobytes per second as you download the files saved in teh URL list.  For downloading smaller files, enforcing a download bandwidth won't work as well.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "To avoid overtaxing the file hosting server, it is more useful to enforce a mandatory wait time between file downloads using the \"--wait\" option flag.  The default time interval is set to seconds.  For example, in below command \"wget --wait=2.5 --limit-rate=200k -i URL_list.txt\", creates a 2.5 seconds pause between downloading each file stored in the URL list file.  \n",
    "\n",
    "\n",
    "As we round out this chapter, it is helpful to do a quick comparison between the 2 command line program tools \"curl\" and \"wget\".  Although both curl and wget can download files from HTTP, HTTPS, FTP.  The Curl alone can download and uploadrom 20 other protocols.  It is also easier to install across all operating systems, compared to wget.  Wget's advantage is its ability to handle multiple file downloads gracefully.  It can also be used to download just about anything, from a full file directory to a HTML page.  \n",
    "\n",
    "\n",
    "\n",
    "With both curl and wget at your disposal, you're now an expert at codnloading files on the command line.  Lets practice.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2d74e0-d99c-4192-949f-d3b5870ec1f9",
   "metadata": {},
   "source": [
    "## Setting constraints for multiple file downloads\n",
    "\n",
    "Which of the following is NOT the correct way to set download constraints for multiple file downloads using wget?\n",
    "Answer the question\n",
    "50XP\n",
    "Possible Answers\n",
    "\n",
    "#    Store all URL locations in a text file (e.g. url_list.txt) and iteratively download using wget and option flag i\n",
    "    press\n",
    "    1\n",
    "    Use wget with the --limit-rate option, followed by the download speed in KB/s.\n",
    "    press\n",
    "    2\n",
    "    Use wget with the --wait option, followed by the wait time in seconds.\n",
    "    press\n",
    "    3\n",
    "    \n",
    "Hint\n",
    "\n",
    "    wget -i url_list.txt iterates through the files but does not set any constraints for downloads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1cb67a-b541-48c1-9af2-99e81821495c",
   "metadata": {},
   "source": [
    "## Creating wait time using Wget\n",
    "\n",
    "For download smaller files, enforcing a mandatory wait time between file downloads makes sure we don't overload the server with too many requests. Here, we will using the built in option flag with wget to create a mandatory wait time (in seconds) between downloading each file stored in the URL list file.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Create a mandatory 1 second pause between downloading all files in url_list.txt.\n",
    "\n",
    "Hint\n",
    "\n",
    "    When in doubt, use man wget to find the correct syntax.\n",
    "    The --wait option flag needs to be followed by the time (default is in seconds) with format --wait={insert time in seconds}.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b402185-0148-4bd2-90df-41c827add25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View url_list.txt to verify content\n",
    "cat url_list.txt\n",
    "\n",
    "# Create a mandatory 1 second pause between downloading all files in url_list.txt\n",
    "wget --wait=1 -i url_list.txt\n",
    "\n",
    "# Take a look at all files downloaded\n",
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e3d838-0541-46e4-b702-5c9237c73a8a",
   "metadata": {},
   "source": [
    "## Data downloading with Wget and curl\n",
    "\n",
    "To kick off a data analysis project, it's good practice to first consolidate all of our data into one place. Often times, this means downloading and pulling data from various locations such as HTTP servers and databases.\n",
    "\n",
    "While curl is handy for downloading a single file, it's somewhat unwieldy for handling multiple file downloads. In this capstone exercise, we will use both curl and Wget to download a series of monthly Spotify files, do some minor processing, and consolidate all downloaded files in our local directory.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "#    Download the zipped 201812SpotifyData data saved in the shortened (redirected) URL using curl. In the same step, rename file as Spotify201812.zip.\n",
    "    Unzip Spotify201812.zip, delete the original zipped file, and rename the unzipped file to Spotify201812.csv to stay consistent.\n",
    "    Use url_list.txt and Wget to download all 3 files: Spotify201809.csv, Spotify201810.csv, and Spotify201811.csv in one step, with an upper cap download speed of 2500KB/s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6a0bf3-ed97-43d4-83c4-acc670285390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use curl, download and rename a single file from URL\n",
    "curl -o Spotify201812.zip -L https://assets.datacamp.com/production/repositories/4180/datasets/eb1d6a36fa3039e4e00064797e1a1600d267b135/201812SpotifyData.zip\n",
    "\n",
    "# Unzip, delete, then re-name to Spotify201812.csv\n",
    "unzip Spotify201812.zip && rm Spotify201812.zip\n",
    "mv 201812SpotifyData.csv Spotify201812.csv\n",
    "\n",
    "\n",
    "# View url_list.txt to verify content\n",
    "cat url_list.txt\n",
    "\n",
    "# Use Wget, limit the download rate to 2500 KB/s, download all files in url_list.txt\n",
    "wget --limit-rate=2500k -i url_list.txt\n",
    "\n",
    "# Take a look at all files downloaded\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf8a228-b71d-4cbd-8f9b-e32b3e538475",
   "metadata": {},
   "outputs": [],
   "source": [
    "jhu@debian:~/Downloads$ curl -o Spotify201812.zip -L https://assets.datacamp.com/production/repositories/4180/datasets/eb1d6a36fa3039e4e00064797e1a1600d267b135/201812SpotifyData.zip\n",
    "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
    "                                 Dload  Upload   Total   Spent    Left  Speed\n",
    "100 1944k  100 1944k    0     0   144k      0  0:00:13  0:00:13 --:--:--  152k\n",
    "jhu@debian:~/Downloads$ ls\n",
    "NewFiles           Training_Machine_Learning_Surrogate_Models_From_a_.pdf\n",
    "Spotify201812.zip\n",
    "jhu@debian:~/Downloads$ unzip Spotify201812.zip && rm Spotify201812.zip\n",
    "Archive:  Spotify201812.zip\n",
    "  inflating: 201812SpotifyData.csv   \n",
    "   creating: __MACOSX/\n",
    "  inflating: __MACOSX/._201812SpotifyData.csv  \n",
    "jhu@debian:~/Downloads$ rm -r __MACOSX/\n",
    "jhu@debian:~/Downloads$ ls\n",
    "201812SpotifyData.csv  Training_Machine_Learning_Surrogate_Models_From_a_.pdf\n",
    "NewFiles\n",
    "jhu@debian:~/Downloads$ mv 201812SpotifyData.csv SpotifyData_201812.csv \n",
    "jhu@debian:~/Downloads$ ls\n",
    "NewFiles                Training_Machine_Learning_Surrogate_Models_From_a_.pdf\n",
    "SpotifyData_201812.csv\n",
    "jhu@debian:~/Downloads$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf8764d-1446-4ee8-887d-072582f48035",
   "metadata": {},
   "source": [
    "## Getting started with csvkit\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Welcome back.  In this lesson, we will explore the basics of \"csvkit\" for data processing on the command line.  Data processing on the command line is computationally efficient and also quite simple once you are familiar with the syntax.  Yet generations of data professionals gravitate toward Python since it comes pre-built with libraries specific for data handling that bash commands lack.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# The \"csvkit\" helps to bridge this gap by bringing a suite of data commands to the command line.  \n",
    "Developed by Wireservice using Python, \"csvkit\" offers a variety of data conversion, processng and cleaning capabilities that rivals Python, R and even SQL.  Because the \"csvkit\" is written in Python, it can be installed with the Python package manager pip.  The syntax is \"pip install csvkit\", or \"pip install --upgrade csvkit\" for upgrade.  Google it for more information.  Unlike most command-line tools, csvkit, as a whole, does not respond to the man command.  Documentation is web-based. For each command in the csv-suite, however, this is different.  \n",
    "\n",
    "# \"in2csv\" command to convert Excel and others to CSV file\n",
    "For example \"in2csv\" is a useful command in csvkit suite, that converts tabular data files, like text or Excel, into CSV.  These is both a web-based documentation as well as command-line manual, use \"in2csv --help\" or \"in2csv -h\" to prints the help information.  The syntax involves calling in2csv, followed by the name of the file you wish to convert, in this case, SpotifyData.xlsx.  The redirect operator is followed by the name of the newly created CSV file SpotifyData.csv.  Please note that [in2csv SpotifyData.xlsx] alone just prints console the data on the first Excel sheetand does not generate a new file.  The redirect operator is crucial for redirecting and saving the output in the new file SpotifyData.csv.  \n",
    "\n",
    "#   in2csv SpotifyData.xlsx > SpotifyData.csv\n",
    "\n",
    "# What if the data we want is not in the first sheet? \n",
    "The \"csvkit\" does let us specify which sheet to convert in an Excel file.  First, use the \"--names\" or \"-n\" option flag to print all sheet names in SpotifyData.xlsx.  Than we use the \"--sheet\" to specify that we want to convert \"Worksheet1_popularity\".  Note the quotation marks around the sheet name.  We re-direct the output of the conversion to Spotify_Popularity.csv.  Please remember that \"in2csv\" does not print any logs to console.  For sanity check, we run ls to confirm that the new CSV has been created.  \n",
    "\n",
    "#   in2csv -n\n",
    "#   in2csv --sheet \"Worksheet1_Popularity\" > Spotify_Popularity.csv\n",
    "\n",
    "There are various ways to preview data on the command line, such as cat, less, more.  The \"csvlook\" also in the scvkit suite, prints CSV files to the command line in a Mark-compatible, fixed-width format thats easier on the eye.  For documentation, use csvlook -h.  Lets test this out on our newly created CSV file.  The following command line [csvlook Spotify_Popularity.csv] prints the Pandas DataFrame style in the console.  \n",
    "\n",
    "# Our last command for this lesson is \"csvstat\".  \n",
    "The \"csvstat\" is similar to the \".describe()\" method in Python's Pandas library.  It intelligently desciphrs the data type in each column of the CSV file and prints descriptive summary statistics for each column according to its data type, such as mean, median, and unique value counts.  [And I need to be the guy who can study through reading the documentation, not video tutorials or towardsdatascience articles.  Or at least can use Google to help me in understanding].  Using the popularity data again, this is a portion of the summary statistics \"csvstat\" prints out the first column, track_id, \n",
    "\n",
    "\n",
    "\n",
    "In this lesson, we learned 3 command line tools in the csvkit-suite, in2csv, csvlook, and csvstat.  Now put our new knowledge into practice.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c0db6c-f9e8-4bcf-9f28-ce3683e432e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "jhu@debian:~/.virtual_environments$ ls | grep \".xlsx\"\n",
    "battledeath.xlsx\n",
    "Data_Dictionary_WiFi_Hotspots.xlsx\n",
    "jhu@debian:~/.virtual_environments$ cp *.xlsx ~/Downloads/\n",
    "jhu@debian:~/.virtual_environments$ cd ~/Downloads/\n",
    "jhu@debian:~/Downloads$ ls\n",
    "battledeath.xlsx\n",
    "Data_Dictionary_WiFi_Hotspots.xlsx\n",
    "NewFiles\n",
    "SpotifyData_201812.csv\n",
    "Training_Machine_Learning_Surrogate_Models_From_a_.pdf\n",
    "jhu@debian:~/Downloads$ csvlook Data_Dictionary_WiFi_Hotspots.xlsx \n",
    "bash: csvlook: command not found\n",
    "jhu@debian:~/Downloads$ cd ~/.virtual_environments/\n",
    "jhu@debian:~/.virtual_environments$ csvlook\n",
    "bash: csvlook: command not found\n",
    "jhu@debian:~/.virtual_environments$ source py39/bin/activate\n",
    "(py39) jhu@debian:~/.virtual_environments$ csvlook -h\n",
    "bash: csvlook: command not found\n",
    "(py39) jhu@debian:~/.virtual_environments$ csvsuite\n",
    "bash: csvsuite: command not found\n",
    "(py39) jhu@debian:~/.virtual_environments$ csvkit\n",
    "bash: csvkit: command not found\n",
    "(py39) jhu@debian:~/.virtual_environments$ pip install --upgrade csvkit\n",
    "Collecting csvkit\n",
    "  Downloading csvkit-1.0.6-py2.py3-none-any.whl (42 kB)\n",
    "     |████████████████████████████████| 42 kB 21 kB/s            \n",
    "Collecting agate-dbf>=0.2.0\n",
    "\n",
    "\n",
    "(py39) jhu@debian:~/.virtual_environments$ in2csv -n battledeath.xlsx \n",
    "2002\n",
    "2004\n",
    "(py39) jhu@debian:~/.virtual_environments$ in2csv --sheet \"2002\" battledeath.xlsx > ~/Downloads/2002battledeath.csv \n",
    "/home/jhu/.virtual_environments/py39/lib/python3.9/site-packages/agate/utils.py:285: UnnamedColumnWarning: Column 2 has no name. Using \"c\".\n",
    "/home/jhu/.virtual_environments/py39/lib/python3.9/site-packages/agate/utils.py:285: UnnamedColumnWarning: Column 3 has no name. Using \"d\".\n",
    "/home/jhu/.virtual_environments/py39/lib/python3.9/site-packages/agate/utils.py:285: UnnamedColumnWarning: Column 4 has no name. Using \"e\".\n",
    "(py39) jhu@debian:~/.virtual_environments$ csvlook ~/Downloads/2002battledeath.csv \n",
    "| War, age-adjusted mortality due to |     2002 | c | d | e |\n",
    "| ---------------------------------- | -------- | - | - | - |\n",
    "| Afghanistan                        |  36.084… |   |   |   |\n",
    "| Albania                            |   0.129… |   |   |   |\n",
    "| Algeria                            |  18.314… |   |   |   |\n",
    "| Andorra                            |   0.000… |   |   |   |\n",
    "| Angola                             |  18.965… |   |   |   |\n",
    "\n",
    "\n",
    "\n",
    "(py39) jhu@debian:~/.virtual_environments$ csvstat ~/Downloads/2002battledeath.csv \n",
    "  1. \"War, age-adjusted mortality due to\"\n",
    "\n",
    "\tType of data:          Text\n",
    "\tContains null values:  False\n",
    "\tUnique values:         192\n",
    "\tLongest value:         32 characters\n",
    "\tMost common values:    Afghanistan (1x)\n",
    "\t                       Albania (1x)\n",
    "\t                       Algeria (1x)\n",
    "\t                       Andorra (1x)\n",
    "\t                       Angola (1x)\n",
    "\n",
    "  2. \"2002\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2de980c-8468-4c28-ad04-4ddcdd13a476",
   "metadata": {},
   "source": [
    "## Installation and documentation for csvkit\n",
    "\n",
    "First step in learning about any libraries, tools, or suite of tools is to make sure we are using the latest and most stable version.\n",
    "\n",
    "Second step is to make sure we know how to access the documentation so we know where to go when we get stuck.\n",
    "\n",
    "Let's do both in this exercise for csvkit and the various commands in this suite of data processing command-line tools.\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "\n",
    "    Question 1\n",
    "    Upgrade csvkit to the latest version using Python package manager pip\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    Print the manual for in2csv on the command line.\n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "    Print the manual for csvlook on the command line.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f77dc9-76b0-454d-8ea5-892a116ff0ee",
   "metadata": {},
   "source": [
    "# Now imagine what code to type, and what outcome pops out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c048284-0758-4c59-acf4-94ad38b0f95c",
   "metadata": {},
   "source": [
    "## File conversion and summary statistics with csvkit\n",
    "\n",
    "It's common for Excel data files to have more than one worksheet (tab) of data. The Excel file SpotifyData.xlsx has two sheets named Worksheet1_Popularity and Worksheet2_MusicAttributes. Each sheet should be treated like its own data file, so we will use csvkit's commands here to convert each sheet to its own CSV file. Then, using the power of the commands we already know, print a high level summary for each column in the CSV files.\n",
    "Instructions 1/4\n",
    "25 XP\n",
    "\n",
    "    Question 1\n",
    "#    From SpotifyData.xlsx, convert the sheet \"Worksheet1_Popularity\" to CSV and call it Spotify_Popularity.csv.\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "#    Print the high level summary statistics for each column in Spotify_Popularity.csv.\n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "#    From SpotifyData.xlsx, convert the tab \"Worksheet2_MusicAttributes\" to CSV and call it Spotify_MusicAttributes.csv.\n",
    "    \n",
    "    \n",
    "    Question 4\n",
    "#    Print a preview of Spotify_MusicAttributes.csv using a function in csvkit with Markdown-compatible, fixed-width format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78548fc4-11c4-4d8d-975b-ef366429b829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to confirm name and location of the Excel data file\n",
    "ls\n",
    "\n",
    "# Convert sheet \"Worksheet1_Popularity\" to CSV\n",
    "___ SpotifyData.xlsx ___ \"Worksheet1_Popularity\" > Spotify_Popularity.csv\n",
    "\n",
    "\n",
    "\n",
    "# Think and think, recall what we have used previously? ###########################################################  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf972422-142e-4877-941f-a51fc94d9d03",
   "metadata": {},
   "source": [
    "## Filtering data using csvkit\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**In the previous lesson, we set ourselves up for success by preparing our data using \"csvkit\".  Now we will dig deeper and learn how to use csvkit's data filtering commands.  \n",
    "\n",
    "# Because our data files are tabular, we can filter by creating a subset of the original data by column or row.  \n",
    "We can use the \"csvcut\" command to filter data by column.  And we can use the \"csvgrep\" to filter by row.  \n",
    "\n",
    "The csvcut can filter file and truncate CSV files by either column name or column position.  For full documentation, use [csvcut --help] If you don't have column names or positions memorized, thats perfectly okay.  The \"csvcut\" has a \"--names\" or \"-n\" option flag, which prints a list of the column names and positions.  Using this command line  [csvcut -n Spotify_MusicAttributes.csv], we see that Spotify_MusicAttributes has 3 columns, track_id, dancebility, and duration_ms.  By referring to the output of the [csvcut -n], we can now filter the data more easily.  \n",
    "\n",
    "# Suppose we want to return only the first column track_id, \n",
    "we can do so by referring to that column by position: using this command \"csvcut -c 1 Spotify_MusicAttributes.csv\",  You can generate the same output by referring to the column name by replacing the position \"1\" with the column name, in double parentheses.  By using the below command line [csvcut -c \"track_id\" Spotify_MusicAttributes.csv].  The output as the same as before.  You can specify more than one column for filtering.  To return the second and third columns, separate the column numbers by commas.  Like this command: [csvcut -c 2,3 Spotify_MusicAttributes.csv].  Note that there is no space between the numbers two and three.  Inserting a space will generate an error.  Same as before, we can generate the same output by replacing the column numbers with names.  [csvcut -c \"track_id\",\"duration_ms\" Spotify_MusicAttributes.csv].  Note again, that column names are wrapped in double quotations, and the columns are separated by comma, with no space.  \n",
    "\n",
    "# The \"csvgrep\" is our go-to in csvkit for filtering data by row value.  \n",
    "Despite its name, csvgrep can filter by both extract match or regex fuzzy match.  It is important to remember that csvgrep must be paired with one of these three option flags, \"-m\" [followed by the exact row value to filter], \"-r\" [followed with a regex pattern], or \"-f\" [followed by the path to a file].  We will focus on \"-m\" in this lesson.  As always, please use the \"-h\" for more ducumentations.  Lets say we want to filter by a certain track_id.  We can do so by following command [csvgrep -c \"track_id\" -m 5RCPsfzEpTXMCTNk7wEfQ Spotify_MusicAttributes.csv]  This will return the entire rows that contains this track_id.  Similar to column filtering with csvcut, we can pass the column location instead.  Keeping everything else the same, we replace track_id with the column position number 1 instead.  This will give us the same output.  \n",
    "\n",
    "\n",
    "\n",
    "We just keep adding more csvkit commands to our data processing toolket.  Lets put these skills to good use with some exercises.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535192cf-af7e-4490-be43-8688db25fe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "(py39) jhu@debian:~/.virtual_environments$ csvcut --names ~/Downloads/2002battledeath.csv \n",
    "  1: War, age-adjusted mortality due to\n",
    "  2: 2002\n",
    "  3: c\n",
    "  4: d\n",
    "  5: e\n",
    "(py39) jhu@debian:~/.virtual_environments$ csvcut -n ~/Downloads/2002battledeath.csv \n",
    "  1: War, age-adjusted mortality due to\n",
    "  2: 2002\n",
    "  3: c\n",
    "  4: d\n",
    "  5: e\n",
    "(py39) jhu@debian:~/.virtual_environments$ \n",
    "\n",
    "\n",
    "  -c COLUMNS, --columns COLUMNS\n",
    "                        A comma-separated list of column indices, names or\n",
    "                        ranges to be extracted, e.g. \"1,id,3-5\". Defaults to\n",
    "                        all columns.\n",
    "  -C NOT_COLUMNS, --not-columns NOT_COLUMNS\n",
    "                        A comma-separated list of column indices, names or\n",
    "                        ranges to be excluded, e.g. \"1,id,3-5\". Defaults to no\n",
    "                        columns.\n",
    "  -x, --delete-empty-rows\n",
    "                        After cutting, delete rows which are completely empty.\n",
    "(py39) jhu@debian:~/.virtual_environments$ \n",
    "\n",
    "\n",
    "                                                     #######  If you have intager column name  ######################\n",
    "(py39) jhu@debian:~/.virtual_environments$ csvcut -c \"$2002\" ~/Downloads/2002battledeath.csv \n",
    "2002\n",
    "36.08399\n",
    "0.1289084\n",
    "18.31412\n",
    "0\n",
    "18.96456\n",
    "0\n",
    "0\n",
    "0.1702969\n",
    "0\n",
    "\n",
    "\n",
    "(py39) jhu@debian:~/.virtual_environments$ csvgrep -c 1 -m \"United States\" ~/Downloads/2002battledeath.csv \n",
    "\"War, age-adjusted mortality due to\",2002,c,d,e\n",
    "United States,0.014938,,,\n",
    "(py39) jhu@debian:~/.virtual_environments$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdbb5d2-110e-45c5-ad2c-903bbd7cb318",
   "metadata": {},
   "source": [
    "# Remember what we've learned in Introduction To Shell course?\n",
    "\n",
    "# To get the variable's value, you must put a dollar sign $ in front of it. Typing\n",
    "\n",
    "echo $USER\n",
    "\n",
    "prints\n",
    "\n",
    "repl\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "This is true everywhere: to get the value of a variable called X, you must write $X. (This is so that the shell can tell whether you mean \"a file named X\" or \"the value of a variable named X\".)\n",
    "Instructions\n",
    "100 XP\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1fcb06-ee63-4cc4-92e8-c1ce6f03e944",
   "metadata": {},
   "source": [
    "## Printing column headers with csvkit\n",
    "\n",
    "There are many ways to preview the data within csvkit alone(e.g. csvlook, csvstat, etc). However, if all we want is to find the position and name of the columns in our data, it is easier to simply print a string of column headers. Let's print the column headers for the data file Spotify_MusicAttributes.csv.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Print in console a list of column headers in the data file Spotify_MusicAttributes.csv using a csvkit command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0195f5-bb79-4726-af29-e296bd169b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to confirm name and location of data file\n",
    "ls\n",
    "\n",
    "\n",
    "# Print a list of column headers in data file    ####################################################################\n",
    "___ ___ Spotify_MusicAttributes.csv              # You must think, trying harder to recall what you have learned\n",
    "\n",
    "\n",
    "\n",
    "(py39) jhu@debian:~/.virtual_environments$ csvcut -n ~/Downloads/2002battledeath.csv \n",
    "  1: War, age-adjusted mortality due to\n",
    "  2: 2002\n",
    "  3: c\n",
    "  4: d\n",
    "  5: e\n",
    "(py39) jhu@debian:~/.virtual_environments$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d12729e-4167-4112-9fd9-8f7628dac803",
   "metadata": {},
   "source": [
    "## Filtering data by column with csvkit\n",
    "\n",
    "Let's get some hands-on practice for filtering data column using the csvkit command csvcut. Remember that we can filter columns by referring to the position of the column (e.g. 1st column, 2nd column) or by referring to the exact name of the column as it appears in the data file.\n",
    "Instructions 1/4\n",
    "25 XP\n",
    "\n",
    "    Question 1\n",
    "    Print the first column in Spotify_MusicAttributes.csv by referring to the column by its position in the file.\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    Print the first, third, and fifth column in Spotify_MusicAttributes.csv by referring to them by position.\n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "    Print the first column in Spotify_MusicAttributes.csv by referring to the column by its name.\n",
    "    \n",
    "    \n",
    "    Question 4\n",
    "    Print the first, third, and fifth column in Spotify_MusicAttributes.csv by referring to them by name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59da9453-0874-4776-ad54-478955a40156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a list of column headers in the data  #######################################################################\n",
    "csvcut -n Spotify_MusicAttributes.csv         ### Everytime you saw a line of code, trying to recall what is it\n",
    "                                              ### Push youself forard enough to change something, to be someone\n",
    "\n",
    "\n",
    "# Print the first column, by position\n",
    "csvcut -c 1 Spotify_MusicAttributes.csv\n",
    "\n",
    "\n",
    "csvcut -c 1,2,3 Spotify_MusicAttributes.csv\n",
    "\n",
    "\n",
    "csvcut -c \"track_id\",\"duration_ms\" Spotify_MusicAttributes.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb19df7-bd98-41b7-aaa2-8c38c6d05a4a",
   "metadata": {},
   "source": [
    "## Filtering data by row with csvkit\n",
    "\n",
    "Now it's time get some hands-on practice for filtering data by exact row values using -m. Whether it's text or numeric, csvgrep can help us filter by these values.\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "\n",
    "    Question 1\n",
    "#    Filter Spotify_MusicAttributes.csv and return the row or rows where track_id equals118GQ70Sp6pMqn6w1oKuki.\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "#    Filter Spotify_MusicAttributes.csv and return the row or rows where danceability equals 0.812.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abafc69-51bf-4530-ab80-525889298e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a list of column headers in the data \n",
    "csvcut -n Spotify_MusicAttributes.csv\n",
    "\n",
    "\n",
    "# Filter for row(s) where track_id = 118GQ70Sp6pMqn6w1oKuki\n",
    "csvgrip -c \"track_id\" -m 118GQ70Sp6pMqn6w1oKuki Spotify_MusicAttributes.csv\n",
    "\n",
    "\n",
    "\n",
    "csggrip -c \"danceability\" -m \"0.812\" Spotify_MusicAttributes.csv\n",
    "\n",
    "\n",
    "\n",
    "(py39) jhu@debian:~/.virtual_environments$ csvgrep -c \"$2002\" -m \"0\" ~/Downloads/2002battledeath.csv  ###############\n",
    "\"War, age-adjusted mortality due to\",2002,c,d,e\n",
    "Afghanistan,36.08399,,,\n",
    "Albania,0.1289084,,,\n",
    "Andorra,0,,,\n",
    "Antigua and Barbuda,0,,,\n",
    "Argentina,0,,,\n",
    "Armenia,0.1702969,,,\n",
    "Australia,0,,,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5055d45b-7a47-4ab7-a2f5-8d28ecab1fe3",
   "metadata": {},
   "source": [
    "## Stacking data and chaining commands with csvkit\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "**In this lesson, lets explore some more advanced techniques in csvkit, whether it is to chain multiple commands together, or to process more than one file at a time.  The \"csvstack\" command stacksthe rows form two or more CSV files together.  This is often used when you have files with the same schema but have been downloaded in chunks due to downloading restrictions such as API request restrictions.  \n",
    "\n",
    "First, lets go over some finer points.  Suppose we have two files, Spotify_Rank6.csv and Spotift_Rank7.csv and we want to stack them together to create one file.  The \"csvstack\" can do this, but first, we need to make sure that the input files have the same numberof columns, in the same columns order, with the same data types.  Lets preview Spotify_Rank6.csv and Spotify_Rank7.csv, the output show us the same: column track_id and popularity.  It looks like both files share the same schema.  We are ready for csvstack.  \n",
    "\n",
    "The syntax for stacking is: [csvstack Spotify_Rank6.csv Spotify_Rank7.csv > Spotify_AllRanks.csv].  Assume both files contain 2 rows each, this results in a 4 row file for Spotify_AllRank.csv.  However, its not always clear how to trace back which row in the final stacked file came from which source file.  To keep a record of the source of the data row, use csvstack's option flag \"-g\" followed by a user entered value to create a source column called group.  [csvstack -g \"Rank6\",\"Rank7\" Spotify_Rank6.csv Spotify_Rank7.csv > Spotify_AllRank.csv].  We doing this by inserting the above syntax after csvstack.  Please note that you can use a backslach(\\) to overcome line overflow.  Now when we preview the new Spotify_AllRank.csv, We see that there's a new column, by default named \"group\", that has the value Rank6 for every row that came from Spotify_Rank6.csv.  Finally, if we want to rename the column from group to something else, we can also do so by adding \"-n\" followed by the new column name.  Hwere is our example command: [csvstack -g \"Rank6\",\"Rank7\" -n \"Source\" Spotify_Rank6.csv Spotify_Rank7.csv > Spotify_AlLRank.csv].  \n",
    "\n",
    "\n",
    "The more we use command line tools, the more we start structuring complex commands.  [remember that one to search all the Excel files in pwd, ls | grep \".xlsx\"].  Just like English where we start combining independent sentences with conjunctions to from complex sentences, in command-line, we use operators to chain together commands in the same line.  \n",
    "# The semi-colon (;) operator links and runs multiple commands in a single line in the command line, \n",
    "sequentially.  Here, we will get both csvlook and csvstat output for the same file.  The command is as followed: [csvlook SpotifyData_All.csv; csvstst SpotifyData_All.csv].  \n",
    "# The double AND (&&) operator also links commands together, \n",
    "but the second command will only execute if the first command succeeds.  Here the csvstat will only run after csvlook succeeds.  \n",
    "# We are already familiar with the re-direct (>) operator.  \n",
    "This re-directs the output from the first command and saves it to the location in the second command.  [in2csv SpotifyData.xlsx > SpotifyData.csv].   \n",
    "# The pipe (|) operator uses the output of the first command as input to the second command.  \n",
    "Here, csvcut filters the data and prints columns track_id, and danceability, but notice that the output is not well formated.  By passing the outputof csvcut as input to csvlook using the pipe operator, the results are printed much more neatly.  [csvcut -c \"track_id\",\"danceability\" Spotify_Popularity.csv | csvlook].  The result are printed much more neatly.  \n",
    "\n",
    "\n",
    "\n",
    "We have learned a lot about data processing with various csvkit commands.  Lets practice putting everything together.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3ce50c-94ad-45dd-8fd3-2974418b1c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "(py39) jhu@debian:~/.virtual_environments$ csvlook --max-rows=10 ~/Downloads/2002battledeath.csv \n",
    "| War, age-adjusted mortality due to |    2002 | c | d | e |\n",
    "| ---------------------------------- | ------- | - | - | - |\n",
    "| Afghanistan                        | 36.084… |   |   |   |\n",
    "| Albania                            |  0.129… |   |   |   |\n",
    "| Algeria                            | 18.314… |   |   |   |\n",
    "| Andorra                            |  0.000… |   |   |   |\n",
    "| Angola                             | 18.965… |   |   |   |\n",
    "| Antigua and Barbuda                |  0.000… |   |   |   |\n",
    "| Argentina                          |  0.000… |   |   |   |\n",
    "| Armenia                            |  0.170… |   |   |   |\n",
    "| Australia                          |  0.000… |   |   |   |\n",
    "| Austria                            |  0.000… |   |   |   |\n",
    "| ...                                |     ... | ... | ... | ... |\n",
    "(py39) jhu@debian:~/.virtual_environments$ \n",
    "\n",
    "\n",
    "\n",
    "#####################################################################################################################\n",
    "(py39) jhu@debian:~/.virtual_environments$ csvcut -c 1,2 ~/Downloads/2002battledeath.csv | csvlook --max-row=7\n",
    "| War, age-adjusted mortality due to |    2002 |\n",
    "| ---------------------------------- | ------- |\n",
    "| Afghanistan                        | 36.084… |\n",
    "| Albania                            |  0.129… |\n",
    "| Algeria                            | 18.314… |\n",
    "| Andorra                            |  0.000… |\n",
    "| Angola                             | 18.965… |\n",
    "| Antigua and Barbuda                |  0.000… |\n",
    "| Argentina                          |  0.000… |\n",
    "| ...                                |     ... |\n",
    "(py39) jhu@debian:~/.virtual_environments$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743aa977-8dbb-4a8b-8647-7c24885cbc93",
   "metadata": {},
   "source": [
    "## Stacking files with csvkit\n",
    "\n",
    "SpotifyData_PopularityRank6.csv and SpotifyData_PopularityRank7.csv have the same file format, column order, and overall data schema. However, one file contains information for songs ranked #6, and the other contains information for songs ranked #7. Combine the two files together into one unified file by stacking them.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Stack SpotifyData_PopularityRank6.csv and SpotifyData_PopularityRank7.csv together. Re-direct the output of this stacking and save as a new file called SpotifyPopularity.csv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59e649b-84b0-4b71-b345-b658331c022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the two files and save results as a new file\n",
    "csvstack SpotifyData_PopularityRank6.csv SpotifyData_PopularityRank7.csv ___ SpotifyPopularity.csv\n",
    "\n",
    "\n",
    "# Preview the newly created file \n",
    "csvlook SpotifyPopularity.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf820ea8-7f91-4d36-8ccf-484c54a4020b",
   "metadata": {},
   "source": [
    "## Chaining commands using operators\n",
    "\n",
    "The more we use command-line tools, the more we start stringing complex commands together. Sometimes it's for convenience, but other times, the output of one command can be used as input to another. Let's get some hands on practice with this by filling in the correct chain operators for the circumstances described in the instructions below.\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "\n",
    "    Question 1\n",
    "    Use the chain operator that allows csvlook to run first, and if it succeeds, then run csvstat.\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "#    Use the chain operator that to pass the output of csvsort as input to csvlook.     **pipe will do this job | \n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "#    Use the 2 chain operators that takes the top 15 results from the sorted output and saves it to a new file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eb9291-53a0-4d62-9aac-331c4cbaef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If csvlook succeeds, then run csvstat \n",
    "csvlook Spotify_Popularity.csv && csvstat Spotify_Popularity.csv\n",
    "\n",
    "\n",
    "\n",
    "# Remember we see this before, we have wget the zip file, then \n",
    "unzip filename.zip && rm filename.zip\n",
    "\n",
    "(reverse-i-search)`&&': unzip Spotify201812.zip && rm Spotify201812.zip\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "csvsort - c 2 Spotify_Popularity.csv | head - n 15 > Spotify_Popularity_Top15.csv\n",
    "#####################################################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac9b59c-3cab-40f9-b26f-65c345309ba7",
   "metadata": {},
   "source": [
    "## Data processing with csvkit\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Once we have assembled a dataset, we still need to process and clean the data prior to more advanced analysis such as predictive modeling. In this capstone exercise, let's make use of various commands in csvkit for some common data processing and cleaning.\n",
    "\n",
    "The Excel file Spotify_201809_201810.xlsx contains two sheets (tabs), named Spotify201809 and Spotify201810. First, we will split the Excel file down to its individual sheets, preview summary statistics, remove some columns, and then stack the two sheets back together again to form one single csv file, ready for further analysis.\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "\n",
    "    Question 1\n",
    "    Convert the Spotify201809 sheet into its own csv file named Spotify201809.csv.\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "#    Familiarize ourselves with the column names by printing a preview of the file using a function in csvkit.\n",
    "#    Find the column names for song track and popularity rank. Create a new CSV containing only these 2 columns.\n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "#    Stack Spotify201809_subset.csv and Spotify201810_subset.csv together to form 1 csv file and create a new column with either Sep2018 or Oct2018, depending on original file source. Leave the name of the new column to its default group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde30f79-560f-4a19-a5b3-bb46f52ebd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Spotify201809 sheet into its own csv file \n",
    "___ Spotify_201809_201810.xlsx ___ ___ ___ Spotify201809.csv\n",
    "\n",
    "# Check to confirm name and location of data file\n",
    "ls\n",
    "\n",
    "csvlook Spotify201809.csv\n",
    "\n",
    "# Create a new csv with 2 columns: track_id and popularity\n",
    "csvcut -c \"track_id\",\"popularity\" Spotify201809.csv > Spotify201809_subset.csv\n",
    "\n",
    "# While stacking the 2 files, create a data source column\n",
    "csvstack -g \"Sep2018\",\"Oct2018\" Spotify201809_subset.csv Spotify201810_subset.csv > Spotify_all_rankings.csv\n",
    "#####################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "(py39) jhu@debian:~/Downloads$ csvcut -n SpotifyData_201812.csv \n",
    "  1: artist_name\n",
    "  2: track_id\n",
    "  3: track_name\n",
    "  4: acousticness\n",
    "  5: danceability\n",
    "  6: duration_ms\n",
    "  7: energy\n",
    "  8: instrumentalness\n",
    "  9: key\n",
    " 10: liveness\n",
    " 11: loudness\n",
    " 12: mode\n",
    " 13: speechiness\n",
    " 14: tempo\n",
    " 15: time_signature\n",
    " 16: valence\n",
    " 17: popularity\n",
    "(py39) jhu@debian:~/Downloads$ \n",
    "\n",
    "\n",
    "\n",
    "csvcut - c \"track_id\",\"popularity\" Spotify201809.csv > Spotify201809_subset.csv\n",
    "\n",
    "\n",
    "\n",
    "csvstack -g \"09\",\"10\" -n \"Source\" Spotify201809.csv Spotify201810.csv > Spotify_All.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d99277-ceb6-4ffa-b87f-ec2a0783bf30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27732d1f-7a7b-442e-8b89-20573296ce36",
   "metadata": {},
   "source": [
    "## Pulling data from database\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "# *******************************************************************************************************************\n",
    "**Welcome back, In this chapter, we'll learn how csvkit library brings SQL-like functionalities to the command line.  First, lets see how we can pull data from databases using the command \"sql2csv\".  \n",
    "\n",
    "# The \"sql2csv\" is a command in the csvkit library that allows us to access a variety of popular SQL databases and dialects, including Microsoft SQL Server, MySQL, Oracle, PostgreSQL and SQLite.  \n",
    "\n",
    "The beauty of this is that sql2csv gives us access via the command line, without having to go through database clients like PgAdmin or TablePlus.  Like the name auggests, sql2csv executes SQL commands, pulls data from databases, and saves the results as a local CSV file.  For full documentation, use the [sql2csv -h] to access.  The documentation is very short, and you have to be comfortable learning functionality through reading these command doc.  The syntax for sql2csv is fairly involved, so lets take this step by step.  First, the entirety of the code block here is one command.  \n",
    "\n",
    "[sql2csv --bd \"sqlite:///SpotifyDatabase.db\" \\\n",
    "         --query \"SELECT * FROM Spotify_Popularity\" \\\n",
    "         > Spotify_Popularity.csv]\n",
    "         \n",
    "Because the syntax is so long, for readability purposes, we need to break it into several lines by using the backslash (\\) at teh end of each line.  The back-slash (\\) tells our terminal that this particular command hasn't finished yet and needs to continue on in the next line.  While you might not need to do this if you are typing directly into the terminal, this is helpful to know if you are saving your commands in, say a shell script.  Each langurage has its own styling guide for how long a line of code can be.  Foe shell scripts, the limit is 80 characters per line.  \n",
    "\n",
    "The \"sql2csv\" needs us to tell it how to connect to the database.  This is done using the [--db] option, followed by a string, in quotation marks, denoting the database connection and location.  Because the sql2csv is compatible with multiple typesof SQL databases, the syntax also varies for each type.  For example, for SQLite databases, the string starts with [sqlite:///] followed by the database name and then end with [.db], short for database.  Had this been a Postgres or MySQL databases, then the string would have started with [postgres:///] or [mysql:///], but there would be no need for the ending database extension.  \n",
    "The second part of this command is [--query] option.  Here we pass in the SQL query to be used to pull data from the tables stored in the database.  Like with the previous argument, the SQL query is also a string so it also needs to be in quotation marks.  For our sample syntax, teh SpotifyDatabase has a table called \"Spotify_Popularity\".  To pul all the data from this table, we pass in the query: \"SQLECT * FROM Spotify_Popularity\".  \n",
    "\n",
    "# Two things worth noting.  \n",
    "One, different databaes have slightly different variations on SQL syntax.  Always use the SQL syntax that is compatible with the database specified.  \n",
    "Two, the query string always needs to be written in one single line, no matter how long or complex the query.  Otherwise, it would lead to unexpected errors.  \n",
    "\n",
    "\n",
    "Lastly, if we do not re-direct the output of our SQL query, then sql2csv will simply prints all the query results in the console log.  Th re-direct sign takes our query results and saves it to a new CSV file in our current directory.  Here, we save the output to Spotify_Popularity.csv.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "That's a lot going on with just one sql2csv command.  Lets put our new knowledge to practice.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0137d1b9-4cc8-4aa1-a742-5d4cd51dc9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to do that on my debian machine too, keep learning interest and fun is a good trick\n",
    "#####################################################################################################################\n",
    "\n",
    "\n",
    "(py39) jhu@debian:~/.virtual_environments$ ls | grep *.db\n",
    "test.db\n",
    "(py39) jhu@debian:~/.virtual_environments$ sqlite3 test.db \n",
    "SQLite version 3.34.1 2021-01-20 14:10:07\n",
    "Enter \".help\" for usage hints.\n",
    "sqlite> .databases\n",
    "main: /home/jhu/.virtual_environments/test.db r/w\n",
    "sqlite> .tables\n",
    "test_table\n",
    "sqlite> \n",
    "\n",
    "(py39) jhu@debian:~/.virtual_environments$ sql2csv --db \"sqlite:///test.db\" --query \"SELECT * FROM test_table\" > Test123.csv\n",
    "(py39) jhu@debian:~/.virtual_environments$ cat Test123.csv \n",
    "seller_id,customer_id,product_id,product_price,product_quantity\n",
    "10001,2,1023,100,12\n",
    "10001,2,1024,120,32\n",
    "10001,2,1023,100,24\n",
    "10001,2,1026,130,9\n",
    "10001,2,1025,128,32\n",
    "10001,2,1027,132,12\n",
    "10002,2,2027,122,24\n",
    "10002,2,2034,231,24\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187c2bbd-a159-49ff-a2ff-160eaeb7f650",
   "metadata": {},
   "source": [
    "## Using sql2csv documentation\n",
    "\n",
    "Suppose you're trying to run a query with sql2csv but you've been having issues because the error message is not detailed enough to help debug the error. Which optional argument in sql2csv will print detailed tracebacks and logs when errors occur while using sql2csv?\n",
    "Instructions\n",
    "50 XP\n",
    "Possible Answers\n",
    "\n",
    "#    -h or --help\n",
    "    -v or --verbose\n",
    "    -l or --linenumbers\n",
    "    -V or --version\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b986166a-9401-40e2-9ec2-984b7f32d1d1",
   "metadata": {},
   "source": [
    "## Understand sql2csv connectors\n",
    "\n",
    "Suppose you have a SQL database you would like to connect to using [sql2csv], but you're not sure yet if this particular database can be connected to. sql2csv's manual does not readily have the list of possible database connectors, but csvsql does!\n",
    "\n",
    "Could you use csvsql's manual to check what SQL database connections are currently NOT supported for sql2csv and for the rest of the csvkit suite?\n",
    "Instructions\n",
    "50 XP\n",
    "Possible Answers\n",
    "\n",
    "    Firebird\n",
    "    Microsoft SQL Server (mssql)\n",
    "    MySQL\n",
    "#    MongoDB\n",
    "    PostgreSQL\n",
    "    \n",
    "Hint\n",
    "\n",
    "    Use csvsql -h to display the help manual for the command and scroll down to look up the description for -i.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa501ad6-d8b6-462d-8ea6-2f8f77d51965",
   "metadata": {},
   "source": [
    "## Practice pulling data from database\n",
    "\n",
    "With the powers of [csvkit], we don't need to download and set up fancy database management software like MS SQL Server, DB2, PgAdmin, or TablePlus to be able to access the data inside a SQL database. We can pull data directly from our command line using csvkit's sql2csv command.\n",
    "\n",
    "In this practice, let's walk through pulling data step by step, by applying SQL manipulations to the table Spotify_Popularity which dwells inside a SQLite database called SpotifyDatabase and then saving the output of the SQL query to a local .csv file Spotify_Popularity_5Rows.csv.\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "\n",
    "    Question 1\n",
    "#    Use sql2csv to access the SQLite database SpotifyDatabase and query and print all data in the table Spotify_Popularity.\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    Use a SQL query to print the first 5 rows in the table Spotify_Popularity. Then, preview the results using csvlook.\n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "    Save queried results to a new file Spotify_Popularity_5Rows.csv. Verify and preview the file with ls and csvlook.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581b5723-7389-422d-ad15-ae3389a44780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify database name \n",
    "ls\n",
    "\n",
    "# Pull the entire Spotify_Popularity table and print in log\n",
    "sql2csv --___ \"___\" \\\n",
    "        --query \"SELECT * FROM Spotify_Popularity\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f798e21-d704-44bb-84e8-9021bb7655ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "(py39) jhu@debian:~/.virtual_environments$ sql2csv --db \"sqlite:///test.db\" --query \"SELECT * FROM test_table LIMIT 5\" > Test123.csv\n",
    "(py39) jhu@debian:~/.virtual_environments$ cat Test123.csv \n",
    "seller_id,customer_id,product_id,product_price,product_quantity\n",
    "10001,2,1023,100,12\n",
    "10001,2,1024,120,32\n",
    "10001,2,1023,100,24\n",
    "10001,2,1026,130,9\n",
    "10001,2,1025,128,32\n",
    "(py39) jhu@debian:~/.virtual_environments$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c2bd24-d893-4fb2-80d1-ed647fda6d5c",
   "metadata": {},
   "source": [
    "## Manipulating data using SQL syntax\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "**You don't necessarily need to be a die-hard QSL fan to appreciate that sometimes data manipulation tasks are just easier with QSL.  (Task Schedule on SQL Server and Scheduled Event on MySQL).  The \"csvkit\" has a command \"csvsql\" that allows us to apply SQL like syntax to local CSV file, without needing to set up a local database.  \n",
    "\n",
    "The [csvsql] is a command in the csvkit library that allows us to apply SQL-like statements to one or more locally saved CSV files.  Under the hood, this works because [csvsql] creates an in-memory SQL database that temporarily host the entire SCV file you are processing.  This means that, while convenient, this command is not suitable for large file processing or complex SQL querying.  For full documentation use the [csvsql --help] or Google it.  \n",
    "\n",
    "Lets set up an example where we have a CSV file [Spotify_MusicAttributes.csv] in our current directory, If we wish to print only the first row in this CSV, one of the many ways we can do so is by applying SQL syntax to Spotify_MusicAttributes.csv via the csvsql command.  The [csvsql] takes in teh SQL query by first using the [--query] option, followed by the SQL query in quotation marks, as a string.  The SQL query is written as though Spotify_MusicAttributes is a table that has been loaded into a database.  Please note, that it is crutial to follow the SQL query with the exact location of the data file we are referencing, relative to the current file directory.  Otherwise, [csvsql] will not know where to look for the data file.  To put everything together, the command show as:\n",
    "[csvsql --query \"SELECT * FROM Test123 LIMIT 3\" Test123.csv]\n",
    "\n",
    "This prints the first row of the data stored in the Spotify_MusicAttributes.csv file.  If we want a prettier printout, we can always pipe the results of our csvsql query to [csvlook] by attaching pipe [|] followed by [csvlook] at the end of our command.  Now its much easier to see which data point belongs to which column.  \n",
    "\n",
    "\n",
    "Lastly, if we want to save the results to a new file, we can use the redirect operator to save the output to OneSongFile.csv.  When the command executes, there is nothing printed to console.  For a sanity check, we use [ls] to confirm that the file is created correctly.  The [csvsql] command can even be applied to more than 1 csv file at a time.  As seen here, you can join 2 csv file file_a and file_b by using SQL join syntax.  \n",
    "[csvsql --query \"SQLECT * FROM file_a INNER JOIN file_b on Id\" file_a.csv file_b.csv]\n",
    "\n",
    "A few key notes are that one, the SQL query, no matter how long or complex, must be written in one line with no line breaks.  Sadly, because the limitations to our slide size, the rest of the query is abbreviated using .... Secondly, any files that are mentioned in teh query must be noted after the query ends, in the order of the file appearance in the SQL query.  In this case, we referenced file_a first in our join then file_b, so the final file order is as this  [file_a.csv file_b.csv]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Lets get some practicein, now that we can use SQL syntax even outside of database.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480a6b62-80ce-4109-b8fd-17044aafa1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################\n",
    "(py39) jhu@debian:~/.virtual_environments$ csvsql --query \"SELECT * FROM Test123 LIMIT 3\" Test123.csv \n",
    "seller_id,customer_id,product_id,product_price,product_quantity\n",
    "10001.0,2.0,1023.0,100.0,12.0\n",
    "10001.0,2.0,1024.0,120.0,32.0\n",
    "10001.0,2.0,1023.0,100.0,24.0\n",
    "(py39) jhu@debian:~/.virtual_environments$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48671802-b328-4bfe-845d-3b4eb63dc384",
   "metadata": {},
   "source": [
    "## Applying SQL to a local CSV file\n",
    "\n",
    "# Sometimes the data manipulation we want to do is just easier to do with SQL. \n",
    "In this situation, we want to find the shortest duration song in Spotify_MusicAttributes.csv by applying the SQL below directly to the data file.\n",
    "\n",
    "[SELECT * FROM Spotify_MusicAttributes ORDER BY duration_ms LIMIT 1]\n",
    "\n",
    "Let's go through this step by step.\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "\n",
    "    Question 1\n",
    "    Complete the command to apply the SQL query to Spotify_MusicAttributes.csv.\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    Further improve the output by piping the output to csvlook.\n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "    Instead of printing to console, re-direct output and save as new file: LongestSong.csv.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df13306c-c01b-4972-b592-c4257d74a3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview CSV file\n",
    "ls\n",
    "\n",
    "# Apply SQL query to Spotify_MusicAttributes.csv\n",
    "csvsql --query \"SELECT * FROM Spotify_MusicAttributes ORDER BY duration_ms LIMIT 1\" Spotify_MusicAttributes.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a479f0-75d1-4dd8-a93f-09b5bd55b478",
   "metadata": {},
   "source": [
    "## Cleaner scripting via shell variables\n",
    "\n",
    "Because SQL queries, by nature, can be long and complex, we will frequently need to deal with line breaks while passing in SQL queries to csvkit commands.\n",
    "\n",
    "# One way to work around this is to store the SQL queries as a shell variable, then pass in the shell variable in place of the SQL query where needed.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Fill in the csvsql command by calling upon the bash variable containing the SQL query instead of writing out the SQL query in full.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8105a7-faba-40ac-9c43-cfa7a56be208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview CSV file\n",
    "ls\n",
    "\n",
    "# Store SQL query as shell variable\n",
    "sqlquery=\"SELECT * FROM Spotify_MusicAttributes ORDER BY duration_ms LIMIT 1\"\n",
    "\n",
    "# Apply SQL query to Spotify_MusicAttributes.csv\n",
    "csvsql --query \"$sqlquery\" Spotify_MusicAttributes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f91895-fcdc-4783-b952-26b4f9e16b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "(py39) jhu@debian:~/.virtual_environments$ sqlquery=\"SELECT * FROM Test123 LIMIT 3\"\n",
    "(py39) jhu@debian:~/.virtual_environments$ csvsql --query $sqlquery Test123.csv \n",
    "/home/jhu/.virtual_environments/py39/lib/python3.9/site-packages/agate/table/from_csv.py:72: RuntimeWarning: Error sniffing CSV dialect: Could not determine delimiter\n",
    "ValueError: Row 1 has 2 values, but Table only has 1 columns.\n",
    "(py39) jhu@debian:~/.virtual_environments$ csvsql --query '$sqlquery' Test123.csv \n",
    "OperationalError: (sqlite3.OperationalError) near \"$sqlquery\": syntax error\n",
    "[SQL: $sqlquery]\n",
    "(Background on this error at: https://sqlalche.me/e/14/e3q8)\n",
    "(py39) jhu@debian:~/.virtual_environments$ csvsql --query \"$sqlquery\" Test123.csv \n",
    "seller_id,customer_id,product_id,product_price,product_quantity\n",
    "10001.0,2.0,1023.0,100.0,12.0\n",
    "10001.0,2.0,1024.0,120.0,32.0\n",
    "10001.0,2.0,1023.0,100.0,24.0\n",
    "(py39) jhu@debian:~/.virtual_environments$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f928fe-e4d8-432c-8c1a-97dbee71455a",
   "metadata": {},
   "source": [
    "## Joining local CSV files using SQL\n",
    "\n",
    "# [csvsql] can be used to join CSV files together even when neither of them are in a database. \n",
    "Here, we have two CSV files Spotify_MusicAttributes.csv and Spotify_Popularity.csv that are both on song level but contain different attributes for each song. We can combine the two files together using a SQL-like JOIN, and we can do so, through the power of csvsql.\n",
    "Instructions 1/2\n",
    "50 XP\n",
    "\n",
    "    Question 1\n",
    "    Explore the data with the commands we have learned so far (e.g. csvstat, csvlook, etc). What is the column that Spotify_MusicAttributes.csv and Spotify_Popularity.csv have in common that can be used as the JOIN key?\n",
    "Possible Answers\n",
    "\n",
    "    popularity\n",
    "    time_signature\n",
    "    id\n",
    "    track_id\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "    Join Spotify_MusicAttributes.csv and Spotify_Popularity.csv together to form a new file Spotify_FullData.csv.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924702ef-e76e-4d63-9d2a-fea5a27591d3",
   "metadata": {},
   "source": [
    "## Pushing data back to database\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**We've used commands in [csvkit] to do variety of tasks related to the database.  We've pulled data, We've manipulated data.  Now lets learn how to push data back to the database.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Our friend [csvsql] makes a re-appearance here, sine it pulls double duty of manipulating local CSV files with SQL statements as well as uploading and pushing CSV data back to the database.  In our previous lesson, we focused on the [--query] option of [csvsql].  \n",
    "\n",
    "# Here, we will focus on some more option args, [--insert], [--db], [--no-inference], and [--no-constraints].  \n",
    "\n",
    "Here is a sample script that use csvsql to upload a local CSV file [Spotify_Music_Attributes.csv] to an existing  SQLite database called [Spotify_Databese.db].  The syntax for this task starts with [csvsql] and the [--db] flag, followed by the full database name, to establish location of the database.  Remember to pass this in as a string, inside quotation marks.  Please note, a quick built-in line break is used here to keep our code clean and readable.  \n",
    "\n",
    "[csvsql --db \"sqlite:///SpotifyDatabase.db\"  \\\n",
    "        --insert Spotify_MusicAttributes.csv]\n",
    "\n",
    "The worth noting that we used 3 forward slashes after naming the database type, which is normal. However, because this is a SQLite database, we need to end the database with the file extension [.db].  Still the same command, but now lets focus on the second option, [--insert] option.  This is new to use and is very interesting because the insert option does many things all in one step.  \n",
    "\n",
    "Traditionally, uploading a new file as a tablein a database has multiple steps.  Depending on your SQL client or user interface, you might need to creat an empty table, specify the table schema, key the table and then write in the data row by row.  Here [--insert] will take care of all of these steps by itsel, by making its best guess at what the schema of the table should be.  Pretty neat right?  \n",
    "\n",
    "\n",
    "The [--insert] option can only by used in conjunction with [--db] option, whcih should make sense, because if we don't specify where the database is, we cant insert data into the database.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "Lastly and unfortunately, [csvsql] in not always going to to be 100% failsafe when it comes to schema inference.  If you knowthat your data have a number of colums that should rather be treated as a text column instead, then you can preemptively account for this by using the [--no-inference] option flag with [csvsql].  This way, csvsql will treat every column as text, whether it looks like numbers, dates, or strings.  \n",
    "\n",
    "Likewise [--no-constraints] allows the table to be created without any character length limits, which is helpful for particularly large data tables and considerablely speeds up the insertion process.  It will allow you to create columns containing all null data, without throwing any errors.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "[csvsql --no-inference --no-constrints \\\n",
    "        --db \"sqlite:///SpotifyDatabase.db\" \\ \n",
    "        --insert Spotify_MusicAttributes.csv]\n",
    "\n",
    "\n",
    "Lets practice.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ef9762-2c4b-4ae4-9c26-ed184da50149",
   "metadata": {},
   "source": [
    "## Practice pushing data back to database\n",
    "\n",
    "It is also possible to go the other way around and push local CSV files back to the database. As long as we specify the database as well as the CSV file to be loaded, csvsql does the rest of the work for us (e.g. inferring table schema), behind the scenes.\n",
    "\n",
    "In the following exercise, complete the command to upload Spotify_MusicAttributes.csv as its own table in the SQLite database SpotifyDatabase. Then, as a sanity check, re-pull the data from the newly created table in the database.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    Upload Spotify_MusicAttributes.csv as its own table in the SQLite database SpotifyDatabase.\n",
    "    Re-pull the data from the newly created table Spotify_MusicAttributes in the SQLite database SpotifyDatabase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73da624-5c6f-405b-ac32-33320ffa0863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview file\n",
    "ls\n",
    "\n",
    "# Upload Spotify_MusicAttributes.csv to database\n",
    "csvsql --db \"sqlite:///SpotifyDatabase.db\" --insert Spotify_MusicAttributes.csv\n",
    "\n",
    "\n",
    "# Store SQL query as shell variable\n",
    "sqlquery=\"SELECT * FROM Spotify_MusicAttributes\"\n",
    "\n",
    "# Apply SQL query to re-pull new table in database\n",
    "sql2csv --db \"sqlite:///SpotifyDatabase.db\" --query \"$sqlquery\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea47cd7-fccf-45d6-b2ed-8b899e6e2dd5",
   "metadata": {},
   "source": [
    "## Database and SQL with csvkit\n",
    "\n",
    "The addition of [csvsql] and [sql2csv] allows us to go through an entire data workflow inside the terminal without needing to install and set up additional SQL clients and software. In this capstone, we will put together and pull data from a SQLite database, merge this data with a locally saved file, and finally, push a final merged file back to the database, all without ever leaving the command line.\n",
    "Instructions 1/3\n",
    "35 XP\n",
    "\n",
    "    Question 1\n",
    "#    Download the entire table SpotifyMostRecentData from the SQLite database SpotifyDatabase and save it as a csv file locally as SpotifyMostRecentData.csv.\n",
    "    \n",
    "    \n",
    "    Question 2\n",
    "#    Manipulate the two local csv files SpotifyMostRecentData.csv and Spotify201812.csv by passing in the stored UNION ALL SQL query into csvsql. Save the newly created file as UnionedSpotifyData.csv.\n",
    "    \n",
    "    \n",
    "    Question 3\n",
    "#    -Push the newly created csv file UnionedSpotifyData.csv back to database SpotifyDatabase as its own table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abec4c41-5edf-4489-b752-082e856088ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store SQL for querying from SQLite database \n",
    "sqlquery_pull=\"SELECT * FROM SpotifyMostRecentData\"\n",
    "\n",
    "# Apply SQL to save table as local file \n",
    "sql2csv --db \"sqlite:///SpotifyDatabase.db\" \n",
    "        --query \"$sqlquery_pull\" > SpotifyMostRecentData.csv\n",
    "\n",
    "\n",
    "\n",
    "csvsql --query \"SELECT * FROM SpotifyMostRecentData UNION ALL SELECT * FROM Spotify201812\" \n",
    "       SpotifyMostRecentData.csv Spotify201812.csv \n",
    "       > UnionedSpotifyData.csv\n",
    "        \n",
    "        \n",
    "csvsql --db \"sqlite:///SpotifyDatabase.db\"\n",
    "       --insert UnionedSpotifyData.csv\n",
    "        \n",
    "#####################################################################################################################\n",
    "UNION ALL Syntax\n",
    "\n",
    "The UNION operator selects only distinct values by default. To allow duplicate values, use UNION ALL:\n",
    "SELECT column_name(s) FROM table1\n",
    "UNION ALL\n",
    "SELECT column_name(s) FROM table2; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba314f1-f551-41ef-bf8f-23a857a2b5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store SQL for querying from SQLite database\n",
    "sqlquery_pull = \"SELECT * FROM SpotifyMostRecentData\"\n",
    "\n",
    "# Apply SQL to save table as local file\n",
    "sql2csv - -db \"sqlite:///SpotifyDatabase.db\" - -query \"$sqlquery_pull\" > SpotifyMostRecentData.csv\n",
    "\n",
    "\n",
    "# Store SQL for UNION of the two local CSV files\n",
    "sqlquery_union = \"SELECT * FROM SpotifyMostRecentData UNION ALL SELECT * FROM Spotify201812\"\n",
    "\n",
    "\n",
    "# Apply SQL to union the two local CSV files and save as local file\n",
    "csvsql - -query \"$sqlquery_union\" SpotifyMostRecentData.csv Spotify201812.csv > UnionedSpotifyData.csv\n",
    "\n",
    "\n",
    "# Push UnionedSpotifyData.csv to database as a new table\n",
    "csvsql - -db \"sqlite:///SpotifyDatabase.db\" - -insert UnionedSpotifyData.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a76e2f-f71d-47af-9999-bfa94d6c715a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "213bc380-002d-4d09-96bd-2141e8901225",
   "metadata": {},
   "source": [
    "## Python on the command line\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "**Although data processing is more productive on the command line, complex data work like predictive modeling is still easier in Python.  In this chapter, we will learn how to run Python on the command line, so that we can combine the best of both worlds.  \n",
    "\n",
    "Python comes pre-installed with MacOS and Linux, but needs to be installed for windows.  The most common way to interact with Python is via GUI interface like JupyterLab, but the more efficient way is to access Python directly on the command line.  Lets explore how to do this.  [man python] calls up the documentation and option flags.  Since Python2 and Python3 have different syntax, its helpful to check on the Python version by using [--version].  Here we have Python 3.5.2 installed.  Likewise, it is possible to have multiple copies of the same version of Python co-existing on your machine.  To see which version you're using, type [which python].  Here we're using the native Python in uers bin.  However, if you installed Python through Anaconda, that will create different copy of Python on top of the  native Python that come with your computer.  Here I'm using the self-built Python 3.9 environment.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "One way to interact with Python on the command line is with an interactive Python session by typing [python] in the terminal and pressing Enter.  You will know this is a success if you aee a printout with the Python version, followed by the curser blinking after 3 re-direct signs [>>>].  The 3 re-direct signs serve as a reminder that you are inside the interactive session.  As long as you are inside the session you can only write Python syntax.  For example, we can type [print(\"hello world\")] print open parenthesis quotation marks hello world end quotation marks end parenthesis, which is Python syntax.  The terminal will echo back \"hello world\".  To exit the session, type [exit()] and press Enter.  Notice that in the next line, the three re-directs are replaced with a dollar sign, which indicates we're back in normal command line.  \n",
    "\n",
    "# *******************************************************************************************************************\n",
    "The interactive session is easy to use, but it isn't helpful for code reproducibility.  To execute the same Python command multiple times and on-demand, we need a second way to use Python on the command line.  This is done by saving code in a Python script and calling this script when we are ready to execute.  Lets dig further.  We can create the Python file using any text editor like Sublime, nano, Vim, or Emacs.  Make sure that the file ends in a [.py] extension.  If our script is very short, we can simply echo this command and re-direct it into the Python file.  Like this [echo \"print('hello world')\" > hello_world.py].  This one-liner does two things: it instantiates a new file called hello_world.py and populates the file with the print command.  The benefit of this method is that we never leave the command line causing less disruption to our workflow.  Lets sanity check and print the file content.  \n",
    "\n",
    "\n",
    "# *******************************************************************************************************************\n",
    "With the Python script created, we still need to call it to execute.  First, we check that we are in the same file directory as the Python script we created.  Then, we tell our terminal to execute this file using Python by calling [python hello_world.py],  python followed by the Python file name.  Now our terminal echeos hello world to us.  Congratulations, We have begun our journey on integrating Python into our command line data workflow.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1bf7a7-5a68-4c5f-86c9-21b5577bb969",
   "metadata": {},
   "outputs": [],
   "source": [
    "(py39) jhu@debian:~/.virtual_environments$ python --help\n",
    "usage: python [option] ... [-c cmd | -m mod | file | -] [arg] ...\n",
    "Options and arguments (and corresponding environment variables):\n",
    "-b     : issue warnings about str(bytes_instance), str(bytearray_instance)\n",
    "         and comparing bytes/bytearray with str. (-bb: issue errors)\n",
    "-B     : don't write .pyc files on import; also PYTHONDONTWRITEBYTECODE=x\n",
    "-c cmd : program passed in as string (terminates option list)\n",
    "-d     : turn on parser debugging output (for experts only, only works on\n",
    "         debug builds); also PYTHONDEBUG=x\n",
    "-E     : ignore PYTHON* environment variables (such as PYTHONPATH)\n",
    "-h     : print this help message and exit (also --help)\n",
    "......\n",
    "\n",
    "\n",
    "(py39) jhu@debian:~/.virtual_environments$ python -c \"print('Hello World', '\\n')\"\n",
    "Hello World \n",
    "\n",
    "(py39) jhu@debian:~/.virtual_environments$ \n",
    "\n",
    "\n",
    "(py39) jhu@debian:~/.virtual_environments$ which python\n",
    "/home/jhu/.virtual_environments/py39/bin/python\n",
    "(py39) jhu@debian:~/.virtual_environments$ \n",
    "\n",
    "\n",
    "\n",
    "(py39) jhu@debian:~/.virtual_environments$ python\n",
    "Python 3.9.2 (default, Feb 28 2021, 17:03:44) \n",
    "[GCC 10.2.1 20210110] on linux\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    ">>> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1a7738-1355-4ee2-ac7b-02ca771b0018",
   "metadata": {},
   "outputs": [],
   "source": [
    "(py39) jhu@debian:~/.virtual_environments$ echo \"print('Bonjour Sawa')\" > ~/Downloads/BonjiurSawa.py\n",
    "(py39) jhu@debian:~/.virtual_environments$ ls ~/Downloads/ | grep *.py\n",
    "(py39) jhu@debian:~/.virtual_environments$ ls ~/Downloads/ | grep \"*.py\"\n",
    "(py39) jhu@debian:~/.virtual_environments$ ls ~/Downloads/ | grep .py\n",
    "BonjiurSawa.py\n",
    "(py39) jhu@debian:~/.virtual_environments$ ls ~/Downloads/ | grep *.py\n",
    "(py39) jhu@debian:~/.virtual_environments$ ls ~/Downloads/ | grep %*.py\n",
    "BonjiurSawa.py\n",
    "(py39) jhu@debian:~/.virtual_environments$ ls ~/Downloads/ | grep $*.py\n",
    "BonjiurSawa.py\n",
    "(py39) jhu@debian:~/.virtual_environments$ cat ~/Downloads/BonjiurSawa.py \n",
    "print('Bonjour Sawa')\n",
    "(py39) jhu@debian:~/.virtual_environments$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27ce772-1495-40c5-8da7-7bdcbc2b7ccb",
   "metadata": {},
   "source": [
    "## Finding Python version on the command line\n",
    "\n",
    "Which of the following commands will NOT show what version of Python is installed? Feel free to try out the various answers on the terminal.\n",
    "Instructions\n",
    "50 XP\n",
    "Possible Answers\n",
    "\n",
    "    python\n",
    "    python -V\n",
    "    python --version\n",
    "#    which python   gives you the location of python env\n",
    "\n",
    "Hint\n",
    "\n",
    "    which python shows the location where Python is installed on your machine. This does not always contain the version of Python in the folder name.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc4eb55-756e-49c8-a595-566eba53ab5d",
   "metadata": {},
   "source": [
    "## Executing Python script on the command line\n",
    "\n",
    "Let's work through an example of how we can use Python on the command line without needing to open up a GUI like Jupyter Notebook or Spyder. Interacting with Python on the command line is faster and more efficient than using a GUI. Here, we will create a Python file and execute it using our native Python, all without leaving the bash terminal.\n",
    "Instructions\n",
    "100 XP\n",
    "\n",
    "    In one step, create a new Python file and pass the Python print command into the file.\n",
    "    Execute the new Python file by calling it directly from the command line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d792906-69b1-4932-85b7-d94e4e47593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in one step, create a new file and pass the print function into the file\n",
    "echo \"print('This is my first Python script')\" > my_first_python_script.py\n",
    "\n",
    "# check file location \n",
    "ls\n",
    "\n",
    "# check file content \n",
    "cat my_first_python_script.py\n",
    "\n",
    "# execute Python script file directly from command line  \n",
    "python my_first_python_script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af330d8b-fca9-4142-9939-4d015bb6fe11",
   "metadata": {},
   "source": [
    "## Python package installation with pip\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**In this lesson, lets learn about pip, the standard package manager for Python.  Python comes by default with a collection of built-in functions such as the print function and built-in packages such as the math or os library.  However, because the Python is a general purpose langurage, packages for specific use cases need to be separately installed by the user.  This includes data science packages such as scikit-learn and statsmodels.  The installation is handled through pip, the standard package manager for Python.  \n",
    "\n",
    "First thing first, read the documentation through [pip -h] command.  The pip has a variety of commands and option flags designed to manager Python packages.  Secondly, note that you can print the pip vwersion the same way you print the Python version.  Its important that the pip version is compatible with he Python version.  Herewe see that pip 19.1.1 is compatible with Python 3.5.  If pip is giving you an upgrade warning, you can upgrade pip using itself via [pip install --upgrade pip] command.  Before we do any installing, its a good idea to see what is already installed.  Using [pip list] on the command line prints an alphabetical list of all the Python packages in your current Python environment.  \n",
    "\n",
    "To install a library that is not currently installed, as indicated by pip list, the syntax command is [pip install scikit-learn], in this case we are installing scikit-learn.  You might notice from the logs through, it looks line more than one Python package is being installed in this one pip-install call.  This is because pip will automatically installscikit-learn as well as any other Python package that it is dependent on to operate.  We call these other packages: dependencies.  \n",
    "\n",
    "Be default, pip-install will always install the latest version of the library.  Here, we have installed Scikit-Learn version 0.13.2.  If we wish to install an older version of scikit-learn, we simply have to specify it during the install statement, with double equal signs, like so.  [pip install scikit-learn=0.19.2].  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af8baba-e9d0-4187-8a3c-50fde2eceb2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332d4ccb-13b4-4ecd-a885-b4f98ab4f4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e97a0f-0f93-449d-9457-22ae30ea11b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4f7d53-210e-4794-bd55-1378203fa008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab5b2e1-49a8-45a0-bb49-43b193930870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f25b47-39bb-4d08-9cb6-2d1c2a72526e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e7b674-c862-44d7-b9f5-c1975b6e7f24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846a80ac-dbb9-4456-94ab-ec093a2fce3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c79d599-5f1a-47cf-8090-404d7e1abe06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7242ef-ef18-4854-b66d-175bc16e717c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d9a5f7-1aef-463a-97d6-de13c85dd1a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f9dfa6-5afb-4d5c-9215-9b4071442889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5123b40d-6753-47f0-be8e-35b6798a47ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a29048-eb92-43ab-9215-a392509e0bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523ba74d-d926-485f-9a21-8c3561073473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb745ca0-0884-492c-9a57-9e512672c93a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818a4443-610a-4a42-8f33-82d476794c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fff00b-e44d-4fc7-baf1-bfb3b7564007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d73974a-fa81-4427-8cfd-16206b88cdbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
